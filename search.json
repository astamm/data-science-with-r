[
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html",
    "title": "Linear Regression with R",
    "section": "",
    "text": "In the following exercises, you will be asked to study the relationship of a continuous response variable and one or more predictors. In doing so, remember to:\n\nperform model diagnosis\nincluding visualization tools\nincluding multicollinearity assessment\nperform informed model selection\ncomment each result of an analysis you run with R"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#expectations",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#expectations",
    "title": "Linear Regression with R",
    "section": "",
    "text": "In the following exercises, you will be asked to study the relationship of a continuous response variable and one or more predictors. In doing so, remember to:\n\nperform model diagnosis\nincluding visualization tools\nincluding multicollinearity assessment\nperform informed model selection\ncomment each result of an analysis you run with R"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-1",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-1",
    "title": "Linear Regression with R",
    "section": "Exercise 1",
    "text": "Exercise 1\nAnalysis of the production data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nx\nNumber of produced pieces\n\n\ny\nProduction cost\n\n\n\nStudy the relationship between x and y."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-2",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-2",
    "title": "Linear Regression with R",
    "section": "Exercise 2",
    "text": "Exercise 2\nAnalysis of the brain data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nbody_weight\nBody weight in kg\n\n\nbrain_weight\nBrain weight in kg\n\n\n\nStudy the relationship between body and brain weights, to establish how the variable brain_weight changes with the variable body_weight."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-3",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-3",
    "title": "Linear Regression with R",
    "section": "Exercise 3",
    "text": "Exercise 3\nAnalysis of the anscombe data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nx1\nPredictor to be used for explaining y1\n\n\nx2\nPredictor to be used for explaining y2\n\n\nx3\nPredictor to be used for explaining y3\n\n\nx4\nPredictor to be used for explaining y4\n\n\ny1\nResponse to be explained by x1\n\n\ny2\nResponse to be explained by x2\n\n\ny3\nResponse to be explained by x3\n\n\ny4\nResponse to be explained by x4\n\n\n\nStudy the relationship between each \\(y_i\\) and the corresponding \\(x_i\\)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-4",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-4",
    "title": "Linear Regression with R",
    "section": "Exercise 4",
    "text": "Exercise 4\nAnalysis of the cement data set, which contains the following variables:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\naluminium\nPercentage of \\(\\mathrm{Ca}_3 \\mathrm{Al}_2 \\mathrm{O}_6\\)\n\n\nsilicate\nPercentage of \\(\\mathrm{C}_2 \\mathrm{S}\\)\n\n\naluminium_ferrite\nPercentage of \\(4 \\mathrm{CaO} \\mathrm{Al}_2 \\mathrm{O}_3 \\mathrm{Fe}_2 \\mathrm{O}_3\\)\n\n\nsilicate_bic\nPercentage of \\(\\mathrm{C}_3 \\mathrm{S}\\)\n\n\nhardness\nHardness of the cement obtained by mixing the above four components\n\n\n\nStudy, using a multiple linear regression model, how the variable hardness depends on the four predictors."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-5",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-5",
    "title": "Linear Regression with R",
    "section": "Exercise 5",
    "text": "Exercise 5\nAnalysis of the job data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\naverage_score\nAverage score obtained by the employee in the test\n\n\nyears_service\nNumber of years of service\n\n\nsex\nMale or female\n\n\n\nWe want to see if it is possible to use the sex of the person in addition to the years of service to predict, with a linear model, the average score obtained in the test. Estimate a linear regression of average_score vs. years_service, considering the categorical variable sex."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-6",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-6",
    "title": "Linear Regression with R",
    "section": "Exercise 6",
    "text": "Exercise 6\nAnalysis of the cars data set, which contains the following variables:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nspeed\nSpeed of the car before starting braking\n\n\ndist\nDistance travelled by the car during the braking period until it completely stops\n\n\n\nVerify if the distance travelled during the braking depends on the starting velocity of the car:\n\nChoose the best model to explain the distance as function of the speed,\nPredict the braking distance for a starting velocity of 25 km/h, using a point estimate and a prediction interval."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-7",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-7",
    "title": "Linear Regression with R",
    "section": "Exercise 7",
    "text": "Exercise 7\nAnalysis of the mussels data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nlength\nLength of a mussel (mm)\n\n\nwidth\nWidth of a mussel (mm)\n\n\nheight\nHeight of a mussel (mm)\n\n\nsize\nMass of a mussel (g)\n\n\nweight\nWeight of eatable part of a mussel (g)\n\n\n\nWe want to study how the eatable part of a mussel varies as a function of the other four variables using a multiple linear regression."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-linear-regression-model",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-linear-regression-model",
    "title": "Linear Regression with R",
    "section": "The linear regression model",
    "text": "The linear regression model\n\nThe goal is to propose and estimate a model for explaining a continuous response variable \\(Y_i\\) from a number of fixed predictors \\(x_{i1}, \\dots, x_{ik}\\).\nMathematically, the model reads: \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik} + \\varepsilon_i, \\quad \\mbox{with} \\quad \\mathbb{E}[\\varepsilon_i] = 0 \\quad \\mbox{and} \\quad \\mathbb{V}\\mbox{ar}[\\varepsilon_i] = \\sigma^2. \\]\nWe can summarize the assumptions on which relies the linear regression model as follows:\n\nThe predictors are fixed. This means that we do not assume (or take into account the) intrinsic variability in the predictor values. The randomness of \\(Y_i\\) all comes from the error term \\(\\varepsilon_i\\). In particular, it implies that considering a sample of \\(n\\) i.i.d. random response variables \\(Y_1, \\dots, Y_n\\) boils down to assuming that \\(\\varepsilon_1, \\dots, \\varepsilon_n\\) are i.i.d.;\nThe error random variables \\(\\varepsilon_i\\) are centered and of constant variance. Combined with Assumption 1, this means that \\(\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\) and \\(\\mathrm{Cov}[\\boldsymbol{\\varepsilon}] = \\sigma^2 \\mathbb{I}_n\\);\n[optional] Parametric hypothesis testing and confidence intervals further require the assumption of normality for the error vector, i.e. \\(\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbb{I}_n)\\)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimation-problem",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimation-problem",
    "title": "Linear Regression with R",
    "section": "Model Estimation Problem",
    "text": "Model Estimation Problem\n\nMatrix representation\n\n\\(Y_1, \\dots, Y_n\\) sample of \\(n\\) i.i.d. random response variables with associated observed values \\(y_1, \\dots, y_n\\),\nDesign matrix \\(\\mathbb{X}\\) of size \\(n \\times (k + 1)\\) with \\(x_{ij}\\) at row \\(i\\) and column \\(j\\) leading to \\(\\mathbf{y} = \\mathbb{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\).\nWarning: a regression is linear in the \\(\\boldsymbol{\\beta}\\) coefficients, not in the predictors.\n\nParameters to be estimated\n\nthe regression coefficients \\(\\boldsymbol{\\beta}\\); and,\nthe common error variance term \\(\\sigma^2\\).\n\nHow to estimate the best parameters?\n\\[ \\mbox{SSD}(\\boldsymbol{\\beta}, \\sigma^2; \\mathbf{y}, \\mathbb{X}) := (\\mathbf{y} - \\mathbb{X} \\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbb{X} \\boldsymbol{\\beta}) = \\sum_{i = 1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik}))^2 .\\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-case-of-categorical-predictors",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-case-of-categorical-predictors",
    "title": "Linear Regression with R",
    "section": "The case of categorical predictors",
    "text": "The case of categorical predictors\n\nIf a predictor is categorical, how do we fill in the numeric matrix \\(\\mathbb{X}\\)?\nDummy variables\n\nA categorical variable with two categories (sex for instance) can be transformed in a single numeric variable sex == \"male\", which evaluates to \\(1\\) if sex is \"male\" and to \\(0\\) if sex is \"female\" (remember logicals are numbers in R).\nA categorical variable with three categories (origin which stores the acronym of the New York airport for departure in the flights data set) can be converted into two numeric variables as shown in the table below:\n\n\n\norigin\noriginJFK\noriginLGA\n\n\n\n\nEWR\n0\n0\n\n\nJFK\n1\n0\n\n\nLGA\n0\n1"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimators",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimators",
    "title": "Linear Regression with R",
    "section": "Model Estimators",
    "text": "Model Estimators\n\nEstimator of the coefficients (unbiased)\n\\[ \\widehat{\\boldsymbol{\\beta}} := (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^\\top \\mathbf{Y}, \\]\nFitted responses\n\\[ \\widehat{\\mathbf{Y}} = \\mathbb{X} \\widehat{\\boldsymbol{\\beta}} = \\mathbb{X} (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^\\top \\mathbf{Y} = \\mathbb{H} \\mathbf{Y}. \\]\nEstimator of the constant variance term \\(\\sigma^2\\) (unbiased)\n\\[ \\widehat{\\sigma^2} := \\frac{(\\mathbf{Y} - \\widehat{\\mathbf{Y}})^\\top (\\mathbf{Y} - \\widehat{\\mathbf{Y}})}{n - k - 1}. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#hat-matrix-and-leverage",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#hat-matrix-and-leverage",
    "title": "Linear Regression with R",
    "section": "Hat matrix and leverage",
    "text": "Hat matrix and leverage\n\nHat matrix (projection matrix, influence matrix)\n\\[ \\mathbb{H} = \\mathbb{X} (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^\\top \\]\nLeverage\n\nThe diagonal terms of \\(\\mathbb{H}\\) are such that \\(0 \\le h_{ii} \\le 1\\).\n\\(h_{ii}\\) is called the leverage score of observation \\((y_i, x_{i1}, \\dots, x_{ik})\\).\nThe leverage score does not depend on \\(y_i\\) at all but only on the predictor values.\nWith some abuse of notation, we can write: \\[ h_{ii} = \\frac{\\partial \\widehat{Y_i}}{\\partial Y_i}, \\] which illustrates that the leverage measures the degree by which the \\(i\\)-th measured value influences the \\(i\\)-th fitted value."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#residuals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#residuals",
    "title": "Linear Regression with R",
    "section": "Residuals",
    "text": "Residuals\n\n\nNatural definition. Difference between observed and fitted response values: \\[ \\widehat{\\boldsymbol{\\varepsilon}} := \\mathbf{y} - \\widehat{\\mathbf{y}} = (\\mathbb{I}_n - \\mathbb{H}) \\mathbf{y}. \\]\n\\(\\mathbb{E}[\\widehat{\\boldsymbol{\\varepsilon}}] = \\mathbf{0}\\)\nResiduals do not have constant variance: \\[ \\mathbb{V}\\mbox{ar}[\\widehat{\\varepsilon}_i] = \\sigma^2 (1 - h_{ii}). \\]\nResiduals are not uncorrelated: \\[ \\mbox{Cov}(\\widehat{\\varepsilon}_i, \\widehat{\\varepsilon}_j) = -\\sigma^2 h_{ij}, \\mbox{ for } i \\ne j. \\]\nStandardized residuals. Residuals with almost constant variance (also known as internally studentized residuals): \\[ r_i := \\frac{\\widehat{\\varepsilon}_i}{s(\\widehat{\\varepsilon}_i)} = \\frac{\\widehat{\\varepsilon}_i}{\\sqrt{\\widehat{\\sigma^2} \\left( 1 - h_{ii} \\right)}}. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#studentized-residuals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#studentized-residuals",
    "title": "Linear Regression with R",
    "section": "Studentized residuals",
    "text": "Studentized residuals\n\n\nDefinition. Studentized residuals for any given data point are calculated from a model fit to every other data point except the one in question. They are also called externally studentized residuals, deleted residuals or jackknifed residuals. \\[ t_i := \\frac{y_i - \\widehat{y}_i^{(-i)}}{\\sqrt{\\widehat{\\sigma^2}^{(-i)} \\left( 1 - h_{ii} \\right)}} = r_i \\left( \\frac{n - k - 2}{n - k - 1 - r_i^2} \\right)^{1/2}, \\] where \\(n\\) is the total number of observations and \\(k\\) is the number of predictors used in the model.\nDistribution. If the normality assumption of the original regression model is met, a studentized residual follows a Student’s t-distribution.\nApplication. The motivation behind studentized residuals comes from their use in outlier testing. If we suspect a point is an outlier, then it was not generated from the assumed model, by definition. Therefore it would be a mistake – a violation of assumptions – to include that outlier in the fitting of the model. Studentized residuals are widely used in practical outlier detection.\nSource. https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#cooks-distance",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#cooks-distance",
    "title": "Linear Regression with R",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\n\nDefinition. Cook’s distance \\(D_i\\) of observation \\(i\\) (for \\(i = 1, \\dots, n\\)) is defined as a scaled sum of squared differences between fitted values obtained by including or excluding that observation: \\[ D_i := \\frac{ \\sum_{j = 1}^{n} \\left( \\widehat{y}_j - \\widehat{y}_j^{(-i)} \\right)^2}{\\widehat{\\sigma^2} (k+1)} = \\frac{r_i^2}{k+1} \\frac{h_{ii}}{1-h_{ii}}. \\]\nIf observation \\(i\\) is an outlier, in the sense that it has probably been drawn from another distribution with respect to the other observations, then \\(r_i\\) should be excessively high, which tends to increase Cook’s distance;\nIf observation \\(i\\) has a lot of influence (high leverage score), then Cook’s distance increases.\nYou can have cases in which an observation might have high influence but is not necessarily an outlier and therefore should be kept for analysis.\nThe reverse can happen as well. Some points with low influence (low leverage score) can be outliers (high residual value). In this case, we could be tempted to remove the observation because it violates our assumption.\nCook’s distance does not help in the above two situations, but it does not really matter, because, in both cases, we can safely include the corresponding observation into our regression."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals",
    "title": "Linear Regression with R",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nGoal\nCompute a confidence interval for the mean response \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) at an already observed point \\(\\mathbf{x}_i\\), \\(i=1,\\dots,n\\).\nPoint estimator of \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)\n\\[ \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}} \\sim \\mathcal{N} \\left( \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{x}_i^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_i \\right), \\]\nPoint estimator of \\(\\sigma^2\\)\nWe do not know \\(\\sigma^2\\) but we have that: \\[ \\frac{\\widehat{\\sigma^2}}{\\sigma^2} \\sim \\chi^2(n-k-1), \\] and \\(\\widehat{\\boldsymbol{\\beta}}\\) and \\(\\widehat{\\sigma^2}\\) are statistically independent."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals-continued",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals-continued",
    "title": "Linear Regression with R",
    "section": "Confidence intervals (continued)",
    "text": "Confidence intervals (continued)\n\nPivotal statistic\nA pivotal statistic for a parameter \\(\\theta\\) is a random variable \\(T(\\theta)\\) such that\n\nthe only unknown in its definition (computation) is the parameter that we aim at estimating,\nthe distribution of \\(T(\\theta)\\) is known and does not depend on the unknown parameter \\(\\theta\\),\n\nWhen the parameter is the mean response \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) at \\(\\mathbf{x}_i\\), we can use the following pivotal statistic:\n\\[ \\frac{\\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}}}{\\sqrt{\\widehat{\\sigma^2} \\mathbf{x}_i^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_i}} \\sim \\mathcal{S}tudent(n-k-1). \\]\nConstruction of the confidence interval\nWe can express a \\((1-\\alpha)\\%\\) confidence interval for the mean response \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) at \\(\\mathbf{x}_i\\) as: \\[ \\mathbb{P} \\left( \\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in \\left[ \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}} \\pm t_{1-\\frac{\\alpha}{2}}(n-k-1) \\sqrt{\\widehat{\\sigma^2} \\mathbf{x}_i^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_i} \\right] \\right) = 1 - \\alpha. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals",
    "title": "Linear Regression with R",
    "section": "Prediction intervals",
    "text": "Prediction intervals\n\nGoal\nCompute a prediction interval for a new not yet observed response \\(Y_{n+1}\\) at a new observed point \\(\\mathbf{x}_{n+1}\\): \\[ Y_{n+1} = \\mathbf{x}_{n+1}^\\top \\boldsymbol{\\beta} + \\varepsilon_{n+1} \\sim \\mathcal{N} \\left( \\mathbf{x}_{n+1}^\\top \\boldsymbol{\\beta}, \\sigma^2 \\right), \\]\nPoint estimator of \\(Y_{n+1}\\)\n\\[ \\widehat{Y_{n+1}} = \\mathbf{x}_{n+1}^\\top \\widehat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}  \\left( \\mathbf{x}_{n+1}^\\top \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{x}_{n+1}^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_{n+1} \\right), \\]\nPoint estimator of \\(\\sigma^2\\)\nWe do not know \\(\\sigma^2\\) but we have that: \\[ \\frac{\\widehat{\\sigma^2}}{\\sigma^2} \\sim \\chi^2(n-k-1), \\] and \\(\\widehat{\\boldsymbol{\\beta}}\\) and \\(\\widehat{\\sigma^2}\\) are statistically independent."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals-continued",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals-continued",
    "title": "Linear Regression with R",
    "section": "Prediction intervals (continued)",
    "text": "Prediction intervals (continued)\n\nPivotal statistic\nWe can now define a random variable of known distribution whose only unknown (once the \\(n\\)-sample is observed) is \\(Y_{n+1}\\): \\[ \\frac{Y_{n+1} - \\widehat{Y_{n+1}}}{\\sqrt{\\widehat{\\sigma^2} \\left( 1 + \\mathbf{x}_{n+1}^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_{n+1} \\right)}} \\sim \\mathcal{S}tudent(n-k-1). \\]\nConstruction of the prediction interval\nWe can express a \\((1-\\alpha)\\%\\) prediction interval for the new not yet observed response \\(Y_{n+1}\\) at \\(\\mathbf{x}_{n+1}\\) as: \\[ \\mathbb{P} \\left( Y_{n+1} \\in \\left[ \\widehat{Y_{n+1}} \\pm t_{1-\\frac{\\alpha}{2}}(n-k-1) \\sqrt{\\widehat{\\sigma^2} \\left( 1 + \\mathbf{x}_{n+1}^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_{n+1} \\right)} \\right] \\right) = 1 - \\alpha. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-step-visualization",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-step-visualization",
    "title": "Linear Regression with R",
    "section": "First step: Visualization",
    "text": "First step: Visualization\n\nThe most important first thing you should do before any modeling attempt is to look at your data.\n\n\n\n\n\n\n\n\n\nAs you can see, both data sets seem to generate the same linear regression predictions, although we already clearly understand which one will go sideways…"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-step-modeling",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-step-modeling",
    "title": "Linear Regression with R",
    "section": "Second step: Modeling",
    "text": "Second step: Modeling\n\n\nWe know the R function to fit linear regression models is lm().\nWe have seen its syntax for simple models.\nWe have been introduced to the broom package for retrieving relevant information from the output of lm().\nThe broom::glance() help page will provide you with a short summary of the information retrieved by glance().\nThe broom::tidy() help page will provide you with a short summary of the information retrieved by tidy().\nThe broom::augment() help page will provide you with a short summary of the information retrieved by augment()."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#third-step-model-diagnosis",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#third-step-model-diagnosis",
    "title": "Linear Regression with R",
    "section": "Third step: Model Diagnosis",
    "text": "Third step: Model Diagnosis\n\nYou must always remember that, even though R facilitates computations, it does not verify for you that the assumptions required by the model are met by your data!\nThis is where model diagnosis comes into play. You can almost entirely diagnose your model graphically. Using the grammar of graphics in ggplot2, this can be achieved using the ggfortify package.\n\n# install.packages(\"ggfortify\")\nlibrary(ggfortify)\nfirst_model |&gt; \n  autoplot(which = 1:6, label.size = 3, nrow = 2) + \n  theme_bw()"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#reminder-model-assumptions-13",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#reminder-model-assumptions-13",
    "title": "Linear Regression with R",
    "section": "Reminder: model assumptions (1/3)",
    "text": "Reminder: model assumptions (1/3)\n\n\n\n\n\n\n\n\n\nAssumptions to check\n\n\n\nLinearity. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.\nNormality. The error terms are assumed to be normally distributed.\nHomogeneity of variance. The error terms are assumed to have a constant variance (homoscedasticity).\nIndependence. The error terms are assumed to be independent.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPotential problems to check\n\n\n\nNon-linearity of the outcome - predictor relationships\nHeteroscedasticity: Non-constant variance of error terms.\nPresence of influential and potential outlier values in the data:\n\nOutliers: typically large standardized residuals\nHigh-leverage points: typically large leverage values"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-residuals-vs-fitted-values",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-residuals-vs-fitted-values",
    "title": "Linear Regression with R",
    "section": "First data set: Residuals vs fitted values",
    "text": "First data set: Residuals vs fitted values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nTo check for non-linearity not accounted for; a horizontal line, without distinct patterns is an indication for a linear relationship, which is good."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-normal-qq-plot",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-normal-qq-plot",
    "title": "Linear Regression with R",
    "section": "First data set: Normal QQ plot",
    "text": "First data set: Normal QQ plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nTo check the normality of residuals; good if residuals follow the straight dashed line."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-scale-location-plot",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-scale-location-plot",
    "title": "Linear Regression with R",
    "section": "First data set: Scale-location plot",
    "text": "First data set: Scale-location plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nTo check the homoscedasticity assumption (constant variance); horizontal line with equally spread points is a good indication of homoscedasticity."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-influential-points-outliers",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-influential-points-outliers",
    "title": "Linear Regression with R",
    "section": "First data set: Influential points, outliers",
    "text": "First data set: Influential points, outliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\nTo check the zero-mean assumption on the residuals and to spot potential outliers in the top and bottom right corners (residuals vs leverages plot)\nTo spot potential outliers as observations with the largest Cook’s distance."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-data-set-model-diagnosis",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-data-set-model-diagnosis",
    "title": "Linear Regression with R",
    "section": "Second data set: Model diagnosis",
    "text": "Second data set: Model diagnosis"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification",
    "title": "Linear Regression with R",
    "section": "Model specification",
    "text": "Model specification\n\n\nmod &lt;- lm(formula = response ~ predictors, data = fancy_data)\n\nThe left-hand side of ~ must contain the response variable as named in the atomic vector storing its values or in the data set containing it.\nThe right-hand side specifies the predictors and will therefore be a combination of potentially transformed variables from your data set (or from atomic vectors defined in your R environment). To get the proper syntax for the rhs of the formula, you should be know the set of allowed operators that have a specific interpretation by R when used within a formula:\n\n+ for adding terms.\n- for removing terms.\n: for adding interaction terms only.\n* for crossing variables, i.e. adding variables and their interactions.\n%in% for nesting variables, i.e. adding one variable and its interaction with another.\n^ for limiting variable crossing to a specified order\n. for adding all other variables in the matrix that have not yet been included in the model."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification-continued",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification-continued",
    "title": "Linear Regression with R",
    "section": "Model specification (continued)",
    "text": "Model specification (continued)\n\nLinear regression assumes that the response variable (y) and at least one predictor (x) are continuous variables. The following table summarizes the effect of operators when adding a variable z to the predictors. When z is categorical, we assume that it can take \\(h\\) unique possible values \\(z_0, z_1, \\dots, z_{h-1}\\).\n\n\n\n\n\n\n\n\nType of z\nFormula\nModel\n\n\n\n\nContinuous\ny ~ x + z\n\\(Y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\varepsilon\\)\n\n\nContinuous\ny ~ x : z\n\\(Y = \\beta_0 + \\beta_1 x z + \\varepsilon\\)\n\n\nContinuous\ny ~ x * z\n\\(Y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 x z + \\varepsilon\\)\n\n\nCategorical\ny ~ x + z\n\\(Y = \\beta_0^{(0)} + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(0)} \\delta_{\\{z = z_\\ell\\}} + \\beta_1^{(1)} x + \\varepsilon\\)\n\n\nCategorical\ny ~ x : z\n\\(Y = \\beta_0^{(0)} + \\beta_1^{(1)} x + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(1)} x \\delta_{\\{z = z_\\ell\\}} + \\varepsilon\\)\n\n\nCategorical\ny ~ x * z\n\\(Y = \\beta_0^{(0)} + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(0)} \\delta_{\\{z = z_\\ell\\}} + \\beta_0^{(1)} x + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(1)} x \\delta_{\\{z = z_\\ell\\}} + \\varepsilon\\)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#i",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#i",
    "title": "Linear Regression with R",
    "section": "I()",
    "text": "I()\n\nWhat strikes from the previous table is that the natural multiplication of variables within a formula does not simply adds the product of the predictors in the model but also the predictors themselves.\nWhat if you want to actually perform an arithmetic operation on your predictors to include a transformed predictor in your model? For example, you might want to include both \\(x\\) and \\(x^2\\) in your model. You have two options:\n\n\n\nYou compute all of the (possibly transformed) predictors you want to include in your model beforehand and store them in the data set (remember dplyr::mutate() which will help you achieve this goal easily); or,\nYou use the as-is operator I(). In this case, you instruct lm() that the operations declared within I() must be performed outside from the formula environment and before generating the design matrix for the model."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effective-model-summaries",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effective-model-summaries",
    "title": "Linear Regression with R",
    "section": "Effective Model Summaries",
    "text": "Effective Model Summaries\n\nThere are two possible software suites:\n\neither jtools and interactions packages:\nor ggeffects and sjPlot packages.\n\nThey both provide tools for summarizing and visualising models, marginal effects, interactions and model predictions.\n\n\n# install.packages(c(\"jtools\", \"interactions\"))\n# install.packages(c(\"ggeffects\", \"sjPlot\"))\nfit1 &lt;- lm(\n  metascore ~ imdb_rating + log(us_gross) + genre5, \n  data = jtools::movies\n)\nfit2 &lt;- lm(\n  metascore ~ imdb_rating + log(us_gross) + log(budget) + genre5, \n  data = jtools::movies\n)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-12",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-12",
    "title": "Linear Regression with R",
    "section": "Tabular summary (1/2)",
    "text": "Tabular summary (1/2)\n\njtools::summ(fit1)\n\n\n\n\n\nObservations\n831 (10 missing obs. deleted)\n\n\nDependent variable\nmetascore\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(6,824)\n169.37\n\n\nR²\n0.55\n\n\nAdj. R²\n0.55\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-39.96\n5.92\n-6.75\n0.00\n\n\nimdb_rating\n12.80\n0.49\n25.89\n0.00\n\n\nlog(us_gross)\n0.47\n0.31\n1.52\n0.13\n\n\ngenre5Comedy\n6.32\n1.06\n5.95\n0.00\n\n\ngenre5Drama\n7.66\n1.08\n7.12\n0.00\n\n\ngenre5Horror/Thriller\n-0.73\n1.51\n-0.48\n0.63\n\n\ngenre5Other\n5.86\n3.25\n1.80\n0.07\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-22",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-22",
    "title": "Linear Regression with R",
    "section": "Tabular summary (2/2)",
    "text": "Tabular summary (2/2)\n\njtools::export_summs(\n  fit1, fit2, \n  error_format = \"[{conf.low}, {conf.high}]\", error_pos = \"right\"\n)\n\n\n\n\nModel 1Model 2\n\n\n\n(Intercept)-39.96 ***[-51.58, -28.34]-16.47 *  [-31.16, -1.78]\n\nimdb_rating12.80 ***[11.83, 13.77]12.28 ***[11.30, 13.26]\n\nlog(us_gross)0.47    [-0.14, 1.07]1.60 ***[0.86, 2.34]\n\ngenre5Comedy6.32 ***[4.24, 8.41]4.84 ***[2.70, 6.97]\n\ngenre5Drama7.66 ***[5.55, 9.77]6.71 ***[4.60, 8.83]\n\ngenre5Horror/Thriller-0.73    [-3.70, 2.24]-2.81    [-5.84, 0.23]\n\ngenre5Other5.86    [-0.52, 12.24]6.30 *  [0.01, 12.59]\n\nlog(budget)           -2.25 ***[-3.13, -1.37]\n\nN831           831           \n\nR20.55        0.57        \n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#visual-summary",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#visual-summary",
    "title": "Linear Regression with R",
    "section": "Visual summary",
    "text": "Visual summary\n\njtools::plot_summs(fit1, fit2, inner_ci_level = .9)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---continuous-predictor",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---continuous-predictor",
    "title": "Linear Regression with R",
    "section": "Effect plot - Continuous predictor",
    "text": "Effect plot - Continuous predictor\n\nfit &lt;- lm(cty ~ displ + year + cyl + class + fl, data = mpg)\nfit_poly &lt;- lm(cty ~ poly(displ, 2) + year + cyl + class + fl, data = mpg)\n\n\n\n\njtools::effect_plot(fit, pred = displ, interval = TRUE, plot.points = TRUE)\n\n\n\n\n\n\n\n\n\njtools::effect_plot(fit_poly, pred = displ, interval = TRUE, plot.points = TRUE)\n\n\n\n\n\n\n\n\n\n\njtools::effect_plot(fit, pred = displ, interval = TRUE, partial.residuals = TRUE)\n\n\n\n\n\n\n\n\n\njtools::effect_plot(fit_poly, pred = displ, interval = TRUE, partial.residuals = TRUE)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---categorical-predictor",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---categorical-predictor",
    "title": "Linear Regression with R",
    "section": "Effect plot - Categorical predictor",
    "text": "Effect plot - Categorical predictor\n\njtools::effect_plot(fit, pred = fl, interval = TRUE, partial.residuals = TRUE, jitter = .2)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#interaction-plots",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#interaction-plots",
    "title": "Linear Regression with R",
    "section": "Interaction plots",
    "text": "Interaction plots\nThis is handled by the interaction package.\n\nJohnson-Neyman plot: sim_slopes()\nTwo-way interaction with at least one continuous predictor: interact_plot()\nTwo-way interaction between categorical predictors: cat_plot()"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#collinearity-multicollinearity",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#collinearity-multicollinearity",
    "title": "Linear Regression with R",
    "section": "Collinearity & Multicollinearity",
    "text": "Collinearity & Multicollinearity\n\n(Multi)-collinearity refers to high correlation in two or more independent variables in the regression model.\nMulticollinearity can arise from poorly designed experiments (Data-based multicollinearity) or from creating new independent variables related to the existing ones (structural multicollinearity).\nCorrelation plots are a great tool to explore colinearity between two variables. They can be seamlessly computed and visualized using packages such as corrr or ggcorrplot."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-13",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-13",
    "title": "Linear Regression with R",
    "section": "Variation Inflation Factor (1/3)",
    "text": "Variation Inflation Factor (1/3)\nThe Variance inflation factor (VIF) measures the degree of multicollinearity or collinearity in the regression model.\n\\[\n\\mathrm{VIF}_i = \\frac{1}{1 - R_i^2},\n\\]\nwhere \\(R_i^2\\) is the multiple correlation coefficient associated to the regression of \\(X_i\\) against all remaining independent variables."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-23",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-23",
    "title": "Linear Regression with R",
    "section": "Variation Inflation Factor (2/3)",
    "text": "Variation Inflation Factor (2/3)\nYou can add VIFs to the summary of a model as follows:\n\njtools::summ(fit, vifs = TRUE)\n\n\n\n\n\nObservations\n234\n\n\nDependent variable\ncty\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(13,220)\n100.37\n\n\nR²\n0.86\n\n\nAdj. R²\n0.85\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\nVIF\n\n\n\n\n(Intercept)\n-199.79\n51.14\n-3.91\n0.00\nNA\n\n\ndispl\n-1.02\n0.29\n-3.53\n0.00\n11.76\n\n\nyear\n0.12\n0.03\n4.52\n0.00\n1.11\n\n\ncyl\n-0.85\n0.20\n-4.26\n0.00\n8.69\n\n\nclasscompact\n-2.81\n1.00\n-2.83\n0.01\n3.69\n\n\nclassmidsize\n-2.95\n0.97\n-3.05\n0.00\n3.69\n\n\nclassminivan\n-5.08\n1.05\n-4.82\n0.00\n3.69\n\n\nclasspickup\n-5.89\n0.92\n-6.37\n0.00\n3.69\n\n\nclasssubcompact\n-2.63\n1.01\n-2.61\n0.01\n3.69\n\n\nclasssuv\n-5.59\n0.88\n-6.33\n0.00\n3.69\n\n\nfld\n5.93\n1.85\n3.21\n0.00\n1.60\n\n\nfle\n-5.13\n1.81\n-2.84\n0.00\n1.60\n\n\nflp\n-2.97\n1.72\n-1.73\n0.08\n1.60\n\n\nflr\n-1.69\n1.70\n-0.99\n0.32\n1.60\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-33",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-33",
    "title": "Linear Regression with R",
    "section": "Variation Inflation Factor (3/3)",
    "text": "Variation Inflation Factor (3/3)\n\nVIFs are always at least equal to 1.\nIn some domains, VIF over 2 is worthy of suspicion. Others set the bar higher, at 5 or 10. Others still will say you shouldn’t pay attention to these at all.\nSmall effects are more likely to be drowned out by higher VIFs, but this may just be a natural, unavoidable fact with your model (e.g., there is no problem with high VIFs when you have an interaction effect)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-selection",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-selection",
    "title": "Linear Regression with R",
    "section": "Model selection",
    "text": "Model selection\n\n\n\n\n\n\n\nForward selection\n\n\n\nStart with no variables in the model.\nTest the addition of each variable using a chosen model fit criterion1, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit\nRepeat until none improves the model to a statistically significant extent.\n\n\n\n\n\n\n\n\n\n\n\n\nBackward elimination\n\n\n\nStart with all candidate variables\nTest the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit\nRepeat until no further variables can be deleted without a statistically significant loss of fit.\n\n\n\n\n\n\n\nA list of possible and often used model fit criterion is available on the Wikipedia Model Selection page. In R, the basic stats::step() function uses the Akaike’s information criterion (AIC) and allows to perform forward selection (direction = \"forward\") or backward elimination (direction = \"backward\")."
  },
  {
    "objectID": "12_PCA/12-PCA-Solutions.html",
    "href": "12_PCA/12-PCA-Solutions.html",
    "title": "PCA - Solutions",
    "section": "",
    "text": "This data set contains:\n\nmonthly recordings of the temperatures of European capitals on a specific year;\nGPS coordinates of each city;\nthermal amplitude: difference between maximal and minimal temperature;\nannual average temperature;\nEuropean area: South, North, West or East.\n\nPerform a PCA to unravel patterns of temperature and which cities follow them.\n\n\nLet us begin by importing the data. After a quick inspection of the raw data file in a text editor, we find that:\n\nthe data is separated by semicolons;\nthe first column contains the city names and have trailing whitespaces.\n\nWe can use the readr::read_delim() function to import the data as follows:\n\ntemperature &lt;- readr::read_delim(\n  \"12-PCA-Data/temperature.csv\", \n  delim = \";\", \n  trim_ws = TRUE,\n  show_col_types = FALSE\n)\n\nNew names:\n• `` -&gt; `...1`\n\ntemperature\n\n# A tibble: 35 × 18\n   ...1  January February March April   May  June  July August September October\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amst…     2.9      2.5   5.7   8.2  12.5  14.8  17.1   17.1      14.5    11.4\n 2 Athe…     9.1      9.7  11.7  15.4  20.1  24.5  27.4   27.2      23.8    19.2\n 3 Berl…    -0.2      0.1   4.4   8.2  13.8  16    18.3   18        14.4    10  \n 4 Brus…     3.3      3.3   6.7   8.9  12.8  15.6  17.8   17.8      15      11.1\n 5 Buda…    -1.1      0.8   5.5  11.6  17    20.2  22     21.3      16.9    11.3\n 6 Cope…    -0.4     -0.4   1.3   5.8  11.1  15.4  17.1   16.6      13.3     8.8\n 7 Dubl…     4.8      5     5.9   7.8  10.4  13.3  15     14.6      12.7     9.7\n 8 Elsi…    -5.8     -6.2  -2.7   3.1  10.2  14    17.2   14.9       9.7     5.2\n 9 Kiev     -5.9     -5    -0.3   7.4  14.3  17.8  19.4   18.5      13.7     7.5\n10 Krak…    -3.7     -2     1.9   7.9  13.2  16.9  18.4   17.6      13.7     8.6\n# ℹ 25 more rows\n# ℹ 7 more variables: November &lt;dbl&gt;, December &lt;dbl&gt;, Annual &lt;dbl&gt;,\n#   Amplitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, Area &lt;chr&gt;\n\n\nWe notice that the first variable has no name. We can rename it using the dplyr::rename() function as follows:\n\ntemperature &lt;- dplyr::rename(temperature, city = ...1)\ntemperature\n\n# A tibble: 35 × 18\n   city  January February March April   May  June  July August September October\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amst…     2.9      2.5   5.7   8.2  12.5  14.8  17.1   17.1      14.5    11.4\n 2 Athe…     9.1      9.7  11.7  15.4  20.1  24.5  27.4   27.2      23.8    19.2\n 3 Berl…    -0.2      0.1   4.4   8.2  13.8  16    18.3   18        14.4    10  \n 4 Brus…     3.3      3.3   6.7   8.9  12.8  15.6  17.8   17.8      15      11.1\n 5 Buda…    -1.1      0.8   5.5  11.6  17    20.2  22     21.3      16.9    11.3\n 6 Cope…    -0.4     -0.4   1.3   5.8  11.1  15.4  17.1   16.6      13.3     8.8\n 7 Dubl…     4.8      5     5.9   7.8  10.4  13.3  15     14.6      12.7     9.7\n 8 Elsi…    -5.8     -6.2  -2.7   3.1  10.2  14    17.2   14.9       9.7     5.2\n 9 Kiev     -5.9     -5    -0.3   7.4  14.3  17.8  19.4   18.5      13.7     7.5\n10 Krak…    -3.7     -2     1.9   7.9  13.2  16.9  18.4   17.6      13.7     8.6\n# ℹ 25 more rows\n# ℹ 7 more variables: November &lt;dbl&gt;, December &lt;dbl&gt;, Annual &lt;dbl&gt;,\n#   Amplitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, Area &lt;chr&gt;\n\n\n\n\n\nThe skimr::skim() function provides a nice summary of the data:\n\nskimr::skim(temperature)\n\n\nData summary\n\n\nName\ntemperature\n\n\nNumber of rows\n35\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n4\n14\n0\n35\n0\n\n\nArea\n0\n1\n4\n5\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nJanuary\n0\n1\n1.35\n5.50\n-9.3\n-1.55\n0.2\n4.90\n10.7\n▅▅▇▇▆\n\n\nFebruary\n0\n1\n2.22\n5.50\n-7.9\n-0.15\n1.9\n5.80\n11.8\n▂▂▇▂▃\n\n\nMarch\n0\n1\n5.23\n4.86\n-3.7\n1.60\n5.4\n8.50\n14.1\n▃▂▇▃▃\n\n\nApril\n0\n1\n9.28\n3.81\n2.9\n7.25\n8.9\n12.05\n16.9\n▅▇▆▆▃\n\n\nMay\n0\n1\n13.91\n3.27\n6.5\n12.15\n13.8\n16.35\n20.9\n▁▃▇▃▂\n\n\nJune\n0\n1\n17.41\n3.32\n9.3\n15.40\n16.9\n19.80\n24.5\n▁▃▇▃▂\n\n\nJuly\n0\n1\n19.62\n3.57\n11.1\n17.30\n18.9\n21.75\n27.4\n▁▅▇▂▃\n\n\nAugust\n0\n1\n18.98\n3.73\n10.6\n16.65\n18.3\n21.60\n27.2\n▁▇▇▂▃\n\n\nSeptember\n0\n1\n15.63\n4.11\n7.9\n13.00\n14.8\n18.25\n24.3\n▂▇▇▂▃\n\n\nOctober\n0\n1\n11.00\n4.32\n4.5\n8.65\n10.2\n13.30\n19.4\n▅▇▆▂▅\n\n\nNovember\n0\n1\n6.07\n4.57\n-1.1\n3.20\n5.1\n7.90\n14.9\n▆▇▆▂▅\n\n\nDecember\n0\n1\n2.88\n4.97\n-6.0\n0.25\n1.7\n5.40\n12.0\n▃▇▇▃▅\n\n\nAnnual\n0\n1\n10.27\n3.96\n4.5\n7.75\n9.7\n12.65\n18.2\n▅▇▃▁▃\n\n\nAmplitude\n0\n1\n18.32\n4.51\n10.2\n14.90\n18.5\n21.45\n27.6\n▃▇▇▆▃\n\n\nLatitude\n0\n1\n49.04\n7.10\n37.2\n43.90\n50.0\n53.35\n64.1\n▆▅▇▃▃\n\n\nLongitude\n0\n1\n13.01\n9.70\n0.0\n5.05\n10.5\n19.30\n37.6\n▇▇▃▂▂\n\n\n\n\n\nWe learn from it that the data contains 18 variables and 35 observations. The skimr::skim() function also provides a nice summary of the data types and the number of missing values. We can see that the city and Area variables are character vectors and that there are no missing values.\n\n\n\nThe first 13 columns are the temperature measurements for each month. We can use the dplyr::select() function to select these columns and the city column as identifier:\n\ntemperature_sub &lt;- dplyr::select(temperature, 1:13)\ntemperature_sub\n\n# A tibble: 35 × 13\n   city  January February March April   May  June  July August September October\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amst…     2.9      2.5   5.7   8.2  12.5  14.8  17.1   17.1      14.5    11.4\n 2 Athe…     9.1      9.7  11.7  15.4  20.1  24.5  27.4   27.2      23.8    19.2\n 3 Berl…    -0.2      0.1   4.4   8.2  13.8  16    18.3   18        14.4    10  \n 4 Brus…     3.3      3.3   6.7   8.9  12.8  15.6  17.8   17.8      15      11.1\n 5 Buda…    -1.1      0.8   5.5  11.6  17    20.2  22     21.3      16.9    11.3\n 6 Cope…    -0.4     -0.4   1.3   5.8  11.1  15.4  17.1   16.6      13.3     8.8\n 7 Dubl…     4.8      5     5.9   7.8  10.4  13.3  15     14.6      12.7     9.7\n 8 Elsi…    -5.8     -6.2  -2.7   3.1  10.2  14    17.2   14.9       9.7     5.2\n 9 Kiev     -5.9     -5    -0.3   7.4  14.3  17.8  19.4   18.5      13.7     7.5\n10 Krak…    -3.7     -2     1.9   7.9  13.2  16.9  18.4   17.6      13.7     8.6\n# ℹ 25 more rows\n# ℹ 2 more variables: November &lt;dbl&gt;, December &lt;dbl&gt;\n\n\nNext, we use the tidyr::pivot_longer() function to reshape the data into a long format which is required for visualization with ggplot2.\n\ntemperature_sub &lt;- tidyr::pivot_longer(\n  temperature_sub, \n  cols = -city, \n  names_to = \"month\", \n  values_to = \"temperature\"\n)\ntemperature_sub\n\n# A tibble: 420 × 3\n   city      month     temperature\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 Amsterdam January           2.9\n 2 Amsterdam February          2.5\n 3 Amsterdam March             5.7\n 4 Amsterdam April             8.2\n 5 Amsterdam May              12.5\n 6 Amsterdam June             14.8\n 7 Amsterdam July             17.1\n 8 Amsterdam August           17.1\n 9 Amsterdam September        14.5\n10 Amsterdam October          11.4\n# ℹ 410 more rows\n\n\nNext, we convert the month column to a proper date-time format using the lubridate::parse_date_time() function:\n\ntemperature_sub &lt;- dplyr::mutate(\n  temperature_sub, \n  month = lubridate::parse_date_time(month, orders = \"m\")\n)\ntemperature_sub\n\n# A tibble: 420 × 3\n   city      month               temperature\n   &lt;chr&gt;     &lt;dttm&gt;                    &lt;dbl&gt;\n 1 Amsterdam 0000-01-01 00:00:00         2.9\n 2 Amsterdam 0000-02-01 00:00:00         2.5\n 3 Amsterdam 0000-03-01 00:00:00         5.7\n 4 Amsterdam 0000-04-01 00:00:00         8.2\n 5 Amsterdam 0000-05-01 00:00:00        12.5\n 6 Amsterdam 0000-06-01 00:00:00        14.8\n 7 Amsterdam 0000-07-01 00:00:00        17.1\n 8 Amsterdam 0000-08-01 00:00:00        17.1\n 9 Amsterdam 0000-09-01 00:00:00        14.5\n10 Amsterdam 0000-10-01 00:00:00        11.4\n# ℹ 410 more rows\n\n\nNow we can plot the temperature measurements for each city using the ggplot2::ggplot() function. We also use the scales::scale_x_datetime() function to format the x-axis labels to only show the month names:\n\ntemperature_sub |&gt;\n  ggplot(aes(x = month, y = temperature, group = city)) +\n  geom_line() +\n  scale_x_datetime(labels = scales::date_format(\"%b\")) +\n  theme_bw() + \n  labs(\n    x = \"Month\", \n    y = \"Temperature (°C)\", \n    color = \"City\"\n  ) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIt is hard to see from this plot which cities have similar temperature profiles. A principal component analysis (PCA) might help us to find out.\n\n\n\nIt is important that PCA be performed on exclusively numeric variables. Hence, we first need to use dplyr::select() to retain only the columns corresponding to the monthly temperatures:\n\nX &lt;- dplyr::select(temperature, 2:13)\n\nBefore performing PCA, it is important to standardize the data. For this purpose, we can use the base::scale() function:\n\nX &lt;- scale(X)\n\nNow, we are going to use FactoMineR::PCA() to perform the PCA and the factoextra package for visualization of the results. When visualizing observations, the package uses the rownames of the input data frame to label the observations. Hence, we anticipate here and affect the city names as rownames of the data set:\n\nrownames(X) &lt;- temperature$city\n\nNow we can perform the PCA using the FactorMineR::PCA() function:\n\npca_results &lt;- FactoMineR::PCA(X, graph = FALSE)\n\n\n\nWe can also use the factoextra::fviz_screeplot() function to plot the eigenvalues:\n\nfactoextra::fviz_screeplot(pca_results, addlabels = TRUE)\n\n\n\n\n\n\n\n\nAnd the eigenvalues can be extracted with:\n\nfactoextra::get_eigenvalue(pca_results)\n\n         eigenvalue variance.percent cumulative.variance.percent\nDim.1  1.042445e+01     86.870441346                    86.87044\nDim.2  1.370499e+00     11.420823117                    98.29126\nDim.3  1.205076e-01      1.004230241                    99.29549\nDim.4  4.233298e-02      0.352774838                    99.64827\nDim.5  2.292280e-02      0.191023370                    99.83929\nDim.6  8.684234e-03      0.072368614                    99.91166\nDim.7  4.178064e-03      0.034817200                    99.94648\nDim.8  2.930325e-03      0.024419371                    99.97090\nDim.9  1.475750e-03      0.012297915                    99.98320\nDim.10 8.529732e-04      0.007108110                    99.99030\nDim.11 7.862929e-04      0.006552441                    99.99686\nDim.12 3.772122e-04      0.003143435                   100.00000\n\n\nThis suggests that the first two principal components explain almost all the variance in the data.\n\n\n\nWe can use the factoextra::fviz_pca_var() function to visualize the coordinates of the variables in the proposed reduced space consisting of the first two principal directions:\n\nfactoextra::fviz_pca_var(pca_results)\n\n\n\n\n\n\n\n\nWe see from the above correlation circle that:\n\nthe first principal component seems to produce a rather uniformly weighted average temperature over the year; a high score on this component means a high average temperature over the year.\nthe second principal component instead contrasts temperatures between the warmest months (summer months) and the coldest months (winter months) hence somehow providing an estimate of the range between maximal and mininal temperature; a high score on this component means an elevated difference between maximal and minimal temperature over the year.\n\nWe can assess whether each original variable is well represented by the first two principal components by looking at the cos2 values:\n\ncos2 &lt;- factoextra::get_pca_var(pca_results)$cos2[, 1:2]\ncorrplot::corrplot(cos2, is.corr = FALSE)\n\n\n\n\n\n\n\n\nThe original variables are indeed well represented. They even would all be well represented by the first component only.\nWe can confirm the interpretation of the first two principal components that we were able to give from the correlation circle by looking at the contribution of each original variable to the principal components:\n\ncontrib &lt;- factoextra::get_pca_var(pca_results)$contrib[, 1:2]\ncorrplot::corrplot(contrib, is.corr = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nWe can use the factoextra::fviz_pca_ind() function to visualize the cities in the plane defined by the first two principal components:\n\nfactoextra::fviz_pca_ind(pca_results)\n\n\n\n\n\n\n\n\nThe plane is naturally divided into 4 panels:\n\nThe top left panel groups cities with negative scores on the first component but positive scores on the second component reflecting rather low average temperature but relatively high thermal amplitude.\nThe top right panel groups cities with positive scores on the first component and positive scores on the second component reflecting rather high average temperature and relatively high thermal amplitude.\nThe bottom left panel groups cities with negative scores on the first component and negative scores on the second component reflecting rather low average temperature and relatively low thermal amplitude.\nThe bottom right panel groups cities with positive scores on the first component but negative scores on the second component reflecting rather high average temperature but relatively low thermal amplitude.\n\nWe can also add colors to the plot to see if the area of the cities is correlated with the temperature profiles:\n\nfactoextra::fviz_pca_ind(\n  pca_results,\n  col.ind = temperature$Area, # color by groups\n  palette = viridis::viridis(4),\n  legend.title = \"By Area\"\n)\n\n\n\n\n\n\n\n\nThis is interesting as it reflects that:\n\nwestern capitals have a medium average temperature and medium thermal amplitude.\nnothern capitals are grouped in the bottom left panel.\neastern capitals are grouped in the top left panel.\nsouthern capitals are grouped on the right side with cities that are therefore globally warm but with some of them featuring a high thermal amplitude like Milan while others featuring a more stable temperature over the year like Lisbon.\n\n\n\n\n\n\n\nInterpretation of units\n\n\n\nThe fact that some cities have a negative score on the second principal component does not mean that winter achieves higher temperature w.r.t. summer.\nRecall that temperatures have been standardized across cities. Hence, if you want to interpret the scores in terms of temperatures, you should do it by hand with something like:\n\norig_temp &lt;- as.matrix(temperature[, 2:13])\nloadings &lt;- factoextra::get_pca_var(pca_results)$coord[, 1:2]\nscores &lt;- orig_temp %*% loadings\nscores_tbl &lt;- tibble::tibble(\n  City = temperature$city, \n  PC1 = scores[, 1], \n  PC2 = scores[, 2]\n)\ngt::gt(scores_tbl)\n\n\n\n\n\n\n\nCity\nPC1\nPC2\n\n\n\n\nAmsterdam\n109.75002\n20.07679\n\n\nAthens\n198.59930\n25.31486\n\n\nBerlin\n100.57744\n26.73047\n\n\nBrussels\n114.61657\n20.60132\n\n\nBudapest\n121.84567\n33.06963\n\n\nCopenhagen\n87.01508\n25.23984\n\n\nDublin\n103.31766\n14.35165\n\n\nElsinki\n52.66654\n31.43762\n\n\nKiev\n78.60049\n36.91400\n\n\nKrakow\n86.57091\n31.42295\n\n\nLisbon\n177.96301\n15.27917\n\n\nLondon\n108.28743\n19.03678\n\n\nMadrid\n154.79387\n26.79321\n\n\nMinsk\n60.59012\n35.15569\n\n\nMoscow\n56.10806\n38.98164\n\n\nOslo\n62.27592\n30.17793\n\n\nParis\n123.96285\n21.37080\n\n\nPrague\n101.58740\n29.64837\n\n\nReykjavik\n51.03193\n16.05866\n\n\nRome\n171.82598\n23.94100\n\n\nSarajevo\n105.54795\n27.96790\n\n\nSofia\n107.82834\n29.99347\n\n\nStockholm\n64.97559\n28.84649\n\n\nAntwerp\n113.97189\n20.71773\n\n\nBarcelona\n180.85690\n20.44746\n\n\nBordeaux\n141.80864\n20.95166\n\n\nEdinburgh\n92.78014\n16.47195\n\n\nFrankfurt\n108.90323\n26.73563\n\n\nGeneva\n108.15154\n26.93270\n\n\nGenoa\n178.52320\n21.81177\n\n\nMilan\n141.14609\n31.35509\n\n\nPalermo\n196.51068\n19.84736\n\n\nSeville\n203.30276\n22.04190\n\n\nSt. Petersburg\n50.62120\n36.91273\n\n\nZurich\n96.74811\n26.57796"
  },
  {
    "objectID": "12_PCA/12-PCA-Solutions.html#the-temperature.csv-data-set",
    "href": "12_PCA/12-PCA-Solutions.html#the-temperature.csv-data-set",
    "title": "PCA - Solutions",
    "section": "",
    "text": "This data set contains:\n\nmonthly recordings of the temperatures of European capitals on a specific year;\nGPS coordinates of each city;\nthermal amplitude: difference between maximal and minimal temperature;\nannual average temperature;\nEuropean area: South, North, West or East.\n\nPerform a PCA to unravel patterns of temperature and which cities follow them.\n\n\nLet us begin by importing the data. After a quick inspection of the raw data file in a text editor, we find that:\n\nthe data is separated by semicolons;\nthe first column contains the city names and have trailing whitespaces.\n\nWe can use the readr::read_delim() function to import the data as follows:\n\ntemperature &lt;- readr::read_delim(\n  \"12-PCA-Data/temperature.csv\", \n  delim = \";\", \n  trim_ws = TRUE,\n  show_col_types = FALSE\n)\n\nNew names:\n• `` -&gt; `...1`\n\ntemperature\n\n# A tibble: 35 × 18\n   ...1  January February March April   May  June  July August September October\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amst…     2.9      2.5   5.7   8.2  12.5  14.8  17.1   17.1      14.5    11.4\n 2 Athe…     9.1      9.7  11.7  15.4  20.1  24.5  27.4   27.2      23.8    19.2\n 3 Berl…    -0.2      0.1   4.4   8.2  13.8  16    18.3   18        14.4    10  \n 4 Brus…     3.3      3.3   6.7   8.9  12.8  15.6  17.8   17.8      15      11.1\n 5 Buda…    -1.1      0.8   5.5  11.6  17    20.2  22     21.3      16.9    11.3\n 6 Cope…    -0.4     -0.4   1.3   5.8  11.1  15.4  17.1   16.6      13.3     8.8\n 7 Dubl…     4.8      5     5.9   7.8  10.4  13.3  15     14.6      12.7     9.7\n 8 Elsi…    -5.8     -6.2  -2.7   3.1  10.2  14    17.2   14.9       9.7     5.2\n 9 Kiev     -5.9     -5    -0.3   7.4  14.3  17.8  19.4   18.5      13.7     7.5\n10 Krak…    -3.7     -2     1.9   7.9  13.2  16.9  18.4   17.6      13.7     8.6\n# ℹ 25 more rows\n# ℹ 7 more variables: November &lt;dbl&gt;, December &lt;dbl&gt;, Annual &lt;dbl&gt;,\n#   Amplitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, Area &lt;chr&gt;\n\n\nWe notice that the first variable has no name. We can rename it using the dplyr::rename() function as follows:\n\ntemperature &lt;- dplyr::rename(temperature, city = ...1)\ntemperature\n\n# A tibble: 35 × 18\n   city  January February March April   May  June  July August September October\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amst…     2.9      2.5   5.7   8.2  12.5  14.8  17.1   17.1      14.5    11.4\n 2 Athe…     9.1      9.7  11.7  15.4  20.1  24.5  27.4   27.2      23.8    19.2\n 3 Berl…    -0.2      0.1   4.4   8.2  13.8  16    18.3   18        14.4    10  \n 4 Brus…     3.3      3.3   6.7   8.9  12.8  15.6  17.8   17.8      15      11.1\n 5 Buda…    -1.1      0.8   5.5  11.6  17    20.2  22     21.3      16.9    11.3\n 6 Cope…    -0.4     -0.4   1.3   5.8  11.1  15.4  17.1   16.6      13.3     8.8\n 7 Dubl…     4.8      5     5.9   7.8  10.4  13.3  15     14.6      12.7     9.7\n 8 Elsi…    -5.8     -6.2  -2.7   3.1  10.2  14    17.2   14.9       9.7     5.2\n 9 Kiev     -5.9     -5    -0.3   7.4  14.3  17.8  19.4   18.5      13.7     7.5\n10 Krak…    -3.7     -2     1.9   7.9  13.2  16.9  18.4   17.6      13.7     8.6\n# ℹ 25 more rows\n# ℹ 7 more variables: November &lt;dbl&gt;, December &lt;dbl&gt;, Annual &lt;dbl&gt;,\n#   Amplitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, Area &lt;chr&gt;\n\n\n\n\n\nThe skimr::skim() function provides a nice summary of the data:\n\nskimr::skim(temperature)\n\n\nData summary\n\n\nName\ntemperature\n\n\nNumber of rows\n35\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n4\n14\n0\n35\n0\n\n\nArea\n0\n1\n4\n5\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nJanuary\n0\n1\n1.35\n5.50\n-9.3\n-1.55\n0.2\n4.90\n10.7\n▅▅▇▇▆\n\n\nFebruary\n0\n1\n2.22\n5.50\n-7.9\n-0.15\n1.9\n5.80\n11.8\n▂▂▇▂▃\n\n\nMarch\n0\n1\n5.23\n4.86\n-3.7\n1.60\n5.4\n8.50\n14.1\n▃▂▇▃▃\n\n\nApril\n0\n1\n9.28\n3.81\n2.9\n7.25\n8.9\n12.05\n16.9\n▅▇▆▆▃\n\n\nMay\n0\n1\n13.91\n3.27\n6.5\n12.15\n13.8\n16.35\n20.9\n▁▃▇▃▂\n\n\nJune\n0\n1\n17.41\n3.32\n9.3\n15.40\n16.9\n19.80\n24.5\n▁▃▇▃▂\n\n\nJuly\n0\n1\n19.62\n3.57\n11.1\n17.30\n18.9\n21.75\n27.4\n▁▅▇▂▃\n\n\nAugust\n0\n1\n18.98\n3.73\n10.6\n16.65\n18.3\n21.60\n27.2\n▁▇▇▂▃\n\n\nSeptember\n0\n1\n15.63\n4.11\n7.9\n13.00\n14.8\n18.25\n24.3\n▂▇▇▂▃\n\n\nOctober\n0\n1\n11.00\n4.32\n4.5\n8.65\n10.2\n13.30\n19.4\n▅▇▆▂▅\n\n\nNovember\n0\n1\n6.07\n4.57\n-1.1\n3.20\n5.1\n7.90\n14.9\n▆▇▆▂▅\n\n\nDecember\n0\n1\n2.88\n4.97\n-6.0\n0.25\n1.7\n5.40\n12.0\n▃▇▇▃▅\n\n\nAnnual\n0\n1\n10.27\n3.96\n4.5\n7.75\n9.7\n12.65\n18.2\n▅▇▃▁▃\n\n\nAmplitude\n0\n1\n18.32\n4.51\n10.2\n14.90\n18.5\n21.45\n27.6\n▃▇▇▆▃\n\n\nLatitude\n0\n1\n49.04\n7.10\n37.2\n43.90\n50.0\n53.35\n64.1\n▆▅▇▃▃\n\n\nLongitude\n0\n1\n13.01\n9.70\n0.0\n5.05\n10.5\n19.30\n37.6\n▇▇▃▂▂\n\n\n\n\n\nWe learn from it that the data contains 18 variables and 35 observations. The skimr::skim() function also provides a nice summary of the data types and the number of missing values. We can see that the city and Area variables are character vectors and that there are no missing values.\n\n\n\nThe first 13 columns are the temperature measurements for each month. We can use the dplyr::select() function to select these columns and the city column as identifier:\n\ntemperature_sub &lt;- dplyr::select(temperature, 1:13)\ntemperature_sub\n\n# A tibble: 35 × 13\n   city  January February March April   May  June  July August September October\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Amst…     2.9      2.5   5.7   8.2  12.5  14.8  17.1   17.1      14.5    11.4\n 2 Athe…     9.1      9.7  11.7  15.4  20.1  24.5  27.4   27.2      23.8    19.2\n 3 Berl…    -0.2      0.1   4.4   8.2  13.8  16    18.3   18        14.4    10  \n 4 Brus…     3.3      3.3   6.7   8.9  12.8  15.6  17.8   17.8      15      11.1\n 5 Buda…    -1.1      0.8   5.5  11.6  17    20.2  22     21.3      16.9    11.3\n 6 Cope…    -0.4     -0.4   1.3   5.8  11.1  15.4  17.1   16.6      13.3     8.8\n 7 Dubl…     4.8      5     5.9   7.8  10.4  13.3  15     14.6      12.7     9.7\n 8 Elsi…    -5.8     -6.2  -2.7   3.1  10.2  14    17.2   14.9       9.7     5.2\n 9 Kiev     -5.9     -5    -0.3   7.4  14.3  17.8  19.4   18.5      13.7     7.5\n10 Krak…    -3.7     -2     1.9   7.9  13.2  16.9  18.4   17.6      13.7     8.6\n# ℹ 25 more rows\n# ℹ 2 more variables: November &lt;dbl&gt;, December &lt;dbl&gt;\n\n\nNext, we use the tidyr::pivot_longer() function to reshape the data into a long format which is required for visualization with ggplot2.\n\ntemperature_sub &lt;- tidyr::pivot_longer(\n  temperature_sub, \n  cols = -city, \n  names_to = \"month\", \n  values_to = \"temperature\"\n)\ntemperature_sub\n\n# A tibble: 420 × 3\n   city      month     temperature\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 Amsterdam January           2.9\n 2 Amsterdam February          2.5\n 3 Amsterdam March             5.7\n 4 Amsterdam April             8.2\n 5 Amsterdam May              12.5\n 6 Amsterdam June             14.8\n 7 Amsterdam July             17.1\n 8 Amsterdam August           17.1\n 9 Amsterdam September        14.5\n10 Amsterdam October          11.4\n# ℹ 410 more rows\n\n\nNext, we convert the month column to a proper date-time format using the lubridate::parse_date_time() function:\n\ntemperature_sub &lt;- dplyr::mutate(\n  temperature_sub, \n  month = lubridate::parse_date_time(month, orders = \"m\")\n)\ntemperature_sub\n\n# A tibble: 420 × 3\n   city      month               temperature\n   &lt;chr&gt;     &lt;dttm&gt;                    &lt;dbl&gt;\n 1 Amsterdam 0000-01-01 00:00:00         2.9\n 2 Amsterdam 0000-02-01 00:00:00         2.5\n 3 Amsterdam 0000-03-01 00:00:00         5.7\n 4 Amsterdam 0000-04-01 00:00:00         8.2\n 5 Amsterdam 0000-05-01 00:00:00        12.5\n 6 Amsterdam 0000-06-01 00:00:00        14.8\n 7 Amsterdam 0000-07-01 00:00:00        17.1\n 8 Amsterdam 0000-08-01 00:00:00        17.1\n 9 Amsterdam 0000-09-01 00:00:00        14.5\n10 Amsterdam 0000-10-01 00:00:00        11.4\n# ℹ 410 more rows\n\n\nNow we can plot the temperature measurements for each city using the ggplot2::ggplot() function. We also use the scales::scale_x_datetime() function to format the x-axis labels to only show the month names:\n\ntemperature_sub |&gt;\n  ggplot(aes(x = month, y = temperature, group = city)) +\n  geom_line() +\n  scale_x_datetime(labels = scales::date_format(\"%b\")) +\n  theme_bw() + \n  labs(\n    x = \"Month\", \n    y = \"Temperature (°C)\", \n    color = \"City\"\n  ) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIt is hard to see from this plot which cities have similar temperature profiles. A principal component analysis (PCA) might help us to find out.\n\n\n\nIt is important that PCA be performed on exclusively numeric variables. Hence, we first need to use dplyr::select() to retain only the columns corresponding to the monthly temperatures:\n\nX &lt;- dplyr::select(temperature, 2:13)\n\nBefore performing PCA, it is important to standardize the data. For this purpose, we can use the base::scale() function:\n\nX &lt;- scale(X)\n\nNow, we are going to use FactoMineR::PCA() to perform the PCA and the factoextra package for visualization of the results. When visualizing observations, the package uses the rownames of the input data frame to label the observations. Hence, we anticipate here and affect the city names as rownames of the data set:\n\nrownames(X) &lt;- temperature$city\n\nNow we can perform the PCA using the FactorMineR::PCA() function:\n\npca_results &lt;- FactoMineR::PCA(X, graph = FALSE)\n\n\n\nWe can also use the factoextra::fviz_screeplot() function to plot the eigenvalues:\n\nfactoextra::fviz_screeplot(pca_results, addlabels = TRUE)\n\n\n\n\n\n\n\n\nAnd the eigenvalues can be extracted with:\n\nfactoextra::get_eigenvalue(pca_results)\n\n         eigenvalue variance.percent cumulative.variance.percent\nDim.1  1.042445e+01     86.870441346                    86.87044\nDim.2  1.370499e+00     11.420823117                    98.29126\nDim.3  1.205076e-01      1.004230241                    99.29549\nDim.4  4.233298e-02      0.352774838                    99.64827\nDim.5  2.292280e-02      0.191023370                    99.83929\nDim.6  8.684234e-03      0.072368614                    99.91166\nDim.7  4.178064e-03      0.034817200                    99.94648\nDim.8  2.930325e-03      0.024419371                    99.97090\nDim.9  1.475750e-03      0.012297915                    99.98320\nDim.10 8.529732e-04      0.007108110                    99.99030\nDim.11 7.862929e-04      0.006552441                    99.99686\nDim.12 3.772122e-04      0.003143435                   100.00000\n\n\nThis suggests that the first two principal components explain almost all the variance in the data.\n\n\n\nWe can use the factoextra::fviz_pca_var() function to visualize the coordinates of the variables in the proposed reduced space consisting of the first two principal directions:\n\nfactoextra::fviz_pca_var(pca_results)\n\n\n\n\n\n\n\n\nWe see from the above correlation circle that:\n\nthe first principal component seems to produce a rather uniformly weighted average temperature over the year; a high score on this component means a high average temperature over the year.\nthe second principal component instead contrasts temperatures between the warmest months (summer months) and the coldest months (winter months) hence somehow providing an estimate of the range between maximal and mininal temperature; a high score on this component means an elevated difference between maximal and minimal temperature over the year.\n\nWe can assess whether each original variable is well represented by the first two principal components by looking at the cos2 values:\n\ncos2 &lt;- factoextra::get_pca_var(pca_results)$cos2[, 1:2]\ncorrplot::corrplot(cos2, is.corr = FALSE)\n\n\n\n\n\n\n\n\nThe original variables are indeed well represented. They even would all be well represented by the first component only.\nWe can confirm the interpretation of the first two principal components that we were able to give from the correlation circle by looking at the contribution of each original variable to the principal components:\n\ncontrib &lt;- factoextra::get_pca_var(pca_results)$contrib[, 1:2]\ncorrplot::corrplot(contrib, is.corr = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nWe can use the factoextra::fviz_pca_ind() function to visualize the cities in the plane defined by the first two principal components:\n\nfactoextra::fviz_pca_ind(pca_results)\n\n\n\n\n\n\n\n\nThe plane is naturally divided into 4 panels:\n\nThe top left panel groups cities with negative scores on the first component but positive scores on the second component reflecting rather low average temperature but relatively high thermal amplitude.\nThe top right panel groups cities with positive scores on the first component and positive scores on the second component reflecting rather high average temperature and relatively high thermal amplitude.\nThe bottom left panel groups cities with negative scores on the first component and negative scores on the second component reflecting rather low average temperature and relatively low thermal amplitude.\nThe bottom right panel groups cities with positive scores on the first component but negative scores on the second component reflecting rather high average temperature but relatively low thermal amplitude.\n\nWe can also add colors to the plot to see if the area of the cities is correlated with the temperature profiles:\n\nfactoextra::fviz_pca_ind(\n  pca_results,\n  col.ind = temperature$Area, # color by groups\n  palette = viridis::viridis(4),\n  legend.title = \"By Area\"\n)\n\n\n\n\n\n\n\n\nThis is interesting as it reflects that:\n\nwestern capitals have a medium average temperature and medium thermal amplitude.\nnothern capitals are grouped in the bottom left panel.\neastern capitals are grouped in the top left panel.\nsouthern capitals are grouped on the right side with cities that are therefore globally warm but with some of them featuring a high thermal amplitude like Milan while others featuring a more stable temperature over the year like Lisbon.\n\n\n\n\n\n\n\nInterpretation of units\n\n\n\nThe fact that some cities have a negative score on the second principal component does not mean that winter achieves higher temperature w.r.t. summer.\nRecall that temperatures have been standardized across cities. Hence, if you want to interpret the scores in terms of temperatures, you should do it by hand with something like:\n\norig_temp &lt;- as.matrix(temperature[, 2:13])\nloadings &lt;- factoextra::get_pca_var(pca_results)$coord[, 1:2]\nscores &lt;- orig_temp %*% loadings\nscores_tbl &lt;- tibble::tibble(\n  City = temperature$city, \n  PC1 = scores[, 1], \n  PC2 = scores[, 2]\n)\ngt::gt(scores_tbl)\n\n\n\n\n\n\n\nCity\nPC1\nPC2\n\n\n\n\nAmsterdam\n109.75002\n20.07679\n\n\nAthens\n198.59930\n25.31486\n\n\nBerlin\n100.57744\n26.73047\n\n\nBrussels\n114.61657\n20.60132\n\n\nBudapest\n121.84567\n33.06963\n\n\nCopenhagen\n87.01508\n25.23984\n\n\nDublin\n103.31766\n14.35165\n\n\nElsinki\n52.66654\n31.43762\n\n\nKiev\n78.60049\n36.91400\n\n\nKrakow\n86.57091\n31.42295\n\n\nLisbon\n177.96301\n15.27917\n\n\nLondon\n108.28743\n19.03678\n\n\nMadrid\n154.79387\n26.79321\n\n\nMinsk\n60.59012\n35.15569\n\n\nMoscow\n56.10806\n38.98164\n\n\nOslo\n62.27592\n30.17793\n\n\nParis\n123.96285\n21.37080\n\n\nPrague\n101.58740\n29.64837\n\n\nReykjavik\n51.03193\n16.05866\n\n\nRome\n171.82598\n23.94100\n\n\nSarajevo\n105.54795\n27.96790\n\n\nSofia\n107.82834\n29.99347\n\n\nStockholm\n64.97559\n28.84649\n\n\nAntwerp\n113.97189\n20.71773\n\n\nBarcelona\n180.85690\n20.44746\n\n\nBordeaux\n141.80864\n20.95166\n\n\nEdinburgh\n92.78014\n16.47195\n\n\nFrankfurt\n108.90323\n26.73563\n\n\nGeneva\n108.15154\n26.93270\n\n\nGenoa\n178.52320\n21.81177\n\n\nMilan\n141.14609\n31.35509\n\n\nPalermo\n196.51068\n19.84736\n\n\nSeville\n203.30276\n22.04190\n\n\nSt. Petersburg\n50.62120\n36.91273\n\n\nZurich\n96.74811\n26.57796"
  },
  {
    "objectID": "12_PCA/12-PCA-Solutions.html#the-chicken.csv-data-set",
    "href": "12_PCA/12-PCA-Solutions.html#the-chicken.csv-data-set",
    "title": "PCA - Solutions",
    "section": "The chicken.csv data set",
    "text": "The chicken.csv data set\nIt regroups 43 chickens who went through six different diets:\n\nnormal diet (N),\nfast for 16h (F16),\nfast for 16h and back to normal diet for 5h (F16R5),\nfast for 16h and back to normal diet for 16h (F16R16),\nfast for 48h (F48),\nfast for 48h and back to normal diet for 24h (F48R24).\n\nAfter each process, chicken underwent gene expression sequencing and 7406 gene expressions were collected.\n\nCan we conclude that genes express differently according to the stress level?\nHow much time is needed for chicken to go back to normality in terms of gene expression?\n\n\nData import & Visualization\nA quick glance at the content of the CSV file shows that the delimiter is the semi-colon. We can therefore resort to readr::read_delim() to import the data:\n\ndf &lt;- readr::read_delim(\n  file = \"12-PCA-Data/chicken.csv\", \n  delim = \";\", \n  show_col_types = FALSE\n)\ndf\n\n# A tibble: 7,406 × 44\n   gene         N_1     N_2     N_3     N_4      N_6     N_7     j16_3   j16_4\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 A4GALT  -0.153   -0.0126 -0.140   0.208  -0.461   -0.217  -0.130    -0.295 \n 2 A4GNT    0.00873 -0.0130  0.0126  0.0713  0.515    0.158   0.600     0.171 \n 3 AACS     0.0426   0.173  -0.0219 -0.0788 -0.00269 -0.0894 -0.0548    0.0145\n 4 AADACL1  0.161    0.205   0.185   0.0576  0.307    0.309   0.000618 -0.0414\n 5 AADACL2 -0.374    0.0320 -0.411   0.0361 -0.490   -0.145  -0.112    -0.151 \n 6 AADACL3 -0.445    0.0638  0.444   0.106  -0.455   -0.0628 -0.0906    0.104 \n 7 AAK1    -0.216   -0.157  -0.306  -0.238  -0.218   -0.114  -0.0109   -0.226 \n 8 AAMP     1.23     0.650   0.350   1.25    0.138    0.690  -0.903    -0.594 \n 9 AARS     0.425    0.0293  0.195  -0.0334 -0.189   -0.297   0.0937    0.0522\n10 AARS2   -0.118   -0.189  -0.486   0.414  -0.404   -0.182  -0.598    -0.508 \n# ℹ 7,396 more rows\n# ℹ 35 more variables: j16_5 &lt;dbl&gt;, j16_6 &lt;dbl&gt;, j16_7 &lt;dbl&gt;, j16r5_1 &lt;dbl&gt;,\n#   j16r5_2 &lt;dbl&gt;, j16r5_3 &lt;dbl&gt;, j16r5_4 &lt;dbl&gt;, j16r5_5 &lt;dbl&gt;, j16r5_6 &lt;dbl&gt;,\n#   j16r5_7 &lt;dbl&gt;, j16r5_8 &lt;dbl&gt;, j16r16_1 &lt;dbl&gt;, j16r16_2 &lt;dbl&gt;,\n#   j16r16_3 &lt;dbl&gt;, j16r16_4 &lt;dbl&gt;, j16r16_5 &lt;dbl&gt;, j16r16_6 &lt;dbl&gt;,\n#   j16r16_7 &lt;dbl&gt;, j16r16_8 &lt;dbl&gt;, j16r16_9 &lt;dbl&gt;, j48_1 &lt;dbl&gt;, j48_2 &lt;dbl&gt;,\n#   j48_3 &lt;dbl&gt;, j48_4 &lt;dbl&gt;, j48_6 &lt;dbl&gt;, j48_7 &lt;dbl&gt;, j48r24_1 &lt;dbl&gt;, …\n\n\nThe analysis that we want to do puts the focus on the chicken rather that on genes. However, the raw data records genes in rows and chicken in columns grouped by diet. We therefore proceed with some data manipulation to get the data set with 43 rows corresponding to chickens. First, we use tidyr::pivot_longer() to have all variables expect gene become the values of a unique column coined name and the corresponding values in the table for these variables in a unique column coined value:\n\ndf &lt;- tidyr::pivot_longer(df, cols = -gene)\ndf\n\n# A tibble: 318,458 × 3\n   gene   name    value\n   &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n 1 A4GALT N_1   -0.153 \n 2 A4GALT N_2   -0.0126\n 3 A4GALT N_3   -0.140 \n 4 A4GALT N_4    0.208 \n 5 A4GALT N_6   -0.461 \n 6 A4GALT N_7   -0.217 \n 7 A4GALT j16_3 -0.130 \n 8 A4GALT j16_4 -0.295 \n 9 A4GALT j16_5 -0.130 \n10 A4GALT j16_6 -0.220 \n# ℹ 318,448 more rows\n\n\nNow, we can see that the name column encodes two different informations:\n\nthe type of diet given to the chicken; and,\nthe chicken ID in his group.\n\nWe therefore use tidyr::separate() to get each piece of information into its own column:\n\ndf &lt;- tidyr::separate(df, col = name, into = c(\"diet\", \"id\"), sep = \"_\")\ndf\n\n# A tibble: 318,458 × 4\n   gene   diet  id      value\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 A4GALT N     1     -0.153 \n 2 A4GALT N     2     -0.0126\n 3 A4GALT N     3     -0.140 \n 4 A4GALT N     4      0.208 \n 5 A4GALT N     6     -0.461 \n 6 A4GALT N     7     -0.217 \n 7 A4GALT j16   3     -0.130 \n 8 A4GALT j16   4     -0.295 \n 9 A4GALT j16   5     -0.130 \n10 A4GALT j16   6     -0.220 \n# ℹ 318,448 more rows\n\n\nFinally, we perform a last reshaping of the data set to get each gene in a dedicated column via tidyr::pivot_wider():\n\nchk &lt;- tidyr::pivot_wider(df, names_from = gene, values_from = value)\nchk\n\n# A tibble: 43 × 7,408\n   diet  id     A4GALT    A4GNT     AACS  AADACL1 AADACL2 AADACL3    AAK1   AAMP\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 N     1     -0.153   0.00873  0.0426   1.61e-1 -0.374  -0.445  -0.216   1.23 \n 2 N     2     -0.0126 -0.0130   0.173    2.05e-1  0.0320  0.0638 -0.157   0.650\n 3 N     3     -0.140   0.0126  -0.0219   1.85e-1 -0.411   0.444  -0.306   0.350\n 4 N     4      0.208   0.0713  -0.0788   5.76e-2  0.0361  0.106  -0.238   1.25 \n 5 N     6     -0.461   0.515   -0.00269  3.07e-1 -0.490  -0.455  -0.218   0.138\n 6 N     7     -0.217   0.158   -0.0894   3.09e-1 -0.145  -0.0628 -0.114   0.690\n 7 j16   3     -0.130   0.600   -0.0548   6.18e-4 -0.112  -0.0906 -0.0109 -0.903\n 8 j16   4     -0.295   0.171    0.0145  -4.14e-2 -0.151   0.104  -0.226  -0.594\n 9 j16   5     -0.130   0.723   -0.120    1.27e-1 -0.187  -1.23   -0.151  -0.733\n10 j16   6     -0.220   0.743    0.200    6.10e-2 -0.265  -0.157  -0.141  -0.683\n# ℹ 33 more rows\n# ℹ 7,398 more variables: AARS &lt;dbl&gt;, AARS2 &lt;dbl&gt;, AARSD1 &lt;dbl&gt;, AASS &lt;dbl&gt;,\n#   AATF &lt;dbl&gt;, ABAT &lt;dbl&gt;, ABAT1 &lt;dbl&gt;, ABCA1 &lt;dbl&gt;, ABCA3 &lt;dbl&gt;,\n#   ABCB10 &lt;dbl&gt;, ABCB7 &lt;dbl&gt;, ABCC4 &lt;dbl&gt;, ABCC5 &lt;dbl&gt;, ABCD3 &lt;dbl&gt;,\n#   ABCD4 &lt;dbl&gt;, ABCE1 &lt;dbl&gt;, ABCF2 &lt;dbl&gt;, ABCG1 &lt;dbl&gt;, ABCG2 &lt;dbl&gt;,\n#   ABCG4 &lt;dbl&gt;, ABHD11 &lt;dbl&gt;, ABHD12 &lt;dbl&gt;, ABHD2 &lt;dbl&gt;, ABHD3 &lt;dbl&gt;,\n#   ABHD5 &lt;dbl&gt;, ABI1 &lt;dbl&gt;, ABI2 &lt;dbl&gt;, ABL1 &lt;dbl&gt;, ABL11 &lt;dbl&gt;, ABL2 &lt;dbl&gt;, …\n\n\nIt is difficult to provide insightful visualization in the original space of variables which is of dimension 7406. We therefore begin with performing PCA and then we will provide hopefully insightful visualizations in reduced space.\n\n\nPCA\nFirst, we select the numerical variables on which we want to perform the PCA and we standardize them:\n\nX &lt;- chk |&gt; \n  dplyr::select(-diet, -id) |&gt; \n  scale()\n\nNow, we can perform the PCA:\n\npca_res &lt;- FactoMineR::PCA(X, graph = FALSE)\n\n\nChoice of the reduced dimension\n\nfactoextra::fviz_screeplot(\n  pca_res, \n  addlabels = TRUE, \n  ncp = 42L\n)\n\n\n\n\n\n\n\n\nThe elbow method applied to the screeplot suggests to keep either 1, 6, 8 or 16 components. We can look at the cumulative percentage of variance explained:\n\npca_res |&gt; \n  factoextra::get_eigenvalue() |&gt; \n  janitor::clean_names() |&gt; \n  tibble::as_tibble() |&gt; \n  gt::gt() |&gt; \n  gt::cols_label(\n    eigenvalue = \"Eigenvalue\",\n    variance_percent = \"Percentage of variance explained\",\n    cumulative_variance_percent = \"Cumulative percentage of variance explained\"\n  )\n\n\n\n\n\n\n\nEigenvalue\nPercentage of variance explained\nCumulative percentage of variance explained\n\n\n\n\n1453.57240\n19.6269565\n19.62696\n\n\n692.78785\n9.3544133\n28.98137\n\n\n536.20796\n7.2401831\n36.22155\n\n\n434.45335\n5.8662348\n42.08779\n\n\n374.62157\n5.0583523\n47.14614\n\n\n324.08250\n4.3759451\n51.52209\n\n\n273.79824\n3.6969787\n55.21906\n\n\n263.23096\n3.5542933\n58.77336\n\n\n238.82728\n3.2247809\n61.99814\n\n\n212.22641\n2.8656010\n64.86374\n\n\n205.86862\n2.7797545\n67.64349\n\n\n186.68809\n2.5207682\n70.16426\n\n\n161.71254\n2.1835342\n72.34780\n\n\n146.67462\n1.9804837\n74.32828\n\n\n134.41024\n1.8148831\n76.14316\n\n\n128.77791\n1.7388321\n77.88199\n\n\n109.49173\n1.4784192\n79.36041\n\n\n104.11595\n1.4058324\n80.76625\n\n\n98.79454\n1.3339798\n82.10023\n\n\n91.34045\n1.2333304\n83.33356\n\n\n83.86158\n1.1323465\n84.46590\n\n\n80.15492\n1.0822971\n85.54820\n\n\n79.43852\n1.0726238\n86.62082\n\n\n74.46548\n1.0054751\n87.62630\n\n\n70.99072\n0.9585568\n88.58486\n\n\n66.21149\n0.8940250\n89.47888\n\n\n62.68820\n0.8464515\n90.32533\n\n\n60.07860\n0.8112153\n91.13655\n\n\n58.69024\n0.7924688\n91.92902\n\n\n56.15042\n0.7581747\n92.68719\n\n\n54.70237\n0.7386224\n93.42581\n\n\n52.81262\n0.7131059\n94.13892\n\n\n50.97364\n0.6882749\n94.82719\n\n\n48.64270\n0.6568012\n95.48400\n\n\n48.15634\n0.6502342\n96.13423\n\n\n46.40970\n0.6266500\n96.76088\n\n\n44.81239\n0.6050822\n97.36596\n\n\n42.90186\n0.5792851\n97.94525\n\n\n40.79995\n0.5509040\n98.49615\n\n\n39.33001\n0.5310560\n99.02721\n\n\n38.25896\n0.5165942\n99.54380\n\n\n33.78608\n0.4561987\n100.00000\n\n\n\n\n\n\n\nIn this case, we see that keeping 6 components retains only 50% of the inertia, keeping 8 components makes it up almost to 60% and going for 16 components makes us above the 75% of inertia.\nIf we use the rule that pertains to keeping all components with an explained variance above 1, then we would keep 42 components.\nFinally, we can get an estimate of the optimal number of components to keep by minimizing the generalized cross-validation score optained as the mean reconstruction error of each chicken gene expression from the reconstructed gene expression using outputs of a PCA computed on all chickens expect that one. The function FactoMineR::estim_ncp() does exactly that for us:\n\nout &lt;- FactoMineR::estim_ncp(X, ncp.min = 1, ncp.max = 42)\ntibble::tibble(gcv = out$criterion, ncp = seq_along(gcv)) |&gt; \n  ggplot(aes(x = ncp, y = gcv)) + \n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\nout$ncp\n\n[1] 15\n\n\n\n\n\nAnalysis of the variables\nThe analysis of variables by means of visualizations or tables becomes tricky as the number of variables grows. Typically here, drawing the correlation circle with 7406 arrows would not be of any help. That’s why the factoextra::fviz_pca_var() comes with an optional argument select.var which instruct the function to visualize only the variables that satisfy some constraints. For example, in the code below, we ask it to retain only variables with a cumulated cos2 value in the first principal plane above 0.8. This allows to visualize only those variables which are well represented in that plane:\n\nfactoextra::fviz_pca_var(pca_res, select.var = list(cos2 = 0.8))\n\n\n\n\n\n\n\n\n\n\nAnalysis of the individuals\nWe can project chickens onto the first principal plane and color points by diet:\n\nfactoextra::fviz_pca_ind(pca_res, col.ind = chk$diet)\n\n\n\n\n\n\n\n\nThis is interesting as it shows that, in this plane, only chicken who underwent fast for 48h did not go back to normal gene expression, including those who had been back to normal diet for 24h after the fast.\n\n\n\n\n\n\nThe limit of linear projections\n\n\n\nOne has to be careful in the interpretation in such cases. Indeed, we could be tempted to conclude that chicken in all the other types of diet went back to normal gene expression. But we are in fact only looking at the chicken in a reduced space that only accounts for less than 30% of the total inertia in the original data! This is what urged researchers typically in the field of genetics to resort to non-linear methods for dimensional reduction such as tSNE or UMAP, which effectively achieve very small reduced spaces in terms of dimension while retaining most of the original inertia. For instance, the code below illustrate the projection of the data in a reduced space of dimension 2 via UMAP:\n\numap_res &lt;- chk |&gt; \n  dplyr::select(-diet, -id) |&gt; \n  scale() |&gt; \n  umap::umap()\ntibble::tibble(\n  X = umap_res$layout[, 1], \n  Y = umap_res$layout[, 2], \n  Diet = chk$diet\n) |&gt; \n  ggplot(aes(x = X, y = Y, color = Diet)) + \n  geom_point() + \n  theme_bw()\n\n\n\n\n\n\n\n\nThis clearly illustrates that claiming that chicken who underwent fast for less than 48h go back to normal gene expression on the basis of the results from the PCA would have been completely misleading."
  },
  {
    "objectID": "12_PCA/12-PCA-Solutions.html#le-jeu-de-données-orange.csv",
    "href": "12_PCA/12-PCA-Solutions.html#le-jeu-de-données-orange.csv",
    "title": "PCA - Solutions",
    "section": "Le jeu de données orange.csv",
    "text": "Le jeu de données orange.csv\nSix jus d’orange de fabriquants différents ont évalués. Toutes les variables sont-elles indisensables ? Y a-t-il des jus qui se dégagent comme particulièrement bons? mauvais ?\n\norange &lt;- readr::read_delim(\"12-PCA-Data/orange.csv\", \n    delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 6 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (3): Product, Way of preserving, Origin\ndbl (14): Odour intensity, Odour typicality, Pulpiness, Intensity of taste, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\norange\n\n# A tibble: 6 × 17\n  Product    `Odour intensity` `Odour typicality` Pulpiness `Intensity of taste`\n  &lt;chr&gt;                  &lt;dbl&gt;              &lt;dbl&gt;     &lt;dbl&gt;                &lt;dbl&gt;\n1 Pampryl a…              2.82               2.53      1.66                 3.46\n2 Tropicana…              2.76               2.82      1.91                 3.23\n3 Fruvita f…              2.83               2.88      4                    3.45\n4 Joker amb.              2.76               2.59      1.66                 3.37\n5 Tropicana…              3.2                3.02      3.69                 3.12\n6 Pampryl f…              3.07               2.73      3.34                 3.54\n# ℹ 12 more variables: Acidity &lt;dbl&gt;, Bitterness &lt;dbl&gt;, Sweetness &lt;dbl&gt;,\n#   Glucose &lt;dbl&gt;, Fructose &lt;dbl&gt;, Saccharose &lt;dbl&gt;, `Sweetening power` &lt;dbl&gt;,\n#   pH &lt;dbl&gt;, `Citric acid` &lt;dbl&gt;, `Vitamin C` &lt;dbl&gt;,\n#   `Way of preserving` &lt;chr&gt;, Origin &lt;chr&gt;"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html",
    "href": "02_Visualize/02-Visualize-Exercises.html",
    "title": "Visualize Data",
    "section": "",
    "text": "Add a setup chunk that loads the tidyverse packages and turn the global option eval to true in the above YAML header.\n\nmpg"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-0",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-0",
    "title": "Visualize Data",
    "section": "",
    "text": "Add a setup chunk that loads the tidyverse packages and turn the global option eval to true in the above YAML header.\n\nmpg"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-1",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-1",
    "title": "Visualize Data",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nRun the code on the slide to make a graph. Pay strict attention to spelling, capitalization, and parentheses!"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-2",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-2",
    "title": "Visualize Data",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nAdd color, size, alpha, and shape aesthetics to your graph. Experiment.\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#help-me",
    "href": "02_Visualize/02-Visualize-Exercises.html#help-me",
    "title": "Visualize Data",
    "section": "Help Me",
    "text": "Help Me\nWhat do facet_grid() and facet_wrap() do? (run the code, interpret, convince your group)\n\n# Makes a plot that the commands below will modify\nq &lt;- ggplot(mpg) + geom_point(aes(x = displ, y = hwy))\n\nq + facet_grid(cols = vars(cyl))\nq + facet_grid(rows = vars(drv))\nq + facet_grid(rows = vars(drv), cols = vars(cyl))\nq + facet_wrap(facets = vars(class))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-3",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-3",
    "title": "Visualize Data",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nAdd the black code to your graph. What does it do?\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(displ, hwy, color = class))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-4",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-4",
    "title": "Visualize Data",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nReplace this scatterplot with one that draws boxplots. Use the cheatsheet. Try your best guess.\n\nggplot(mpg) + geom_point(aes(class, hwy))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-5",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-5",
    "title": "Visualize Data",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nMake a histogram of the hwy variable from mpg. Hint: do not supply a y variable."
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-6",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-6",
    "title": "Visualize Data",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nUse the help page for geom_histogram to make the bins 2 units wide."
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-7",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-7",
    "title": "Visualize Data",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nMake a bar chart class colored by class. Use the help page for geom_bar to choose a “color” aesthetic for class."
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#quiz",
    "href": "02_Visualize/02-Visualize-Exercises.html#quiz",
    "title": "Visualize Data",
    "section": "Quiz",
    "text": "Quiz\nWhat will this code do?\n\nggplot(mpg) + \n  geom_point(aes(displ, hwy)) +\n  geom_smooth(aes(displ, hwy))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#quiz-1",
    "href": "02_Visualize/02-Visualize-Exercises.html#quiz-1",
    "title": "Visualize Data",
    "section": "Quiz",
    "text": "Quiz\nWhat is different about this plot? Run the code!\n\np &lt;- ggplot(mpg) + \n  geom_point(aes(displ, hwy)) +\n  geom_smooth(aes(displ, hwy))\n\nlibrary(plotly)\nggplotly(p)"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html",
    "href": "07_Join/07-Join-Exercises.html",
    "title": "Join Data with dplyr",
    "section": "",
    "text": "flights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nVisualizing a raw tibble is not that pretty. Let’s wrap the next output in a DT::datatable() function to make it more readable:\n\nDT::datatable(airlines)"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html#your-turn-1",
    "href": "07_Join/07-Join-Exercises.html#your-turn-1",
    "title": "Join Data with dplyr",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nWhich airlines had the largest arrival delays? Complete the code below.\n\nJoin airlines to flights\nCompute and order the average arrival delays by airline. Display full names, no codes.\n\n(Hint: Be sure to remove each _ before turning eval to true)\n\nflights |&gt;\n  filter(!is.na(arr_delay)) |&gt;\n  __________________  |&gt;\n  group_by(_________) |&gt;\n  __________________  |&gt;\n  arrange(__________)"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html#your-turn-2",
    "href": "07_Join/07-Join-Exercises.html#your-turn-2",
    "title": "Join Data with dplyr",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nJoin flights and airports by dest and faa.\nThen for each name, compute the distance from NYC and the average arr_delay. Hint: use first() to get the first value of distance.\nOrder by average delay, worst to best.\n(Hint: Be sure to remove each _ before turning eval to true)\n\nflights |&gt; \n  filter(!is.na(arr_delay)) |&gt;\n  ______(airports, __________________) |&gt;\n  group_by(name) |&gt;\n  _________(distance = _____________, \n               delay = _____________) |&gt;\n  ______(_____(delay))"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html#your-turn-3",
    "href": "07_Join/07-Join-Exercises.html#your-turn-3",
    "title": "Join Data with dplyr",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nHow many airports in airports are serviced by flights in flights? (i.e. how many places can you fly to direct from New York?)\nNotice that the column to join on is named faa in the airports data set and dest in the flights data set.\n(Hint: Be sure to remove each _ before turning eval to true)\n\n__________ |&gt;\n _____________________________ |&gt;\n  select(faa)"
  },
  {
    "objectID": "01_Introduction/01-Introduction-Exercises.html",
    "href": "01_Introduction/01-Introduction-Exercises.html",
    "title": "Quarto",
    "section": "",
    "text": "This is a Quarto file. It contains plain text interspersed with grey chunks of code. You can use the file to take notes and run code. For example, you can write your name on the line below. Try it:\n\n# You can write code in chunks that look like this.\n# This chunk uses the code plot(cars) to plot a data set.\n# To run the code, click the Green play button at the\n# top right of this chunk. Try it!\nplot(cars)\n\n\n\n\n\n\n\n\nGood job! The results of a code chunk will appear beneath the chunk. You can click the x above the results to make them go away, but let’s not do that.\nYou can open a new Quarto file by going to File &gt; New File &gt; Quarto Document…. Then click OK. But let’s not open a new file now—keep reading this one!\n\nAdding chunks\nTo add a new code chunk, press Cmd+Option+I (Ctrl+Alt+I on Windows), or click the Insert button at the top of this document, then select R. Quarto will add a new, empty chunk at your cursor’s location.\nTry making a code chunk below:\nGood job! For today, you should place all of your R code inside of code chunks.\n\n# Sometimes you might want to run only some of the code \n# in a code chunk. To do that, highlight the code to \n# run and then press Cmd + Enter (Control + Enter on \n# Windows). If you do not highlight any code, R will \n# run the line of code that your cursor is on.\n# Try it now. Run mean(1:5) but not the line below.\nmean(1:5)\n\n[1] 3\n\nwarning(\"You shouldn't run this!\")\n\nWarning: You shouldn't run this!\n\n\n\n# You can click the downward facing arrow to the left of the play button to run\n# every chunk above the current code chunk. This is useful if the code in your\n# chunk uses object that you made in previous chunks.\n# Sys.Date()\n\nDid you notice the green lines (if your RStudio theme is the default one) in the code chunk above? They are code comments, lines of text that R ignores when it runs the code. R will treat everything that appears after # on a line as a code comment. As a result, if you run the chunk above, nothing will happen—it is all code comments (and that’s fine)!\nRemove the # on the last line of the chunk above and then rerun the chunk. Can you tell what Sys.Date() does?\nBy the way, you only need to use code comments inside of code chunks. R knows not to try to run the text that you write outside of code chunks. You can press Cmd+Shift+C to comment or uncomment a line of code.\n\n\nText formatting\nHave you noticed the funny highlighting that appears in this document? Quarto treats text surrounded by asterisks, double asterisks, and backticks in special ways. It is Quarto’s way of saying that these words are in\n\nitalics\nalso italics\nbold, and\ncode font\n\n*, **, and ` are signals used by a text editing format known as markdown. Quarto uses markdown to turn your plain looking .Rmd documents into polished reports. Let’s give that a try.\n\n\nReports\nWhen you click the Render button at the top of an Quarto file (like this one), Quarto generates a polished copy of your report. Quarto:\n\nTransforms all of your markdown cues into actual formatted text (e.g. bold text, italic text, etc.)\nReruns all of your code chunks in a clean R session and appends the results to the finished report.\nSaves the finished report alongside your .Rmd file\n\nClick the Render button at the top of this document or press Cmd+Shift+K (Ctrl+Shift+K on Windows) to render the finished report. The RStudio IDE will open the report so you can see its contents. For now, our reports will be HTML files. Try clicking Render now.\nGood job! You’ll learn more about Quarto throughout the day!\n\n\nR Packages\nHere is one last code chunk that we will use in the next exercise. If you uncomment the code and try to run it, it won’t work. If you don’t believe me try!\n\n# ggplot(data = diamonds) + geom_point(aes(x = carat, y = price))"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We want to study the energy efficiency of a chemical reaction that is documented having a nominal energy efficiency of \\(90\\%\\). Based on previous experiments on the same reaction, we know that the energy efficiency is a Gaussian random variable with unknown mean \\(\\mu\\) and variance equal to \\(2\\). In the last \\(5\\) days, the plant has given the following energy efficiencies (in percentage):\n\n\n\n\n\n\n\n\n\n\n\n\nEnergy efficiency of chemical reactions\n\n\nEnergy efficicency\n(J)\nChemical Reaction\nIdentification Number\n\n\n\n\n91.60\n1\n\n\n88.75\n2\n\n\n90.80\n3\n\n\n89.95\n4\n\n\n91.30\n5\n\n\n\n\n\n\n\n1. Is the data in accordance with the specifications?\nLet us first propose a mathematical formulation of the problem.\n\nLet \\(X\\) be a random variable which represents the energy efficiency of one random such chemical reaction. The specifications suggest us to assume that \\(X \\sim N(\\mu, \\sigma^2)\\), with unknown mean energy efficiency \\(\\mu\\) and known variance \\(\\sigma^2 = 2\\).\n\nThe specification from the plant is that the nominal mean energy efficiency should be \\(\\mu_0 = 90\\%\\) which leads us to test:\n\\[ H_0: \\mu = \\mu_0 \\quad v.s. \\quad H_a: \\mu \\ne \\mu_0. \\] This suggests that an appropriate test statistic to use is\n\\[\nZ_0 = \\sqrt{n} \\frac{\\overline{X} - \\mu_0}{\\sigma} \\stackrel{H_0}{\\sim} N(0,1).\n\\]\n\nNext, an experiment has been carried out to measure \\(n = 5\\) energy efficiencies corresponding to five random chemical reactions produced by the plant. Hence, an \\(n\\)-sample \\(X_1, \\dots, X_5 \\sim X\\) has been collected with corresponding observed values \\(x_1, \\dots, x_5\\).\n\nLet us now compute the observed value of the statistic \\(Z_0\\):\n\n# n-sample\nx &lt;- c(91.6, 88.75, 90.8, 89.95, 91.3)\n# sample size\nn &lt;- length(x)\n# nominal value of energy efficiency\nmu0 &lt;- 90\n# square root of variance (known)\nsigma &lt;- sqrt(2)\n# observed value of test stat\nz0 &lt;- sqrt(n) * (mean(x) - mu0) / sigma\nz0\n\n[1] 0.7589466\n\n\nLet’s say that we want a significance level \\(\\alpha = 5%\\). We need to compute the quantile of order \\(1 - \\alpha/2\\) of the standard normal distribution:\n\nalpha &lt;- 0.05\nzs &lt;- qnorm(1 - alpha / 2)\nzs\n\n[1] 1.959964\n\n\nThe value \\(z_0\\) does not belong to the critical region of the test. Hence, we lack statistical evidence to reject \\(H_0\\).\nWe could also use the p-value:\n\npval &lt;- 2 * min(pnorm(z0), 1 - pnorm(z0))\npval\n\n[1] 0.4478845\n\n\nThe decision rule for rejecting \\(H_0\\) is when the p-value is smaller than \\(\\alpha\\). Here, we conclude that for any reasonable significance level, the p-value will always be higher, suggesting that we lack statistical evidence to reject \\(H_0\\).\n2. What is a point estimate of the energy efficiency?\nThis is simply obtained by computing the sample mean of the sample of energy efficiencices which is provided by the mean() function as follows:\n\nmean(x)\n\n[1] 90.48\n\n\n3. Does that mean that the data significantly prove that the energy efficiency is larger than the expected nominal value?\nHere we want to test the following hypotheses:\n\\[ H_0: \\mu = \\mu_0 \\quad v.s. \\quad H_a: \\mu &gt; \\mu_0. \\] Let’s compute the p-value:\n\n# P_{H_0}(Z_0 &gt; z_0)\npval2 &lt;- 1 - pnorm(z0)\npval2\n\n[1] 0.2239422\n\n\nThe p-value is greater than any reasonable significance level. Hence, we have not enough statistical evidence to reject \\(H_0\\). We can therefore conclude that, despite the point estimate of \\(90.48\\%\\), the nominal energy efficiency cannot be claimed to be larger than \\(90\\%\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-1",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We want to study the energy efficiency of a chemical reaction that is documented having a nominal energy efficiency of \\(90\\%\\). Based on previous experiments on the same reaction, we know that the energy efficiency is a Gaussian random variable with unknown mean \\(\\mu\\) and variance equal to \\(2\\). In the last \\(5\\) days, the plant has given the following energy efficiencies (in percentage):\n\n\n\n\n\n\n\n\n\n\n\n\nEnergy efficiency of chemical reactions\n\n\nEnergy efficicency\n(J)\nChemical Reaction\nIdentification Number\n\n\n\n\n91.60\n1\n\n\n88.75\n2\n\n\n90.80\n3\n\n\n89.95\n4\n\n\n91.30\n5\n\n\n\n\n\n\n\n1. Is the data in accordance with the specifications?\nLet us first propose a mathematical formulation of the problem.\n\nLet \\(X\\) be a random variable which represents the energy efficiency of one random such chemical reaction. The specifications suggest us to assume that \\(X \\sim N(\\mu, \\sigma^2)\\), with unknown mean energy efficiency \\(\\mu\\) and known variance \\(\\sigma^2 = 2\\).\n\nThe specification from the plant is that the nominal mean energy efficiency should be \\(\\mu_0 = 90\\%\\) which leads us to test:\n\\[ H_0: \\mu = \\mu_0 \\quad v.s. \\quad H_a: \\mu \\ne \\mu_0. \\] This suggests that an appropriate test statistic to use is\n\\[\nZ_0 = \\sqrt{n} \\frac{\\overline{X} - \\mu_0}{\\sigma} \\stackrel{H_0}{\\sim} N(0,1).\n\\]\n\nNext, an experiment has been carried out to measure \\(n = 5\\) energy efficiencies corresponding to five random chemical reactions produced by the plant. Hence, an \\(n\\)-sample \\(X_1, \\dots, X_5 \\sim X\\) has been collected with corresponding observed values \\(x_1, \\dots, x_5\\).\n\nLet us now compute the observed value of the statistic \\(Z_0\\):\n\n# n-sample\nx &lt;- c(91.6, 88.75, 90.8, 89.95, 91.3)\n# sample size\nn &lt;- length(x)\n# nominal value of energy efficiency\nmu0 &lt;- 90\n# square root of variance (known)\nsigma &lt;- sqrt(2)\n# observed value of test stat\nz0 &lt;- sqrt(n) * (mean(x) - mu0) / sigma\nz0\n\n[1] 0.7589466\n\n\nLet’s say that we want a significance level \\(\\alpha = 5%\\). We need to compute the quantile of order \\(1 - \\alpha/2\\) of the standard normal distribution:\n\nalpha &lt;- 0.05\nzs &lt;- qnorm(1 - alpha / 2)\nzs\n\n[1] 1.959964\n\n\nThe value \\(z_0\\) does not belong to the critical region of the test. Hence, we lack statistical evidence to reject \\(H_0\\).\nWe could also use the p-value:\n\npval &lt;- 2 * min(pnorm(z0), 1 - pnorm(z0))\npval\n\n[1] 0.4478845\n\n\nThe decision rule for rejecting \\(H_0\\) is when the p-value is smaller than \\(\\alpha\\). Here, we conclude that for any reasonable significance level, the p-value will always be higher, suggesting that we lack statistical evidence to reject \\(H_0\\).\n2. What is a point estimate of the energy efficiency?\nThis is simply obtained by computing the sample mean of the sample of energy efficiencices which is provided by the mean() function as follows:\n\nmean(x)\n\n[1] 90.48\n\n\n3. Does that mean that the data significantly prove that the energy efficiency is larger than the expected nominal value?\nHere we want to test the following hypotheses:\n\\[ H_0: \\mu = \\mu_0 \\quad v.s. \\quad H_a: \\mu &gt; \\mu_0. \\] Let’s compute the p-value:\n\n# P_{H_0}(Z_0 &gt; z_0)\npval2 &lt;- 1 - pnorm(z0)\npval2\n\n[1] 0.2239422\n\n\nThe p-value is greater than any reasonable significance level. Hence, we have not enough statistical evidence to reject \\(H_0\\). We can therefore conclude that, despite the point estimate of \\(90.48\\%\\), the nominal energy efficiency cannot be claimed to be larger than \\(90\\%\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-2",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-2",
    "title": "Hypothesis Testing",
    "section": "Exercise 2",
    "text": "Exercise 2\nA study about air pollution done by a research station measured, on \\(8\\) different air samples, the following values of a polluant (in \\(\\mu\\)g/m\\(^2\\)):\n\n\n\n\n\n\n\n\n\n\n\n\nConcentration of a polluant in air samples\n\n\nPolluant Concentration\n(μg/m2)\nAir Sample\nIdentification Number\n\n\n\n\n2.2\n1\n\n\n1.8\n2\n\n\n3.1\n3\n\n\n2.0\n4\n\n\n2.4\n5\n\n\n2.0\n6\n\n\n2.1\n7\n\n\n1.2\n8\n\n\n\n\n\n\n\nAssuming that the sampled population is normal,\n1. Can we say that the polluant is present with less than \\(2.5 \\mu\\)g/m\\(^2\\)?\nLet \\(X\\) be a random variable which represents the concentration in polluant of an air sample taken at random. We assume that \\(X \\sim N(\\mu, \\sigma^2)\\).\nFor both this question and the next one, we aim at testing the following hypotheses:\n\\[\nH_0: \\mu = \\mu_0 \\quad v.s. \\quad \\mu &lt; \\mu_0,\n\\]\nwith \\(\\mu_0 = 2.5\\) here and \\(\\mu_0 = 2.5\\) in the 2nd question.\nThe variance \\(\\sigma^2\\) is not provided which means that we will need to estimate it from the data using the sample variance. Hence, a good test statistic to look at is Student’s t-statistic:\n\\[\nT_0 = \\sqrt{n} \\frac{\\overline X - \\mu_0}{s} \\stackrel{H_0}{\\sim} t(n - 1).\n\\]\nOnly large negative values of \\(T_0\\) will be in favor of \\(H_a\\).\nThis leads us to apply Student’s t-test which is implemented in R via the function t.test() which we can use as follow:\n\nx2 &lt;- c(2.2, 1.8, 3.1, 2.0, 2.4, 2.0, 2.1, 1.2)\nalpha &lt;- 0.05\nout &lt;- t.test(\n  x = x2, \n  alternative = \"less\", \n  mu = 2.5, \n  conf.level = 1 - alpha\n)\nout |&gt; \n  broom::tidy() |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.1\n-2.106097\n0.03660458\n7\n-Inf\n2.459827\nOne Sample t-test\nless\n\n\n\n\n\n\n\nIf I set the upper bound for probability of committing type I errors to \\(\\alpha\n= 5\\%\\), then I have strong statistical evidence to reject \\(H_0\\) in favor of \\(H_a\\). I can therefore conclude that indeed the average amount of polluant is less than \\(2.5\\) \\(\\mu\\)g/m\\(^2\\).\n2. Can we say that the polluant is present with less than \\(2.4 \\mu\\)g/m\\(^2\\)?\nWe answer this question in the same way as the previous but considering \\(\\mu_0 =\n2.4\\), which leads to:\n\nout &lt;- t.test(x = x2, alternative = \"less\", mu = 2.4)\nout |&gt; \n  broom::tidy() |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.1\n-1.579573\n0.0791076\n7\n-Inf\n2.459827\nOne Sample t-test\nless\n\n\n\n\n\n\n\nIf I still consider \\(\\alpha = 5\\%\\), I lack evidence for rejecting \\(H_0\\) and cannot claim that the average amount of polluant is less than \\(2.4 \\mu\\)g/m\\(^2\\).\n3. Is the normality hypothesis essential to justify the method used?\nThe normality assumption is essential to lead to an exact test.\nWe could invoke the CLT in case of large sample size but this is not the case here.\nLet’s check the normality assumption using the Shapiro-Wilk test which is valid here because the sample size is between \\(3\\) and \\(5000\\):\n\nout &lt;- shapiro.test(x = x2)\nout |&gt; \n  broom::tidy() |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nstatistic\np.value\nmethod\n\n\n\n\n0.9428648\n0.639469\nShapiro-Wilk normality test\n\n\n\n\n\n\n\nThe null hypothesis in the Shapiro-Wilk test is that the sample has been drawn from a normal distribution. The resulting p-value is 0.64, which is larger than any reasonable significance level. Hence, I lack statistical evidence to reject the fact that the sample has been drawn from a normal distribution. This gives credit to the analysis conducted in the previous two questions which required normality of the sample."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-3",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-3",
    "title": "Hypothesis Testing",
    "section": "Exercise 3",
    "text": "Exercise 3\nA medical inspection in an elementary school during a measles epidemic led to the examination of \\(30\\) children to assess whether they were affected. The results are in a tibble exam which contains the following:\n\n\n\n\n\n\n\n\n\n\n\n\nMedical inspection on a sample of children\n\n\nChild Status\nChild\nIdentification Number\n\n\n\n\nHealthy\n1\n\n\nHealthy\n2\n\n\nHealthy\n3\n\n\nHealthy\n4\n\n\nHealthy\n5\n\n\nHealthy\n6\n\n\nHealthy\n7\n\n\nHealthy\n8\n\n\nHealthy\n9\n\n\nHealthy\n10\n\n\nHealthy\n11\n\n\nHealthy\n12\n\n\nHealthy\n13\n\n\nSick\n14\n\n\nHealthy\n15\n\n\nHealthy\n16\n\n\nHealthy\n17\n\n\nHealthy\n18\n\n\nHealthy\n19\n\n\nHealthy\n20\n\n\nHealthy\n21\n\n\nHealthy\n22\n\n\nHealthy\n23\n\n\nHealthy\n24\n\n\nHealthy\n25\n\n\nHealthy\n26\n\n\nHealthy\n27\n\n\nSick\n28\n\n\nHealthy\n29\n\n\nHealthy\n30\n\n\n\n\n\n\n\nLet \\(p\\) be the probability that a child from the same school is sick.\n1. Determine a point estimate \\(\\widehat{p}\\) for \\(p\\).\nLet \\(X\\) be a random variable which represents whether a child taken at random in the population is sick. Let \\(X = 1\\) if the child is sick and \\(X = 0\\) if not. Then, by definition, \\(X \\sim Be(p)\\).\nLet now \\(X_1, \\dots, X_n \\sim X\\) be an \\(n\\)-sample of children which were checked for the disease. The total number of infected students is given by \\(\\sum_{i=1}^n\nX_i\\). Hence, a point estimate of the probability that a child is sick is given by:\n\\[\n\\widehat{p} = \\overline X.\n\\]\nNumerical application. We can use the data and R to help us with the calculation:\n\n# first compute the values x_1, ..., x_n from the Status variable\nexam &lt;- dplyr::mutate(exam, x_values = child_status == \"Sick\")\n\n# next, compute the point estimate of p as the sample mean of the variable\n# x_values\nmean(exam$x_values)\n\n[1] 0.06666667\n\n\n2. The school will be closed if more than 5% of the children are sick. Can you conclude that, statistically, this is the case? Use a significance level of 5%.\nDespite a point estimate that exceeds \\(5\\%\\), since we only assess the presence of the disease in a subset of the total population, this might not be statistically significant given the variability. To provide insight into this, we can provide the p-value of an appropriate hypothesis test. We were asked to use a significance level \\(\\alpha = 5\\%\\) for such a test. The hypotheses that we want to test here are:\n\\[\nH_0: p = p_0 \\quad v.s. \\quad p &gt; p_0,\n\\] with \\(p_0 = 5\\%\\).\nSince \\(X_1, \\dots, X_n \\stackrel{iid}{\\sim} Be(p)\\), then:\n\\[\nS_n = \\sum_{i=1}^n X_i \\sim Binom(n, p),\n\\] which can be used as test statistic because, under \\(H_0\\), its distribution is binomial with \\(n\\) and \\(p_0\\) parameters which are all known. We can use the exact.test() function which can help us with the calculations:\n\nout &lt;- binom.test(\n  x = sum(exam$x_values), # Number of sick children in the sample\n  n = 30, # Total number of children in the sample\n  p = 0.05, # Value of p_0\n  alternative = \"greater\" # Type of alternative hypothesis\n)\nout |&gt; \n  broom::tidy() |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.06666667\n2\n0.4464579\n30\n0.0119758\n1\nExact binomial test\ngreater\n\n\n\n\n\n\n\nThe p-value of the test is 0 which exceeds any reasonable significance level. Hence, I lack statistical evidence to reject \\(H_0\\). There is thus no tangible reason to close the school."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-4",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-4",
    "title": "Hypothesis Testing",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe capacities (in ampere-hours) of \\(10\\) batteries were recorded as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nBatteries\n\n\nCapacity\n(ampere-hours)\nIdentification\nNumber\n\n\n\n\n140\n1\n\n\n136\n2\n\n\n150\n3\n\n\n144\n4\n\n\n148\n5\n\n\n152\n6\n\n\n138\n7\n\n\n141\n8\n\n\n143\n9\n\n\n151\n10\n\n\n\n\n\n\n\n\nEstimate the population variance \\(\\sigma^2\\).\nCan we claim that the mean capacity of a battery is greater than 142 ampere-hours ?\nCan we claim that the mean capacity of a battery is greater than 140 ampere-hours ?\nCan we claim that the standard deviation of the capacity is less than 6 ampere-hours ?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-5",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-5",
    "title": "Hypothesis Testing",
    "section": "Exercise 5",
    "text": "Exercise 5\nA company produces barbed wire in skeins of \\(100\\)m each, nominally. The real length of the skeins is a random variable \\(X\\) distributed as a \\(\\mathcal{N}(\\mu,\n4)\\). Measuring \\(10\\) skeins, we get the following lengths:\n\n\n\n\n\n\n\n\n\n\n\n\nSkeins\n\n\nLength\n(m)\nIdentification\nNumber\n\n\n\n\n\n98.683\n1\n\n\n96.599\n2\n\n\n99.617\n3\n\n\n102.544\n4\n\n\n100.110\n5\n\n\n102.000\n6\n\n\n98.394\n7\n\n\n100.324\n8\n\n\n98.743\n9\n\n\n103.247\n10\n\n\n\n\n\n\n\n\nPerform a conformity test at significance level \\(\\alpha = 5\\%\\).\nDetermine, on the basis of the observed values, the p-value of the test."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-6",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-6",
    "title": "Hypothesis Testing",
    "section": "Exercise 6",
    "text": "Exercise 6\nIn an atmospheric study the researchers registered, over \\(8\\) different samples of air, the following concentration of COG (in micrograms over cubic meter):\n\n\n\n\n\n\n\n\n\n\n\n\nConcentration of COG in different air samples\n\n\nConcentration of COG\n(μg/m3)\nAir sample\n(Identification number)\n\n\n\n\n2.3\n1\n\n\n1.7\n2\n\n\n3.2\n3\n\n\n2.1\n4\n\n\n2.3\n5\n\n\n2.0\n6\n\n\n2.2\n7\n\n\n1.2\n8\n\n\n\n\n\n\n\n\nUsing unbiased estimators, determine a point estimate of the mean and variance of COG concentration.\n\nAssume now that the COG concentration is normally distributed.\n\nUsing a suitable statistical tool, establish whether the measured data allow to say that the mean concentration of COG is greater than \\(1.8\\) \\(\\mu\\)g/m\\(^3\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-7",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-7",
    "title": "Hypothesis Testing",
    "section": "Exercise 7",
    "text": "Exercise 7\nOn a total of \\(2350\\) interviewed citizens, \\(1890\\) approve the construction of a new movie theater.\n\nPerform an hypothesis test of level \\(5\\%\\), with null hypothesis that the percentage of citizens that approve the construction is at least \\(81\\%\\), versus the alternative hypothesis that the percentage is less than \\(81\\%\\).\nCompute the \\(p\\)-value of the test.\n[difficult] Determine the minimum sample size such that the power of the test with significance level \\(\\alpha = 0.05\\) when the real proportion \\(p\\) is \\(0.8\\) is at least \\(50\\%\\).\n\nQuestion 1 & 2.\nLet \\(X\\) be a random variable that represents the opinion about the construction of the movie theater of a citizen taken at random in the population. By definition, \\(X \\sim Be(p)\\), where \\(p\\) is the proportion of citizens that approve the construction. The hypotheses that we want to test here are:\n\\[\nH_0: p \\geq p_0 \\quad v.s. \\quad H_a: p &lt; p_0,\n\\] with \\(p_0 = 81\\%\\). Since \\(X \\sim Be(p)\\), then:\n\\[\nS_n = \\sum_{i=1}^n X_i \\sim Binom(n, p),\n\\] which can be used as test statistic because, under \\(H_0\\), its distribution is binomial with \\(n\\) and \\(p_0\\) parameters which are all known.\nNow, \\(S_n / n\\) is an unbiased estimator of \\(p\\). Hence, data in favor of the alternative hypothesis are those that are far from \\(p_0\\) in the direction of \\(p_a &lt; p_0\\). We can therefore define a rejection region of the form \\(S_n - n p_0\n\\leq c\\) for some \\(c \\in \\mathbb{R}\\). With such a rejection region, the probability of making a type I error reads:\n\\[\n\\mathbb{P}[\\mathrm{type\\ I\\ error}] = \\mathbb{P}_{H_0}[S_n / n - p_0 \\leq c].\n\\]\nWe want to find \\(c\\) such that the probability of making a type I error is upper-bounded by a predefined significance level \\(\\alpha\\) of the test. This is equivalent to finding \\(c\\) such that:\n\\[\n\\begin{aligned}\n& \\mathbb{P}_{H_0}[S_n / n - p_0 \\leq c] = \\alpha \\\\\n\\Longleftrightarrow \\quad & \\mathbb{P}_{H_0}[S_n - n p_0 \\leq n c] = \\alpha \\\\\n\\Longleftrightarrow \\quad & \\mathbb{P}_{H_0}[S_n \\leq n c + n p_0] = \\alpha \\\\\n\\Longleftrightarrow \\quad & n c + n p_0 = q_{Bi(n, p_0)}(\\alpha) \\\\\n\\Longleftrightarrow \\quad & c = \\frac{q_{Bi(n, p_0)}(\\alpha) - n p_0}{n},\n\\end{aligned}\n\\] where \\(q_{Bi(n, p_0)}(\\alpha)\\) is the \\(\\alpha\\)-quantile of the binomial distribution. Hence, the critical region of level \\(\\alpha = 5\\%\\) is:\n\\[\n\\begin{aligned}\n\\mathcal{C}_\\alpha(x_1, \\dots, x_n) &= \\left\\{ (x_1, \\dots, x_n) \\in\n\\mathbb{R}^n \\mid \\overline x_n - p_0 \\leq\n\\frac{q_{Bi(n, p_0)}(\\alpha) - n p_0}{n} \\right\\}, \\\\\n&= \\left\\{ (x_1, \\dots, x_n) \\in \\mathbb{R}^n \\mid n \\overline x_n \\leq\nq_{Bi(n, p_0)}(\\alpha) \\right\\}.\n\\end{aligned}\n\\]\nWe can use the binom.test() function which can help us with the calculations:\n\nout &lt;- binom.test(\n  x = 1890, # Number of citizens that approve the construction in the sample\n  n = 2350, # Total number of citizens in the sample\n  p = 0.81, # Value of p_0\n  alternative = \"less\",  # Type of alternative hypothesis\n  conf.level = 0.95 # Confidence level\n)\n\nThe output of the function is:\n\nout |&gt; \n  broom::tidy() |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.8042553\n1890\n0.2462091\n2350\n0\n0.8176451\nExact binomial test\nless\n\n\n\n\n\n\n\nThe p-value of the test is 0.2462 which exceeds any reasonable significance level. Hence, I lack statistical evidence to reject \\(H_0\\).\nQuestion 3.\nThe power of a test is the probability of rejecting \\(H_0\\) when \\(H_a\\) is true. In our case, the power of the test is:\n\\[\n\\begin{aligned}\n\\mathbb{P}_{H_a}[\\mathrm{reject\\ } H_0] &= \\mathbb{P}_{H_a} \\left[ S_n / n - p_0\n\\leq \\frac{q_{Bi(n, p_0)}(\\alpha) - n p_0}{n} \\right] \\\\\n&= \\mathbb{P}_{H_a}[S_n \\leq q_{Bi(n, p_0)}(\\alpha)] \\\\\n&= F_{Bi(n, p)}(q_{Bi(n, p_0)}(\\alpha)),\n\\end{aligned}\n\\] where \\(F_{Bi(n, p)}\\) is the cumulative distribution function of the binomial distribution with parameters \\(n\\) and \\(p\\), the latter being the real proportion of citizens that approve the construction. Assuming that \\(p = 0.8\\), we want to find \\(n\\) such that:\n\\[\nF_{Bi(n, 0.8)}(q_{Bi(n, 0.81)}(0.05)) \\geq 0.5.\n\\]\nThis equation is not easy to solve analytically. We can use the uniroot() function to find the root of the function \\(f(n) = F_{Bi(n, 0.8)}(q_{Bi(n,\n0.81)}(0.05)) - 0.5\\) which can be implemented as follows:\n\nf &lt;- function(n) {\n  n &lt;- round(n)\n  x &lt;- qbinom(p = 0.05, size = n, prob = 0.81)\n  pbinom(q = x, size = n, prob = 0.8) - 0.5\n}\n\nThe root of the function is:\n\nround(uniroot(f = f, interval = c(1, 100000))$root)\n\n[1] 4208"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-8",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-8",
    "title": "Hypothesis Testing",
    "section": "Exercise 8",
    "text": "Exercise 8\nA computer chip manufacturer claims that no more than \\(1\\%\\) of the chips it sends out are defective. An electronics company, impressed with this claim, has purchased a large quantity of such chips. To determine if the manufacturer’s claim can be taken literally, the company has decided to test a sample of \\(300\\) of these chips. If \\(5\\) of these \\(300\\) chips are found to be defective, should the manufacturer’s claim be rejected?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-9",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-9",
    "title": "Hypothesis Testing",
    "section": "Exercise 9",
    "text": "Exercise 9\nTo determine the impurity level in alloys of steel, two different tests can be used. \\(8\\) specimens are tested, with both procedures, and the results are written in the following table:\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpurity level in a sample of alloys of steel\n\n\nImpurity level\n(Test 1)\nImpurity level\n(Test 2)\nSpecimen\n(Identification number)\n\n\n\n\n1.2\n1.4\n1\n\n\n1.3\n1.7\n2\n\n\n1.7\n2.0\n3\n\n\n1.8\n2.1\n4\n\n\n1.5\n1.5\n5\n\n\n1.4\n1.3\n6\n\n\n1.4\n1.7\n7\n\n\n1.3\n1.6\n8\n\n\n\n\n\n\n\nAssume that the data are normal.\n\nbased on the data in the table, can we state that at significance level \\(\\alpha=5\\%\\) the Test 1 and 2 give a different average level of impurity?\nbased on the data in the table, can we state that at significance level \\(\\alpha=1\\%\\) the Test 2 gives an average level of impurity greater than Test 1?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-10",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-10",
    "title": "Hypothesis Testing",
    "section": "Exercise 10",
    "text": "Exercise 10\nA sample of \\(300\\) voters from region A and \\(200\\) voters from region B showed that the \\(56\\%\\) and the \\(48\\%\\), respectively, prefer a certain candidate. Can we say that at a significance level of \\(5\\%\\) there is a difference between the two regions?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-11",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Solutions.html#exercise-11",
    "title": "Hypothesis Testing",
    "section": "Exercise 11",
    "text": "Exercise 11\nIn a sample of \\(100\\) measures of the boiling temperature of a certain liquid, we obtain a sample mean \\(\\overline{x} = 100^{o}C\\) with a sample variance \\(s^2 =\n0.0098^{o}C^2\\). Assuming that the observation comes from a normal population:\n\nWhat is the smallest level of significance that would lead to reject the null hypothesis that the variance is \\(\\leq 0.015\\)?\nOn the basis of the previous answer, what decision do we take if we fix the level of the test equal to \\(0.01\\)?"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html",
    "href": "08_Types/08-Types-Exercises.html",
    "title": "Data Types",
    "section": "",
    "text": "Use flights to create delayed, a variable that displays whether a flight was delayed (arr_delay &gt; 0).\nThen, remove all rows that contain an NA in delayed.\nFinally, create a summary table that shows:\n\nHow many flights were delayed\n\nWhat proportion of flights were delayed"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-1",
    "href": "08_Types/08-Types-Exercises.html#your-turn-1",
    "title": "Data Types",
    "section": "",
    "text": "Use flights to create delayed, a variable that displays whether a flight was delayed (arr_delay &gt; 0).\nThen, remove all rows that contain an NA in delayed.\nFinally, create a summary table that shows:\n\nHow many flights were delayed\n\nWhat proportion of flights were delayed"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-2",
    "href": "08_Types/08-Types-Exercises.html#your-turn-2",
    "title": "Data Types",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nFill in the blanks to:\n\nIsolate the last letter of every name\nCreate a logical variable that displays whether the last letter is one of “a”, “e”, “i”, “o”, “u”, or “y”.\nUse a weighted mean to calculate the proportion of children whose name ends in a vowel (by year and sex)\n\nand then display the results as a line plot.\n\n(Hint: Be sure to remove each _ before turning eval to true)\n\nbabynames |&gt; \n  _______(last = _________, \n          vowel = __________) |&gt; \n  group_by(__________) |&gt; \n  _________(p_vowel = weighted.mean(vowel, n)) |&gt; \n  _________ +\n  __________"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-3",
    "href": "08_Types/08-Types-Exercises.html#your-turn-3",
    "title": "Data Types",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nRepeat the demonstration, some of whose code is below, to make a sensible graph of average TV consumption by marital status.\n(Hint: Be sure to remove each _ before turning eval to true)\n\ngss_cat |&gt; \n  filter(_is.na(________)) |&gt;\n  group_by(________) |&gt;\n  summarise(_________________) |&gt;\n  ggplot() +\n    geom_point(mapping = aes(x = _______, y = _________________________))"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-4",
    "href": "08_Types/08-Types-Exercises.html#your-turn-4",
    "title": "Data Types",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nDo you think liberals or conservatives watch more TV? Compute average tv hours by party ID an then plot the results."
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-5",
    "href": "08_Types/08-Types-Exercises.html#your-turn-5",
    "title": "Data Types",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nWhat is the best time of day to fly?\nUse the hour and minute variables in flights to make a new variable that shows the time of each flight as an hms.\nThen use a smooth line to plot the relationship between time of day and arr_delay."
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-6",
    "href": "08_Types/08-Types-Exercises.html#your-turn-6",
    "title": "Data Types",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nWhat is the best day of the week to fly?\nLook at the code skeleton for Your Turn 7. Discuss with your neighbor:\n\nWhat does each line do?\nWhat will the missing parts need to do?"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-7",
    "href": "08_Types/08-Types-Exercises.html#your-turn-7",
    "title": "Data Types",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nFill in the blank to:\nExtract the day of the week of each flight (as a full name) from time_hour.\nPlot the average arrival delay by day as a column chart (bar chart).\n(Hint: Be sure to remove each _ before turning eval to true)\n\nflights |&gt; \n  mutate(weekday = _______________________________) |&gt; \n  group_by(weekday) |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  summarise(avg_delay = mean(arr_delay)) |&gt; \n  ggplot() +\n    geom_col(mapping = aes(x = weekday, y = avg_delay))"
  },
  {
    "objectID": "05_Report/05-Report-Parameters.html",
    "href": "05_Report/05-Report-Parameters.html",
    "title": "Garrett",
    "section": "",
    "text": "There have been 130211 children named Garrett. The name Garrett was most popular in 2000, when there were 5840 boys named Garrett. Garrett is traditionally a boy’s name."
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html",
    "href": "06_Tidy/06-Tidy-Exercises.html",
    "title": "Tidy Data",
    "section": "",
    "text": "table1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\ntable5\n\n# A tibble: 6 × 4\n  country     century year  rate             \n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan 19      99    745/19987071     \n2 Afghanistan 20      00    2666/20595360    \n3 Brazil      19      99    37737/172006362  \n4 Brazil      20      00    80488/174504898  \n5 China       19      99    212258/1272915272\n6 China       20      00    213766/1280428583"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-1",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-1",
    "title": "Tidy Data",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nOn a sheet of paper, draw how the cases data set would look if it had the same values grouped into three columns: country, year, n"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-2",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-2",
    "title": "Tidy Data",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nUse pivot_longer() to reorganize table4a into three columns: country, year, and cases."
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-3",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-3",
    "title": "Tidy Data",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nOn a sheet of paper, draw how this data set would look if it had the same values grouped into three columns: city, large, small"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-4",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-4",
    "title": "Tidy Data",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nUse pivot_wider() to reorganize table2 into four columns: country, year, cases, and population."
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-5",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-5",
    "title": "Tidy Data",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nGather the 5th through 60th columns of who into a pair of key:value columns named codes and n.\nThen select just the county, year, codes and n variables.\n\nwho\n\n# A tibble: 7,240 × 60\n   country  iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghani… AF    AFG    1980          NA           NA           NA           NA\n 2 Afghani… AF    AFG    1981          NA           NA           NA           NA\n 3 Afghani… AF    AFG    1982          NA           NA           NA           NA\n 4 Afghani… AF    AFG    1983          NA           NA           NA           NA\n 5 Afghani… AF    AFG    1984          NA           NA           NA           NA\n 6 Afghani… AF    AFG    1985          NA           NA           NA           NA\n 7 Afghani… AF    AFG    1986          NA           NA           NA           NA\n 8 Afghani… AF    AFG    1987          NA           NA           NA           NA\n 9 Afghani… AF    AFG    1988          NA           NA           NA           NA\n10 Afghani… AF    AFG    1989          NA           NA           NA           NA\n# ℹ 7,230 more rows\n# ℹ 52 more variables: new_sp_m4554 &lt;dbl&gt;, new_sp_m5564 &lt;dbl&gt;,\n#   new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;, new_sp_f1524 &lt;dbl&gt;,\n#   new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;, new_sp_f4554 &lt;dbl&gt;,\n#   new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;, new_sn_m014 &lt;dbl&gt;,\n#   new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;, new_sn_m3544 &lt;dbl&gt;,\n#   new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;, new_sn_m65 &lt;dbl&gt;, …"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-6",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-6",
    "title": "Tidy Data",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nSeparate the sexage column into sex and age columns.\n(Hint: Be sure to remove each _ before running the code and switch eval option to true)\n\nwho |&gt; \n  pivot_longer(cols = 5:60, names_to = \"codes\", values_to = \"n\") |&gt; \n  select(-iso2, -iso3) |&gt; \n  separate(codes, c(\"new\", \"type\", \"sexage\"), sep = \"_\") |&gt; \n  select(-new) |&gt; \n  _______________________________"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-7",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-7",
    "title": "Tidy Data",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nExtend this code to reshape the data into a data set with three columns:\n\nyear\nM\nF\n\nCalculate the percent of male (or female) children by year. Then plot the percent over time.\n\nbabynames |&gt; \n  group_by(year, sex) |&gt; \n  summarise(n = sum(n))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 276 × 3\n# Groups:   year [138]\n    year sex        n\n   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n 1  1880 F      90993\n 2  1880 M     110491\n 3  1881 F      91953\n 4  1881 M     100743\n 5  1882 F     107847\n 6  1882 M     113686\n 7  1883 F     112319\n 8  1883 M     104627\n 9  1884 F     129020\n10  1884 M     114442\n# ℹ 266 more rows"
  },
  {
    "objectID": "Evaluations/2024_2025/02_Homework/project.html",
    "href": "Evaluations/2024_2025/02_Homework/project.html",
    "title": "Car accidents in France",
    "section": "",
    "text": "This page explains the project proposed to dig deeper into Quarto and its features. The project is about car accidents in France. The data is available on the Open Data platform of the French government. The data is available in the form of several CSV files. The data contains information about car accidents in France from 2005 to 2023. The data contains information about the date and time of the accident, the location of the accident, the number of people involved in the accident, the number of people injured, the number of people killed, the type of vehicles involved in the accident, the type of road, the weather conditions, the lighting conditions, the type of intersection, the type of collision, the type of accident, the causes of the accident, etc."
  },
  {
    "objectID": "Evaluations/2024_2025/02_Homework/project.html#description-of-the-data-set",
    "href": "Evaluations/2024_2025/02_Homework/project.html#description-of-the-data-set",
    "title": "Car accidents in France",
    "section": "",
    "text": "This page explains the project proposed to dig deeper into Quarto and its features. The project is about car accidents in France. The data is available on the Open Data platform of the French government. The data is available in the form of several CSV files. The data contains information about car accidents in France from 2005 to 2023. The data contains information about the date and time of the accident, the location of the accident, the number of people involved in the accident, the number of people injured, the number of people killed, the type of vehicles involved in the accident, the type of road, the weather conditions, the lighting conditions, the type of intersection, the type of collision, the type of accident, the causes of the accident, etc."
  },
  {
    "objectID": "Evaluations/2024_2025/02_Homework/project.html#aims-of-the-project",
    "href": "Evaluations/2024_2025/02_Homework/project.html#aims-of-the-project",
    "title": "Car accidents in France",
    "section": "Aims of the project",
    "text": "Aims of the project\nThe project aims to dig into interactive Quarto dashboards to analyze the data and extract insights. The project can involve the following steps:\n\nData collection: Download the data from the Open Data platform of the French government. You can use packages such as {readr} or {readxl} to read the data into R.\nData preprocessing: Clean the data, handle missing values, and transform the data into a suitable format for analysis. For this task, you can use:\n\n{dplyr} and {tidyr} and other tidyverse packages for data manipulation and reshaping;\n{skimr} to get a summary of the data;\n{janitor} to clean the data;\n{naniar} to visualize missing values and take informed decisions about handling them.\n\nData analysis: Use Quarto Dashboard and shiny to create interactive dashboards to analyze the data and extract insights. See https://quarto.org/docs/dashboards/interactivity/ for easy interactivity in Quarto dashboard using Shiny.\nData visualization: Use {ggplot2} and {plotly} to create interactive visualizations to present the insights.\nSpatial Data: Use appropriate packages to create maps to visualize the location of accidents with superimposed relevant information.\n\nThe project will help to understand the patterns and trends in car accidents in France and identify the factors that contribute to accidents. The project will also help to explore the relationship between different variables and their impact on accidents. The project will be a good example to showcase the capabilities of Quarto for data analysis and visualization."
  },
  {
    "objectID": "Evaluations/2024_2025/02_Homework/project.html#rules-expected-delivrables",
    "href": "Evaluations/2024_2025/02_Homework/project.html#rules-expected-delivrables",
    "title": "Car accidents in France",
    "section": "Rules & Expected delivrables",
    "text": "Rules & Expected delivrables\nThe following delivrables are expected from the project on Monday, 6th January 2025 at 11:59 PM at the latest:\n\nInteractive dashboards to analyze the data and extract insights.\nA Quarto document detailing the thinking process and actual steps taken that led to the provided dashboards.\n\nHence, delivrables should be made of two .qmd files.\nThe project is to be done in groups of 3 students. You should email the instructor with the names of the students in your group by Tuesday, 10th December 2024. The instructor will assign a group number to each group. The group number should be included in the Quarto document and the interactive dashboards.\n\n\n\n\n\n\nDelivrable\n\n\n\nDelivrable is expected to be a ZIP archive containing the two .qmd files (report and dashboard) along with all the necessary data files.\nPlease ensure that both files compile withour error."
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html",
    "href": "03_Transform/03-Transform-Exercises.html",
    "title": "Transform Data",
    "section": "",
    "text": "Make sure that the .csv file is in the same directory as the .qmd file you’re currently working on. Then import the babynames.csv data set. Give it the name babynames. Then copy the import code into the code chunk below. Does it run?\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-1",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-1",
    "title": "Transform Data",
    "section": "",
    "text": "Make sure that the .csv file is in the same directory as the .qmd file you’re currently working on. Then import the babynames.csv data set. Give it the name babynames. Then copy the import code into the code chunk below. Does it run?\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-2",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-2",
    "title": "Transform Data",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nAlter the code to select just the n column:\n\nselect(babynames, name, prop)\n\n# A tibble: 1,924,665 × 2\n   name        prop\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Mary      0.0724\n 2 Anna      0.0267\n 3 Emma      0.0205\n 4 Elizabeth 0.0199\n 5 Minnie    0.0179\n 6 Margaret  0.0162\n 7 Ida       0.0151\n 8 Alice     0.0145\n 9 Bertha    0.0135\n10 Sarah     0.0132\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#quiz",
    "href": "03_Transform/03-Transform-Exercises.html#quiz",
    "title": "Transform Data",
    "section": "Quiz",
    "text": "Quiz\nWhich of these is NOT a way to select the name and n columns together?\n\nselect(babynames, -c(year, sex, prop))\n\n# A tibble: 1,924,665 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Mary       7065\n 2 Anna       2604\n 3 Emma       2003\n 4 Elizabeth  1939\n 5 Minnie     1746\n 6 Margaret   1578\n 7 Ida        1472\n 8 Alice      1414\n 9 Bertha     1320\n10 Sarah      1288\n# ℹ 1,924,655 more rows\n\nselect(babynames, name:n)\n\n# A tibble: 1,924,665 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Mary       7065\n 2 Anna       2604\n 3 Emma       2003\n 4 Elizabeth  1939\n 5 Minnie     1746\n 6 Margaret   1578\n 7 Ida        1472\n 8 Alice      1414\n 9 Bertha     1320\n10 Sarah      1288\n# ℹ 1,924,655 more rows\n\nselect(babynames, starts_with(\"n\"))\n\n# A tibble: 1,924,665 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Mary       7065\n 2 Anna       2604\n 3 Emma       2003\n 4 Elizabeth  1939\n 5 Minnie     1746\n 6 Margaret   1578\n 7 Ida        1472\n 8 Alice      1414\n 9 Bertha     1320\n10 Sarah      1288\n# ℹ 1,924,655 more rows\n\nselect(babynames, ends_with(\"n\"))\n\n# A tibble: 1,924,665 × 1\n       n\n   &lt;int&gt;\n 1  7065\n 2  2604\n 3  2003\n 4  1939\n 5  1746\n 6  1578\n 7  1472\n 8  1414\n 9  1320\n10  1288\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-3",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-3",
    "title": "Transform Data",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nUse filter, babynames, and the logical operators to find:\n\nAll of the names where prop is greater than or equal to 0.08\n\nAll of the children named “Sea”"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-4",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-4",
    "title": "Transform Data",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nUse Boolean operators to return only the rows that contain:\n\nBoys named Sue\n\nNames that were used by exactly 5 or 6 children in 1880\n\nNames that are one of Acura, Lexus, or Yugo"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#help-me",
    "href": "03_Transform/03-Transform-Exercises.html#help-me",
    "title": "Transform Data",
    "section": "Help Me",
    "text": "Help Me\nWhat is the smallest value of n? What is the largest?"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-5",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-5",
    "title": "Transform Data",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nUse |&gt; to write a sequence of functions that:\n\nFilters babynames to just the girls that were born in 2017, then…\n\nSelects the name and n columns, then…\n\nArranges the results so that the most popular names are near the top."
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-6",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-6",
    "title": "Transform Data",
    "section": "Your Turn 6",
    "text": "Your Turn 6\n\nTrim babynames to just the rows that contain your name and your sex\n\nTrim the result to just the columns that will appear in your graph (not strictly necessary, but useful practice)\nPlot the results as a line graph with year on the x axis and prop on the y axis"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-7",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-7",
    "title": "Transform Data",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nCpmplete the code below to extract the rows where name == \"Khaleesi\". Then use summarise() and sum() and min() to find:\n\nThe total number of children named Khaleesi\nThe first year Khaleesi appeared in the data\n\nMake sure to turn the eval option to true before running the code.\n(Hint: Be sure to remove each _ before running the code)\n\nbabynames ___ \n  filter(_______________________) ___\n  ___________(total = ________, first = _______)"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-8",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-8",
    "title": "Transform Data",
    "section": "Your Turn 8",
    "text": "Your Turn 8\nUse group_by(), summarise(), and arrange() to display the ten most popular names. Compute popularity as the total number of children of a single gender given a name.\nMake sure to turn the eval option to true before running the code.\n(Hint: Be sure to remove each _ before running the code)\n\nbabynames |&gt; \n  _______(name, sex) |&gt; \n  _______(total = _____(n)) |&gt; \n  _______(desc(_____))"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-9",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-9",
    "title": "Transform Data",
    "section": "Your Turn 9",
    "text": "Your Turn 9\nUse group_by() to calculate and then plot the total number of children born each year over time."
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-10",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-10",
    "title": "Transform Data",
    "section": "Your Turn 10",
    "text": "Your Turn 10\nUse mutate() and min_rank()to rank each row in babynames from largest n to lowest n.\nMake sure to turn the eval option to true before running the code.\n(Hint: Be sure to remove each _ before running the code)\n\nbabynames |&gt; \n  ______(rank = _______(____(prop)))"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-11",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-11",
    "title": "Transform Data",
    "section": "Your Turn 11",
    "text": "Your Turn 11\nGroup babynames by year and then re-rank the data. Filter the results to just rows where rank == 1."
  },
  {
    "objectID": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html",
    "href": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html",
    "title": "Data Science with R",
    "section": "",
    "text": "Global Instructions\n\n\n\n\nData Sets\n\nAll data sets are available at here. The ZIP file contains two files: an RDS file and a CSV file.\n\nDeliverable\n\nAll you have to do is send the QMD file with your answers and your name as author by mail to the instructor."
  },
  {
    "objectID": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html#exercise-1",
    "href": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html#exercise-1",
    "title": "Data Science with R",
    "section": "Exercise 1",
    "text": "Exercise 1\nAnalysis of the mussels data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nlength\nLength of a mussel (mm)\n\n\nwidth\nWidth of a mussel (mm)\n\n\nheight\nHeight of a mussel (mm)\n\n\nsize\nMass of a mussel (g)\n\n\nweight\nWeight of eatable part of a mussel (g)\n\n\n\nWe want to study how the eatable part of a mussel varies as a function of the other four variables using a multiple linear regression.\n\n\n\n\n\nMussel Data Set"
  },
  {
    "objectID": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html#exercise-2",
    "href": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html#exercise-2",
    "title": "Data Science with R",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nA data set on taxi prices\nThis dataset is designed to predict taxi trip fares based on various factors such as distance, time of day, traffic conditions, and more. It provides realistic synthetic data for regression tasks, offering a unique opportunity to explore pricing trends in the taxi industry.\n\nKey Features\n\n\nDistance (in kilometers): The length of the trip.\nPickup Time: The starting time of the trip.\nDropoff Time: The ending time of the trip.\nTraffic Condition: Categorical indicator of traffic (light, medium, heavy).\nPassenger Count: Number of passengers for the trip.\nWeather Condition: Categorical data for weather (clear, rain, snow).\nTrip Duration (in minutes): Total trip time.\nFare Amount (target): The cost of the trip (in USD).\n\n\n\n\n\nInstructions\nThe code cell below importa the data from taxi_trip_pricing.csv, stores it as taxi_pricing and display its content using the gt package:\n\ntaxi_pricing &lt;- read_csv(\"taxi_trip_pricing.csv\", show_col_types = FALSE)\ntaxi_pricing |&gt; \n  gt::gt() |&gt; \n  gt::tab_header(\"Taxi Trip Pricing\") |&gt; \n  gt::cols_label(\n    Trip_Distance_km = gt::html(\"Distance&lt;br&gt;(km)\"),\n    Time_of_Day = \"Pick-up time of the day\",\n    Day_of_Week = \"Pick-up day\",\n    Passenger_Count = \"Number of passengers\",\n    Traffic_Conditions = \"Traffic conditions\",\n    Weather = \"Weather\",\n    Base_Fare = gt::html(\"Base fare&lt;br&gt;(&#36;)\"),\n    Per_Km_Rate = gt::html(\"Rate per km&lt;br&gt;(&#36;)\"),\n    Per_Minute_Rate = gt::html(\"Rate per min&lt;br&gt;(&#36;)\"),\n    Trip_Duration_Minutes = gt::html(\"Trip duration&lt;br&gt;(min)\"),\n    Trip_Price = gt::html(\"Trip price&lt;br&gt;(&#36;)\")\n  ) |&gt; \n  gt::opt_interactive()\n\n\n\n\nTaxi Trip Pricing\n\n\n\n\n\n\n\nUsing the taxi_pricing data set:\n\nDescribe the data set;\nVisualize the data;\nBuild a model to predict fare amount paying attention to (multi)collinearity issues, model selection, validating linear regression assumptions. Finally present and discuss the effects of what you think the best model is.\n\nDiscuss all your choices."
  },
  {
    "objectID": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html#exercise-3",
    "href": "Evaluations/2024_2025/03_Final_Exam/FinalExam_20250116.html#exercise-3",
    "title": "Data Science with R",
    "section": "Exercise 3",
    "text": "Exercise 3\nA company produces barbed wire in skeins of \\(100\\)m each, nominally. The real length of the skeins is a random variable \\(X\\) distributed as a \\(\\mathcal{N}(\\mu, \\sigma^2)\\). Measuring \\(10\\) skeins, we get the following lengths:\n\n\n\n\n\n\n\n\n\n\n\n\nSkeins Data Set\n\n\nLength\n(m)\nIdentification\nNumber\n\n\n\n\n\n98.683\n1\n\n\n96.599\n2\n\n\n99.617\n3\n\n\n102.544\n4\n\n\n100.110\n5\n\n\n102.000\n6\n\n\n98.394\n7\n\n\n100.324\n8\n\n\n98.743\n9\n\n\n103.247\n10\n\n\n\n\n\n\n\n\nPerform a conformity test at significance level \\(\\alpha = 5\\%\\) (define a proper mathematical model for the problem, appropriate hypotheses, a test statistic and the critical region of level \\(\\alpha\\)).\nDetermine, on the basis of the observed values, the p-value of the test.\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition. If \\(Z\\) is a standard normal random variable and \\(V\\) is a chi-squared random variable with \\(\\nu\\) degrees of freedom, then the random variable:\n\\[\nT = \\frac{Z + \\rho}{\\sqrt{V / \\nu}}\n\\]\nis said to have a non-central t-distribution with \\(\\nu\\) degrees of freedom and non-centrality parameter \\(\\rho\\).\nR implementation. The function pt() computes the cumulative distribution function of a t-distribution. The function qt() computes the quantile function of a t-distribution. The function rt() generates random numbers from a t-distribution. ALl these functions have an argument ncp which is the non-centrality parameter \\(\\delta\\).\n\n\nLet \\[\nT_0 = \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma}\n\\] be the test statistic and define the following random variable: \\[\nT = \\sqrt{n} \\frac{\\overline X - \\mu}{\\sigma}.\n\\]\n\nWhat is the distribution of \\(T\\)?\n\nThe statistical power of the test is defined as \\(\\gamma = \\mathbb{P}_{H_a}(\\mathrm{Reject }H_0)\\). Let also \\(\\delta\\) be the difference between the true mean and the nominal mean, i.e. \\(\\delta = \\mu - \\mu_0\\).\n\nPlot the power curve which if \\(\\gamma\\) as a function of \\(\\delta\\)."
  },
  {
    "objectID": "Evaluations/2024_2025/01_First_Exam/exam1.html",
    "href": "Evaluations/2024_2025/01_First_Exam/exam1.html",
    "title": "First Exam",
    "section": "",
    "text": "The data set you are going to work on is the penguins data set. The data set contains data on the body mass, flipper length, species of penguins. The data set is available in the {palmerpenguins} package. You can install the package using the following code:\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "Evaluations/2024_2025/01_First_Exam/exam1.html#data",
    "href": "Evaluations/2024_2025/01_First_Exam/exam1.html#data",
    "title": "First Exam",
    "section": "",
    "text": "The data set you are going to work on is the penguins data set. The data set contains data on the body mass, flipper length, species of penguins. The data set is available in the {palmerpenguins} package. You can install the package using the following code:\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "Evaluations/2024_2025/01_First_Exam/exam1.html#goal",
    "href": "Evaluations/2024_2025/01_First_Exam/exam1.html#goal",
    "title": "First Exam",
    "section": "Goal",
    "text": "Goal\nThe goal of this exam is to reproduce the following plot using the penguins data set:\n\nHint: You need to carefully analze the plot to understand how you need to manipulate the original data set in order to get the correct answer. For the sake of clarity, with fixed slopes means that the slopes of the lines are the same for all species and sexes. with varying slopes means that the slopes of the lines are different for each species and sex."
  },
  {
    "objectID": "Evaluations/2024_2025/01_First_Exam/exam1.html#rules-expected-delivrables",
    "href": "Evaluations/2024_2025/01_First_Exam/exam1.html#rules-expected-delivrables",
    "title": "First Exam",
    "section": "Rules & Expected delivrables",
    "text": "Rules & Expected delivrables\nThe exam starts at 11am on Friday, 13th December 2024 and ends at 12pm on the same day. You are not allowed to communicate with anyone during the exam.\nThe following delivrable is expected by email to the instructor by 12:15pm on Friday, 13th December 2024:\n\nA Quarto file which renders without error with the code to reproduce the plot. The name of the file must be of the form exam1_&lt;your_name&gt;.qmd."
  },
  {
    "objectID": "05_Report/05-Report-Exercises.html",
    "href": "05_Report/05-Report-Exercises.html",
    "title": "Garrett",
    "section": "",
    "text": "There have been TODO children named Garrett. The name Garrett was most popular in TODO, when TODO TODO were named Garrett. Garrett is traditionally a TODO name."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Instructions for the class",
    "section": "",
    "text": "This repository is build on the work of Garrett Grolemund from posit. In particular, it reuses an important part of the material he developed for tidyverse-related workshops, which is available at https://github.com/rstudio-education/remaster-the-tidyverse under the Creative Commons BY-SA 4.0 copyright."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Instructions for the class",
    "section": "",
    "text": "This repository is build on the work of Garrett Grolemund from posit. In particular, it reuses an important part of the material he developed for tidyverse-related workshops, which is available at https://github.com/rstudio-education/remaster-the-tidyverse under the Creative Commons BY-SA 4.0 copyright."
  },
  {
    "objectID": "index.html#material",
    "href": "index.html#material",
    "title": "Instructions for the class",
    "section": "Material",
    "text": "Material\nThe main webpage is at https://astamm.github.io/data-science-with-r/."
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Instructions for the class",
    "section": "Outline",
    "text": "Outline\n\nData wrangling with R\nThe class is organised in 9 parts each of which has its own set of slides and exercises. The slides are available in the above Data Wranging - Slides tab and the exercises in the above Data Wranging - Labs tab. The slides are written partly with Keynote (exported as PDFs) and partly in Quarto reveajs slides. The exercises are written in Quarto.\n\n\n\nPart\nTitle\nSlides\nExercises\nSuppl. Material\n\n\n\n\n1\nIntroduction\nPDF\nQuarto\n\n\n\n2\nVisualize Data\nPDF\nQuarto\n\n\n\n3\nTransform Data\nPDF\nQuarto\nCSV\n\n\n4\nModel Data\nPDF\nQuarto\nZIP\n\n\n5\nCommunicate Data\nPDF\nQuarto\nQuarto\n\n\n6\nTidy Data\nPDF\nQuarto\n\n\n\n7\nJoin Data\nPDF\nQuarto\n\n\n\n8\nManipulate Data Types\nPDF\nQuarto\n\n\n\n9\nManipulate Lists\nPDF\nQuarto"
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Instructions for the class",
    "section": "Requirements",
    "text": "Requirements\n\nR: https://www.r-project.org\nRStudio: https://posit.co/download/rstudio-desktop/ ou Positron https://positron.posit.co/download.html\nQuarto: https://quarto.org/docs/get-started/\nTidyverse: https://www.tidyverse.org\nSpecific R packages by theme:\n\nData sets:\n\n{babynames}: a data set of frequency of baby names in the US from 1880 to 2017.\n{ncyflights13}: information about all flights that departed from NYC (e.g. EWR, JFK and LGA) to destinations in the United States, Puerto Rico, and the American Virgin Islands) in 2013: 336,776 flights in total.\n\nData wrangling:\n\n{janitor}: simple functions for examining and cleaning dirty data.\n{skimr}: a frictionless approach to summary statistics which conforms to the principle of least surprise, displaying summary statistics the user can skim quickly to understand their data.\n{tidyverse}: an opinionated collection of R packages designed for data science.\n\nExtra data visualization packages:\n\n{ggcorrplot}: visualize easily a correlation matrix using ‘ggplot2’.\n{ggfortify}: unified plotting tools for statistics commonly used, such as GLM, time series, PCA families, clustering and survival analysis.\n{plotly}: an interactive plotting library.\n\nReporting\n\n{DT}: an R interface to the JavaScript library DataTables.\n{gt}: produce nice-looking display tables.\n{kableExtra}: construct complex table with knitr::kable() + |&gt;.\n\nModel summaries:\n\n{broom}: a package that provides tidy summaries of model outputs."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#general-data-modeling-framework",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#general-data-modeling-framework",
    "title": "Hypothesis Testing",
    "section": "General data modeling framework",
    "text": "General data modeling framework\n\n\nYou have some data \\(\\{x_1, \\dots, x_n\\}\\) that you presume have been sampled independently from their corresponding random variables \\(\\{X_1, \\dots, X_n\\}\\);\nYou formulate a simple hypothesis \\(H_0\\), called null hypothesis, about the distribution from which this data were sampled;\nYou want to confront this hypothesis against an alternative hypothesis \\(H_a\\) using your finite amount of data;\nYou use a test statistic \\(T(X_1, \\dots, X_n)\\) that depends on the sample (and thus that you can calculate anytime you observe a sample) and of which you know (an approximation of) the distribution when you assume that your null hypothesis is true; it is called the null distribution of the test statistic \\(T\\);\nYou compute the value \\(t_0\\) of this test statistic with your observed data;\nYou strongly reject \\(H_0\\) if \\(t_0\\) falls on the tails of the null distribution of \\(T\\); or,\nYou lack evidence to reject \\(H_0\\) if \\(t_0\\) ends up in the central part of the null distribution of \\(T\\).\nWarning: It is straightforward from this setup to understand that the problem is not symmetric in the hypotheses. Indeed, the procedure relies on what happens when \\(H_0\\) is true but does not depend on \\(H_a\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#theoretical-aspects",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#theoretical-aspects",
    "title": "Hypothesis Testing",
    "section": "Theoretical aspects",
    "text": "Theoretical aspects\n\nYou design a test statistic \\(T(X_1, \\dots, X_n)\\) for the purpose of performing this test which must satisfy at least the first three of the following four properties:\n\nIt evaluates to real values;\nYou have all the required knowledge to compute its value once you observe the data;\nIf \\(H_0\\) is true, then small values of the statistic should comfort you with the idea that \\(H_0\\) is a reasonable assumption, while larger values of the statistic should generate suspicion about \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\);\n[optional] If \\(H_0\\) is true, then it can be very helpful to have access to the (asymptotic) distribution of the test statistic under classical assumptions (such as normality)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean",
    "title": "Hypothesis Testing",
    "section": "Example: Test on the mean",
    "text": "Example: Test on the mean\n\nSuppose you have a sample of \\(n\\) i.i.d. random variables \\(X_1, \\dots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) and you know the value of \\(\\sigma^2\\). We want to test whether the mean \\(\\mu\\) of the distribution is equal to some pre-specified value \\(\\mu_0\\). We can therefore use \\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\). At this point, a good candidate test statistic to look at for performing this test is: \\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma}, \\quad \\mbox{with} \\quad \\overline X := \\frac{1}{n} \\sum_{i=1}^n X_i. \\]\n\nIt evaluates to real values;\nYou have all the required knowledge to compute its value once you observe the data (\\(\\overline x\\));\nThe sample mean \\(\\overline X\\) is an unbiased estimator of the true unknown mean \\(\\mu\\); so, if \\(H_0\\) is true, then \\(\\overline X\\) will produce values that are close to \\(\\mu_0\\); hence, small values of \\(T\\) will comfort you with the idea that \\(H_0\\) is a reasonable assumption, while larger values, both positive or negative, of the statistic should generate suspicion about \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\);\nIf you assume normality and independence of the sample, then \\(T \\sim \\mathcal{N}(0, 1)\\) under \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#distribution-of-the-test-statistic-under-h_0",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#distribution-of-the-test-statistic-under-h_0",
    "title": "Hypothesis Testing",
    "section": "Distribution of the test statistic under \\(H_0\\)",
    "text": "Distribution of the test statistic under \\(H_0\\)\n\n\nParametric testing. If you designed your test statistic carefully, you might have access to its theoretical distribution when \\(H_0\\) is true under distributional assumptions about the data. This is called parametric hypothesis testing.\nAsymptotic testing. If it is not the case, you can often derive the theoretical distribution of the statistic under the null hypothesis asymptotically, i.e. assuming that you have a large sample (\\(n \\gg 1\\)); this is called asymptotic hypothesis testing.\nBootstrap testing. If you are in a large sample size regime but still cannot have access to the theoretical distribution of your test statistic, you can approach this distribution using bootstrapping; this is called bootstrap hypothesis testing.\nPermutation testing. If you are in a low sample size regime, then you can approach the distribution of the test statistic using permutations; this is called permutation hypothesis testing."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#two-sided-vs.-one-sided-hypothesis-tests",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#two-sided-vs.-one-sided-hypothesis-tests",
    "title": "Hypothesis Testing",
    "section": "Two-sided vs. one-sided hypothesis tests",
    "text": "Two-sided vs. one-sided hypothesis tests\n\nDepending on what you put into the alternative hypothesis \\(H_a\\), larger values of the test statistic that raise suspicion regarding the validity of \\(H_0\\) might mean:\n\nlarger values only on the right tail of the null distribution of \\(T\\);\nlarger values only on the left tail of the null distribution of \\(T\\);\nlarger values on both tails.\n\nIn the first two cases, we say that the test is one-sided. In the latter case, we say that the test is two-sided because we interpret large values in both tails as suspicious."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean-continued",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean-continued",
    "title": "Hypothesis Testing",
    "section": "Example: Test on the mean (continued)",
    "text": "Example: Test on the mean (continued)\n\nSuppose you have a sample of \\(n\\) i.i.d. random variables \\(X_1, \\dots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) and you know the value of \\(\\sigma^2\\). We want to test whether the mean \\(\\mu\\) of the distribution is equal to some pre-specified value \\(\\mu_0\\). As we have seen, a good candidate test statistic to look at for performing this test is:\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma}, \\quad \\mbox{with} \\quad \\overline X := \\frac{1}{n} \\sum_{i=1}^n X_i. \\]\nNow, using this test statistic, we might be interested in performing three different tests:\n\n\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &gt; \\mu_0\\);\n\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &lt; \\mu_0\\);\n\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0",
    "title": "Hypothesis Testing",
    "section": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu > \\mu_0\\)",
    "text": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &gt; \\mu_0\\)\n\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma} \\] Remember that we must look at large values of \\(T\\) that raise suspicion in favor of \\(H_a\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\overline X\\) is an unbiased estimator of the true mean.\nLarge negative values of \\(T\\) that happen when the true mean is far less than \\(\\mu_0\\) does not raise suspicion in favor of \\(H_a\\).\nLarge positive values of \\(T\\) that happen when the true mean is far more than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nConclusion: we are interested only in the right tail of the null distribution of \\(T\\) to find evidence to reject \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0-1",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0-1",
    "title": "Hypothesis Testing",
    "section": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu < \\mu_0\\)",
    "text": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &lt; \\mu_0\\)\n\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma} \\] Remember that we must look at large values of \\(T\\) that raise suspicion in favor of \\(H_a\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\overline X\\) is an unbiased estimator of the true mean.\nLarge negative values of \\(T\\) that happen when the true mean is far more than \\(\\mu_0\\) does not raise suspicion in favor of \\(H_a\\).\nLarge positive values of \\(T\\) that happen when the true mean is far less than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nConclusion: we are interested only in the left tail of the null distribution of \\(T\\) to find evidence to reject \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-ne-mu_0",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-ne-mu_0",
    "title": "Hypothesis Testing",
    "section": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\)",
    "text": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\)\n\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma} \\] Remember that we must look at large values of \\(T\\) that raise suspicion in favor of \\(H_a\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\overline X\\) is an unbiased estimator of the true mean.\nLarge negative values of \\(T\\) that happen when the true mean is far more than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nLarge positive values of \\(T\\) that happen when the true mean is far less than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nConclusion: we are interested in both the left and the right tails of the null distribution of \\(T\\) to find evidence to reject \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#type-i-and-type-ii-errors",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#type-i-and-type-ii-errors",
    "title": "Hypothesis Testing",
    "section": "Type I and Type II errors",
    "text": "Type I and Type II errors\n\nSo we are now at a point where we know which tail(s) which we should look at and we now need to make a decision as to what large means. In other words, above which threshold on all possible values of my test statistic should I consider that I can reject \\(H_0\\).\n\nNotice that you are going to take this decision based on the null distribution.\nWhen you decide to reject or not, you might make an error:\n\n\n\n\n\n\nDecision\n\\(H_0\\) is true\n\\(H_a\\) is true\n\n\n\n\nDo not reject \\(H_0\\)\nWell done\nType II error\n\n\nReject \\(H_0\\)\nType I error\nWell done\n\n\n\n\n\n\nThe only error rate you can control is the type I error rate because it is a probability computed assuming that the null hypothesis is true, which is exactly the situation we put ourselves in for making the decision (i.e. looking at the tails of the null distribution of \\(T\\))."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level",
    "title": "Hypothesis Testing",
    "section": "Significance level",
    "text": "Significance level\n\n\nAt this point, you can decide that you do not want to make more than a certain amount of type I errors. So you want to force that \\(\\mathbb{P}_{H_0}(\\mbox{reject } H_0) \\le \\alpha\\), for some upper bound threshold \\(\\alpha\\) on the probability of type I errors. This threshold is called significance level of the test and is often denoted by the greek letter \\(\\alpha\\).\nLet us now translate what this rule implies for the right-tail alternative case. The event “\\(\\mbox{reject } H_0\\)” translates in this case into \\(T &gt; x\\) for some \\(x\\) value of the test statistic \\(T\\). Hence, the rule becomes: \\[ \\mathbb{P}_{H_0}(T &gt; x) \\le \\alpha \\\\ \\Leftrightarrow 1 - \\mathbb{P}_{H_0}(T \\le x) \\le \\alpha \\\\ \\Leftrightarrow 1 - F_T^{\\left(H_0\\right)}(x) \\le \\alpha \\\\ \\Leftrightarrow F_T^{\\left(H_0\\right)}(x) \\ge 1 - \\alpha \\] verified for all \\(x \\ge q_{1-\\alpha}\\), where \\(q_{1-\\alpha}\\) is the quantile of order \\(1-\\alpha\\) of the null distribution of \\(T\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level-continued",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level-continued",
    "title": "Hypothesis Testing",
    "section": "Significance level (continued)",
    "text": "Significance level (continued)\n\nDecision-making rule:\n\nWe reject \\(H_0\\) if the value \\(t_0\\) of the test statistic computed on the observed sample is greater than the quantile of order \\(1-\\alpha\\) of the null distribution of \\(T\\);\nWe decide that we lack evidence to reject \\(H_0\\) if the value \\(t_0\\) of the test statistic calculated on the observed sample is smaller than the quantile of order \\(1-\\alpha\\) of the null distribution of \\(T\\).\nThis decision-making rule guarantees that the probability of making a type I error is upper-bounded by the significance level \\(\\alpha\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#p-value",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#p-value",
    "title": "Hypothesis Testing",
    "section": "p-value",
    "text": "p-value\n\n\nDefinition. The p-value is a scalar value between \\(0\\) and \\(1\\) that measures what was the probability, assuming that the null hypothesis \\(H_0\\) is true, of observing the data we did observe, or data even more in favor of the alternative hypothesis.\nMathematical expression. If \\(t_0\\) is the value of the test statistic computed from the observed sample, then:\n\n\\(p = \\mathbb{P}_{H_0}(T &gt; t_0)\\) for right-tail hypothesis tests (e.g. \\(H_a: \\mu &gt; \\mu_0\\) when testing the mean);\n\\(p = \\mathbb{P}_{H_0}(T &lt; t_0)\\) for left-tail hypothesis tests (e.g. \\(H_a: \\mu &lt; \\mu_0\\) when testing the mean);\n\\(p = 2 \\min\\left( \\mathbb{P}_{H_0} \\left( T &gt; t_0 \\right), \\mathbb{P}_{H_0} \\left( T &lt; t_0 \\right) \\right)\\) for two-tail hypothesis tests (e.g. \\(H_a: \\mu \\ne \\mu_0\\) when testing the mean).\n\nInterpretation. If the \\(p\\)-value is very small, it means that\n\neither we observed a miracle,\nor the null hypothesis might be wrong.\n\nDecision-making rule. We can show that rejecting the null hypothesis when \\(p \\le \\alpha\\) also produces a decision-making rule that guarantees a probability of type I error at most \\(\\alpha\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#decision-making-summary-right-tail-scenario",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#decision-making-summary-right-tail-scenario",
    "title": "Hypothesis Testing",
    "section": "Decision-Making: Summary (right-tail scenario)",
    "text": "Decision-Making: Summary (right-tail scenario)"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#statistical-power-of-a-test",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#statistical-power-of-a-test",
    "title": "Hypothesis Testing",
    "section": "Statistical power of a test",
    "text": "Statistical power of a test\n\n\nDefinition. It is the probability of correctly rejecting the null hypothesis, i.e. to reject it when the alternative is in fact correct. It is often denoted by \\(\\mu\\). In terms of events, it is defined by: \\[ \\mu := \\mathbb{P}_{H_a}(\\mbox{Reject } H_0). \\]\nUsage. The statistical power of a test is an important aspect of the test because:\n\nit is an important performance indicator to compare different testing procedures (observe that there is not a unique statistic to perform a given test);\nit is often used in clinical trials or other types of trials (e.g. crash tests) to calibrate the number of observations required to achieve a given statistical power.\n\nRemarks.\n\nThe statistical power \\(\\mu\\) is equal to \\(1 - \\beta\\), where \\(\\beta\\) is the greek letter often used to designate the probability of type II errors, \\(\\beta := \\mathbb{P}_{H_a}(\\mbox{Do not reject } H_0)\\).\nPower calculations are difficult because it requires to put ourselves under \\(H_a\\), which is often of the form \\(\\mu &gt; \\mu_0\\) or \\(\\mu &lt; \\mu_0\\) or \\(\\mu \\ne \\mu_0\\). In other words, you often lack information to compute probabilities assuming that the alternative hypothesis is true. You have to assess how the power changes as you explore different alternatives."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean",
    "title": "Hypothesis Testing",
    "section": "Testing the mean",
    "text": "Testing the mean\n\n\nModel. Let \\((X_1, \\dots, X_n)\\) be \\(n\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nHypotheses. \\[ H_0: \\mu = \\mu_0 \\quad \\mbox{vs.} \\quad H_a: \\mu \\ne \\mu_0 \\quad \\mbox{or} \\quad H_a: \\mu &gt; \\mu_0 \\quad \\mbox{or} \\quad H_a: \\mu &lt; \\mu_0. \\]\nTest Statistic. \\[ Z_n = \\sqrt{n} \\frac{\\overline X_n - \\mu_0}{\\sigma}. \\]\nProblem. Under the null hypothesis, I do not have complete knowledge to compute the test statistic because I do not know the value of \\(\\sigma\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-known-variance",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-known-variance",
    "title": "Hypothesis Testing",
    "section": "Testing the mean with known variance",
    "text": "Testing the mean with known variance\n\n\nProblem solved. I now have complete knowledge to compute the test statistic.\nNull distribution. The null distribution is \\[ Z_n \\stackrel{H_0}{\\sim} \\mathcal{N}(0, 1). \\]\nR function. None"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-unknown-variance",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-unknown-variance",
    "title": "Hypothesis Testing",
    "section": "Testing the mean with unknown variance",
    "text": "Testing the mean with unknown variance\n\n\nProblem solved. I plug in the empirical standard deviation instead of \\(\\sigma\\) in the definition of the test statistic.\nTest Statistic. \\[ T_n = \\frac{\\overline X_n - \\mu_0}{\\sqrt{\\frac{S_{n-1}^2}{n}}}, \\quad \\mbox{with} \\quad S_{n-1}^2 := \\frac{1}{n-1} \\sum_{i = 1}^n (X_i - \\overline X)^2. \\]\nNull distribution. \\[ T_n \\stackrel{H_0}{\\sim} \\mathcal{S}tudent(n-1). \\]\nR function. t.test()"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance",
    "title": "Hypothesis Testing",
    "section": "Testing the variance",
    "text": "Testing the variance\n\n\nAssumptions. Let \\((X_1, \\dots, X_n)\\) be \\(n\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nHypotheses. \\[ H_0: \\sigma^2 = \\sigma_0^2 \\quad \\mbox{vs.} \\quad H_a: \\sigma^2 \\ne \\sigma_0^2 \\quad \\mbox{or} \\quad H_a: \\sigma^2 &gt; \\sigma_0^2 \\quad \\mbox{or} \\quad H_a: \\sigma^2 &lt; \\sigma_0^2. \\]\nTest Statistic. \\[ U_n = \\sum_{i = 1}^n \\frac{(X_i - \\mu)^2}{\\sigma_0^2}. \\]\nProblem. Under the null hypothesis, I do not have complete knowledge to compute the test statistic because I do not know the value of \\(\\mu\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-known-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-known-mean",
    "title": "Hypothesis Testing",
    "section": "Testing the variance with known mean",
    "text": "Testing the variance with known mean\n\n\nProblem solved. I now have complete knowledge to compute the test statistic.\nNull distribution. The null distribution is \\[ U_n \\stackrel{H_0}{\\sim} \\chi^2(n). \\]\nR function. None."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-unknown-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-unknown-mean",
    "title": "Hypothesis Testing",
    "section": "Testing the variance with unknown mean",
    "text": "Testing the variance with unknown mean\n\n\nProblem solved. I plug in the empirical mean instead of \\(\\mu\\) in the definition of the test statistic.\nTest Statistic. \\[ U_{n-1} = \\sum_{i = 1}^n \\frac{\\left( X_i - \\overline X_n \\right)^2}{\\sigma_0^2}. \\]\nNull distribution. \\[ U_{n-1} \\stackrel{H_0}{\\sim} \\chi^2(n-1). \\]\nR function. None."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-a-proportion",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-a-proportion",
    "title": "Hypothesis Testing",
    "section": "Testing a proportion",
    "text": "Testing a proportion\n\nHere we want to test whether the proportion \\(p\\) of individuals in a given population who have a feature of interest is equal to a pre-specified rate.\n\nModel. Let \\((X_1, \\dots, X_n)\\) be \\(n\\) i.i.d. random variables that follow a Bernoulli distribution \\(\\mathcal{B}e(p)\\). The interpretation is that \\(X_i\\) measures if individual \\(i\\) possesses the characteristic of which we want to know the proportion or not.\nHypotheses. \\[ H_0: p = p_0 \\quad \\mbox{vs.} \\quad H_a: p \\ne p_0 \\quad \\mbox{or} \\quad H_a: p &gt; p_0 \\quad \\mbox{or} \\quad H_a: p &lt; p_0. \\]\nTest Statistic. \\[ B_n = \\sum_{i=1}^n X_i \\]\nNull distribution. \\[ B_n \\sim \\mathcal{B}inom(n,p_0) \\]\nR function. binom.test()"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-variance-differences",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-variance-differences",
    "title": "Hypothesis Testing",
    "section": "Testing for variance differences",
    "text": "Testing for variance differences\n\n\nModel. Let \\((X_1, \\dots, X_{n_x})\\) be \\(n_X\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_X, \\sigma_X^2)\\) and \\((Y_1, \\dots, Y_{n_Y})\\) be \\(n_Y\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_Y, \\sigma_Y^2)\\). Assume furthermore that the two samples are statistically independent.\nHypotheses. \\[ H_0: \\sigma_X^2 = \\sigma_Y^2 \\quad \\mbox{vs.} \\quad H_a: \\sigma_X^2 \\ne \\sigma_Y^2 \\quad \\mbox{or} \\quad H_a: \\sigma_X^2 &gt; \\sigma_Y^2 \\quad \\mbox{or} \\quad H_a: \\sigma_X^2 &lt; \\sigma_Y^2 \\]\nTest Statistic. \\[ V_n = \\frac{S_X^2}{S_Y^2}, \\quad \\mbox{with} \\quad S_X^2 = \\frac{1}{n_X - 1} \\sum_{i = 1}^{n_X} (X_i - \\overline X_n)^2 \\quad \\mbox{and} \\quad S_Y^2 = \\frac{1}{n_Y - 1} \\sum_{i = 1}^{n_Y} (Y_i - \\overline Y_n)^2 \\]\nNull distribution. \\[ V_n \\stackrel{H_0}{\\sim} \\mathcal{F}isher(n_X - 1, n_Y - 1) \\]\nR function. var.test()\nValidity. Relies on the normality assumption and independence within and between samples."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences",
    "title": "Hypothesis Testing",
    "section": "Testing for mean differences",
    "text": "Testing for mean differences\n\n\nModel. Let \\((X_1, \\dots, X_{n_x})\\) be \\(n_X\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_X, \\sigma_X^2)\\) and \\((Y_1, \\dots, Y_{n_Y})\\) be \\(n_Y\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_Y, \\sigma_Y^2)\\). Assume furthermore that the two samples are statistically independent.\nHypotheses. \\[ H_0: \\mu_X = \\mu_Y \\quad \\mbox{vs.} \\quad H_a: \\mu_X \\ne \\mu_Y \\quad \\mbox{or} \\quad H_a: \\mu_X &gt; \\mu_Y \\quad \\mbox{or} \\quad H_a: \\mu_X &lt; \\mu_Y \\]\nR function. t.test()\nValidity. Relies on the normality assumption and independence within and between samples."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-equal",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-equal",
    "title": "Hypothesis Testing",
    "section": "Testing for mean differences when variances are equal",
    "text": "Testing for mean differences when variances are equal\n\n\nTest Statistic. Let \\(\\delta = \\mu_X - \\mu_Y\\) be the mean difference and \\(\\delta_0\\) be the assumed mean difference under \\(H_0\\). Then, \\[\nT_n = \\frac{\\left( \\overline X_n - \\overline Y_n \\right) - \\delta_0}{\\sqrt{S_\\mathrm{pooled}^2 \\left( \\frac{1}{n_X} + \\frac{1}{n_Y} \\right)}} \\mbox{ with } S_\\mathrm{pooled}^2 = \\frac{(n_X - 1) S_X^2 + (n_Y - 1) S_Y^2}{n_X + n_Y - 2}\n\\]\nNull distribution. \\[\nT_n \\stackrel{H_0}{\\sim} \\mathcal{S}tudent(n_X + n_Y - 2)\n\\]"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-not-equal",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-not-equal",
    "title": "Hypothesis Testing",
    "section": "Testing for mean differences when variances are not equal",
    "text": "Testing for mean differences when variances are not equal\n\n\nTest Statistic. \\[\nT_n = \\frac{\\left( \\overline X_n - \\overline Y_n \\right) - \\delta_0}{\\sqrt{\\frac{S_X^2}{n_X} + \\frac{S_Y^2}{n_Y}}}\n\\]\nNull distribution. \\[\nT_n \\stackrel{H_0}{\\sim} \\mathcal{S}tudent(m) \\mbox{ with } m = \\frac{\\left( \\frac{S_X^2}{n_X} + \\frac{S_Y^2}{n_Y} \\right)^2}{\\frac{\\left( \\frac{S_X^2}{n_X} \\right)^2}{n_X-1} + \\frac{\\left( \\frac{S_Y^2}{n_Y} \\right)^2}{n_Y-1}}.\n\\]"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#shapiro-wilk-test",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#shapiro-wilk-test",
    "title": "Hypothesis Testing",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\n\n\nHypotheses. \\[ H_0: (X_1, \\dots, X_n) \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\mu, \\sigma^2) \\quad \\mbox{vs.} \\quad H_a: (X_1, \\dots, X_n) \\not\\sim \\mathcal{N}(\\mu, \\sigma^2). \\]\nTest Statistic. \\[\nT_n = {\\left(\\sum_{i=1}^n a_i X_{(i)}\\right)^2 \\over \\sum_{i=1}^n (X_i-\\overline{X}_n)^2}\n\\] where \\(X_{(i)}\\) is the order statistic for observation \\(i\\), \\(\\overline X_n\\) the sample mean and \\((a_1, \\dots, a_n)\\) are weights computed from the first two moments of the order statistics of standard normal variables.\nNull distribution. \\[ T \\stackrel{H_0}{\\sim} \\mathcal{W}ilks(n). \\]\nR function. shapiro.test()\nValidity. The sample size should meet \\(3 \\le n \\le 5000\\). The Wilks distribution is approximated except for \\(n=3\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#kolmogorov-smirnov-test",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#kolmogorov-smirnov-test",
    "title": "Hypothesis Testing",
    "section": "Kolmogorov-Smirnov test",
    "text": "Kolmogorov-Smirnov test\n\n\nHypotheses. \\[ H_0: (X_1, \\dots, X_n) \\stackrel{i.i.d.}{\\sim} F_0 \\quad \\mbox{vs.} \\quad H_a: (X_1, \\dots, X_n) \\not\\sim F_0. \\]\nTest Statistic. \\[\nT_n = \\sup_{x \\in \\mathbb R} | F_n(x) - F(x)|\n\\] where \\(F_n\\) is the cumulative distribution function and \\(F\\) is the cumulative distribution function of the law under testing.\nNull distribution. \\[ \\sqrt{n} T \\xrightarrow{\\mathcal{L}} \\sup_{x \\in \\mathbb R} | B(F(x))|, \\] where \\(B\\) is the Brownian bridge.\nR function. ks.test()\nValidity. Asymptotic."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi2-test-of-adequacy-for-a-single-categorical-variable",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi2-test-of-adequacy-for-a-single-categorical-variable",
    "title": "Hypothesis Testing",
    "section": "\\(\\chi^2\\) test of adequacy for a single categorical variable",
    "text": "\\(\\chi^2\\) test of adequacy for a single categorical variable\n\n\nModel. We group observations in classes and we compare the observed frequencies of these classes to the corresponding theoretical frequencies as given by the hypothesized law \\(F_0\\).\nHypotheses. \\(H_0: (X_1, \\dots, X_n) \\stackrel{i.i.d.}{\\sim} F_0 \\quad \\mbox{vs.} \\quad H_a: (X_1, \\dots, X_n) \\not\\sim F_0\\).\nTest Statistic. \\[\nU_n = n\\sum_{i=1}^k\\frac{(f_i-f_{0i})^2}{f_{0i}},\n\\] where \\(n\\) is the total number of observations, \\(k\\) the number of classes, \\(f_i = n_i / n\\) and \\(f_{i0}\\) the theoretical frequency of class \\(i\\), i.e. the probability that the random variable ends up in class \\(i\\).\nNull distribution. \\[ U_n \\xrightarrow{\\mathcal{L}} \\chi^2_{k - 1 - \\ell}, \\mbox{ where } \\ell \\mbox{ is the number of estimated parameters for } F_0.\\]\nR function. chisq.test()\nValidity. Requires large class frequencies. Typically, \\(n_i \\ge 5\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi-2-test-of-independence-between-two-categorical-variables",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi-2-test-of-independence-between-two-categorical-variables",
    "title": "Hypothesis Testing",
    "section": "\\(\\chi ^2\\) test of independence between two categorical variables",
    "text": "\\(\\chi ^2\\) test of independence between two categorical variables\n\n\nModel. Let \\(X = (Y, Z) = ((Y_1, Z_1), \\dots, (Y_n, Z_n))\\) be a bivariate sample of \\(n\\) i.i.d. pairs of categorical random variables. Let \\(\\nu\\) be the law of \\((Y_1, Z_1)\\), \\(\\mu\\) the law of \\(Y_1\\) and \\(\\lambda\\) the law of \\(Z_1\\). Let \\(\\{y_1, \\dots, y_s\\}\\) be the set of possible values for \\(Y_1\\) and \\(\\{z_1, \\dots, z_r\\}\\) the set of possible values for \\(Z_1\\). For \\(\\ell \\in \\{1, \\dots, s\\}\\) et \\(h \\in \\{1, \\dots, r\\}\\), let \\[ N_{\\ell,\\cdot} = \\left| \\left\\{ i  \\in \\{1, \\dots, n\\}; Y_i = y_\\ell \\right\\} \\right|,\\quad N_{\\cdot, h} = \\left| \\left\\{ i  \\in \\{1, \\dots, n\\}; Z_i = z_h \\right\\} \\right|, \\\\ N_{\\ell,h} = \\left| \\left\\{(i \\in \\{1, \\dots, n\\}; Y_i = y_\\ell, Z_i = z_h \\right\\} \\right|.\n\\]\nHypotheses. \\(H_0: \\nu = \\mu \\otimes \\lambda \\quad \\mbox{vs.} \\quad H_a: \\nu \\ne \\mu \\otimes \\lambda\\).\nTest statistic. \\[ U_n = n  \\sum_{\\ell = 1}^s \\sum_{h = 1}^r \\frac{ \\left( \\frac{N_{\\ell, h}}{n}  -  \\frac{N_{\\ell, \\cdot}}{n} \\frac{N_{\\cdot, h}}{n} \\right)^2 }{ \\frac{N_{\\ell, \\cdot}}{n} \\frac{N_{\\cdot, h}}{n} }. \\]\nNull distribution. \\(U_n \\xrightarrow{\\mathcal{L}} \\chi^2 \\left( (s-1)(r-1) \\right)\\).\nR function. chisq.test()\nValidity. Asymptotic. Often, \\(n \\gg 30\\) and \\(N_{\\ell, h} \\gg 5\\), for each pair \\((\\ell, h)\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We want to study the energy efficiency of a chemical reaction that is documented having a nominal energy efficiency of \\(90\\%\\). Based on previous experiments on the same reaction, we know that the energy efficiency is a Gaussian random variable with unknown mean \\(\\mu\\) and variance equal to \\(2\\). In the last \\(5\\) days, the plant has given the following energy efficiencies (in percentage):\n\\[ 91.6, \\quad 88.75, \\quad 90.8, \\quad 89.95, \\quad 91.3 \\]\n\nIs the data in accordance with the specifications?\nWhat is a point estimate of the mean energy efficiency?\nDoes that mean that the data significantly prove that the mean energy efficiency is larger than the expected nominal value?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-1",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We want to study the energy efficiency of a chemical reaction that is documented having a nominal energy efficiency of \\(90\\%\\). Based on previous experiments on the same reaction, we know that the energy efficiency is a Gaussian random variable with unknown mean \\(\\mu\\) and variance equal to \\(2\\). In the last \\(5\\) days, the plant has given the following energy efficiencies (in percentage):\n\\[ 91.6, \\quad 88.75, \\quad 90.8, \\quad 89.95, \\quad 91.3 \\]\n\nIs the data in accordance with the specifications?\nWhat is a point estimate of the mean energy efficiency?\nDoes that mean that the data significantly prove that the mean energy efficiency is larger than the expected nominal value?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-2",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-2",
    "title": "Hypothesis Testing",
    "section": "Exercise 2",
    "text": "Exercise 2\nA study about air pollution done by a research station measured, on \\(8\\) different air samples, the following values of a polluant concentration (in \\(\\mu\\)g/m\\(^2\\)):\n\\[ 2.2 \\quad 1.8 \\quad 3.1 \\quad 2.0 \\quad 2.4 \\quad 2.0 \\quad 2.1 \\quad 1.2 \\]\nAssuming that the sampled population is normal,\n\nCan we say that the mean polluant concentration is present with less than \\(2.5 \\mu\\)g/m\\(^2\\)?\nCan we say that the mean polluant concentration is present with less than \\(2.4 \\mu\\)g/m\\(^2\\)?\nIs the normality hypothesis essential to justify the method used?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-3",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-3",
    "title": "Hypothesis Testing",
    "section": "Exercise 3",
    "text": "Exercise 3\nA medical inspection in an elementary school during a measles epidemic led to the examination of \\(30\\) children to assess whether they were affected. The results are in a tibble exam which contains the following:\n\n\n# A tibble: 30 × 2\n      Id Status \n   &lt;int&gt; &lt;chr&gt;  \n 1     1 Healthy\n 2     2 Healthy\n 3     3 Healthy\n 4     4 Healthy\n 5     5 Healthy\n 6     6 Healthy\n 7     7 Healthy\n 8     8 Healthy\n 9     9 Healthy\n10    10 Healthy\n# ℹ 20 more rows\n\n\nLet \\(p\\) be the probability that a child from the same school is sick.\n\nDetermine a point estimate \\(\\widehat{p}\\) for \\(p\\).\nThe school will be closed if more than 5% of the children are sick. Can you conclude that, statistically, this is the case? Use a significance level of 5%."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-4",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-4",
    "title": "Hypothesis Testing",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe capacities (in ampere-hours) of \\(10\\) batteries were recorded as follows:\n\\[ 140, \\quad 136, \\quad 150, \\quad 144, \\quad 148, \\quad 152, \\quad 138, \\quad 141, \\quad 143, \\quad 151 \\]\n\nEstimate the population variance \\(\\sigma^2\\).\nCan we claim that the mean capacity of a battery is greater than 142 ampere-hours ?\nCan we claim that the mean capacity of a battery is greater than 140 ampere-hours ?\nCan we claim that the standard deviation of the capacity is less than 6 ampere-hours ?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-5",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-5",
    "title": "Hypothesis Testing",
    "section": "Exercise 5",
    "text": "Exercise 5\nA company produces barbed wire in skeins of \\(100\\)m each, nominally. The real length of the skeins is a random variable \\(X\\) distributed as a \\(\\mathcal{N}(\\mu, 4)\\). Measuring \\(10\\) skeins, we get the following lengths:\n\\[ 98.683, 96.599, 99.617, 102.544, 100.110, 102.000, 98.394, 100.324, 98.743, 103.247 \\]\n\nPerform a conformity test at significance level \\(\\alpha = 5\\%\\).\nDetermine, on the basis of the observed values, the p-value of the test."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-6",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-6",
    "title": "Hypothesis Testing",
    "section": "Exercise 6",
    "text": "Exercise 6\nIn an atmospheric study the researchers registered, over \\(8\\) different samples of air, the following concentration of COG (in micrograms over cubic meter):\n\\[ 2.3;\\; 1.7;\\; 3.2;\\; 2.1;\\; 2.3;\\; 2.0;\\; 2.2;\\; 1.2 \\]\n\nUsing unbiased estimators, determine a point estimate of the mean and variance of COG concentration.\n\nAssume now that the COG concentration is normally distributed.\n\nUsing a suitable statistical tool, establish whether the measured data allow to say that the mean concentration of COG is greater than \\(1.8\\) \\(\\mu\\)g/m\\(^3\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-7",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-7",
    "title": "Hypothesis Testing",
    "section": "Exercise 7",
    "text": "Exercise 7\nOn a total of \\(2350\\) interviewed citizens, \\(1890\\) approve the construction of a new movie theater.\n\nPerform an hypothesis test of level \\(5\\%\\), with null hypothesis that the percentage of citizens that approve the construction is at least \\(81\\%\\), versus the alternative hypothesis that the percentage is less than \\(81\\%\\).\nCompute the \\(p\\)-value of the test.\n[difficult] Determine the minimum sample size such that the power of the test with significance level \\(\\alpha = 0.05\\) when the real proportion \\(p\\) is \\(0.8\\) is at least \\(50\\%\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-8",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-8",
    "title": "Hypothesis Testing",
    "section": "Exercise 8",
    "text": "Exercise 8\nA computer chip manufacturer claims that no more than \\(1\\%\\) of the chips it sends out are defective. An electronics company, impressed with this claim, has purchased a large quantity of such chips. To determine if the manufacturer’s claim can be taken literally, the company has decided to test a sample of \\(300\\) of these chips. If \\(5\\) of these \\(300\\) chips are found to be defective, should the manufacturer’s claim be rejected?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-9",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-9",
    "title": "Hypothesis Testing",
    "section": "Exercise 9",
    "text": "Exercise 9\nTo determine the impurity level in alloys of steel, two different tests can be used. \\(8\\) specimens are tested, with both procedures, and the results are written in the following table:\n\n\n\nspecimen n.\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nTest 1\n1.2\n1.3\n1.7\n1.8\n1.5\n1.4\n1.4\n1.3\n\n\nTest 2\n1.4\n1.7\n2.0\n2.1\n1.5\n1.3\n1.7\n1.6\n\n\n\nAssume that the data are normal.\n\nbased on the data in the table, can we state that at significance level \\(\\alpha=5\\%\\) the Test 1 and 2 give a different average level of impurity?\nbased on the data in the table, can we state that at significance level \\(\\alpha=1\\%\\) the Test 2 gives an average level of impurity greater than Test 1?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-10",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-10",
    "title": "Hypothesis Testing",
    "section": "Exercise 10",
    "text": "Exercise 10\nA sample of \\(300\\) voters from region A and \\(200\\) voters from region B showed that the \\(56\\%\\) and the \\(48\\%\\), respectively, prefer a certain candidate. Can we say that at a significance level of \\(5\\%\\) there is a difference between the two regions?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-11",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-11",
    "title": "Hypothesis Testing",
    "section": "Exercise 11",
    "text": "Exercise 11\nIn a sample of \\(100\\) measures of the boiling temperature of a certain liquid, we obtain a sample mean \\(\\overline{x} = 100^{o}C\\) with a sample variance \\(s^2 = 0.0098^{o}C^2\\). Assuming that the observation comes from a normal population:\n\nWhat is the smallest level of significance that would lead to reject the null hypothesis that the variance is \\(\\leq 0.015\\)?\nOn the basis of the previous answer, what decision do we take if we fix the level of the test equal to \\(0.01\\)?"
  },
  {
    "objectID": "01_Introduction/authoring-with-quarto.html",
    "href": "01_Introduction/authoring-with-quarto.html",
    "title": "Authoring with Quarto",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "01_Introduction/authoring-with-quarto.html#section-1",
    "href": "01_Introduction/authoring-with-quarto.html#section-1",
    "title": "Authoring with Quarto",
    "section": "Section 1",
    "text": "Section 1\nText written in markdown.\n\n# Code written in R\n(x &lt;- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "01_Introduction/authoring-with-quarto.html#section-2",
    "href": "01_Introduction/authoring-with-quarto.html#section-2",
    "title": "Authoring with Quarto",
    "section": "Section 2",
    "text": "Section 2\nText written in markdown.\n\nggplot(data = mpg) +\n  geom_point(aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n\n\n…and so on."
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html",
    "href": "04_Model/04-Model-Exercises.html",
    "title": "Models",
    "section": "",
    "text": "Change the working directory to the folder where wages.xlsx is located and this file is saved.\nThen import wages.xlsx as wages and copy the code to your setup chunk.\nBe sure to set NA: to NA.\nSwitch the eval option in the YAML header to true."
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-1",
    "href": "04_Model/04-Model-Exercises.html#your-turn-1",
    "title": "Models",
    "section": "",
    "text": "Change the working directory to the folder where wages.xlsx is located and this file is saved.\nThen import wages.xlsx as wages and copy the code to your setup chunk.\nBe sure to set NA: to NA.\nSwitch the eval option in the YAML header to true."
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-2",
    "href": "04_Model/04-Model-Exercises.html#your-turn-2",
    "title": "Models",
    "section": "Your Turn 2",
    "text": "Your Turn 2\n\nFit the model\n\n\\[\n\\log(\\text{income}) = \\beta_0 + \\beta_1 \\cdot \\text{education} + \\epsilon\n\\]\n\nStore the result in an object called mod_e.\nExamine the output. What does it look like?"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-3",
    "href": "04_Model/04-Model-Exercises.html#your-turn-3",
    "title": "Models",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nUse a pipe to model log(income) against height. Then use broom and dplyr functions to extract:\n\nThe coefficient estimates and their related statistics\nThe adj.r.squared and p.value for the overall model"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-4",
    "href": "04_Model/04-Model-Exercises.html#your-turn-4",
    "title": "Models",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nModel log(income) against education and height and sex. Can you interpret the coefficients?"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-5",
    "href": "04_Model/04-Model-Exercises.html#your-turn-5",
    "title": "Models",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nAdd + geom_smooth(method = lm) to the code below. What happens?\n\nwages |&gt; \n  ggplot(aes(x = height, y = log(income))) +\n  geom_point(alpha = 0.1)"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-6",
    "href": "04_Model/04-Model-Exercises.html#your-turn-6",
    "title": "Models",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nUse add_predictions() to make the plot below. Facetting is by level of education.\n\n\n# In case you haven't made the ehs model\nmod_ehs &lt;- wages|&gt; \n  lm(log(income) ~ education + height + sex, data = _)\n\n# Make plot here"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-7",
    "href": "04_Model/04-Model-Exercises.html#your-turn-7",
    "title": "Models",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nUse gather_residuals() to make the plot below.\n\n\n\n\n\n\nCaution\n\n\n\nModels mod_h and mod_ehs should be available in your environment because you fitted them in previous sections. But you have to fit and store the model mod_eh which stands for education and height.\n\n\n\n\n# Make the plot here"
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html",
    "href": "12_PCA/12-PCA-Exercises.html",
    "title": "PCA - Exercises",
    "section": "",
    "text": "Cette base contient:\n\nles enregistrements des températures des capitales européennes de Janvier à Décembre\nles coordonnées GPS de chaque ville\namplitude thermale : Différence entre les températures maximales et minimales\nmoyenne annuelle\nune variable qualitative : la direction (S, N, O, E).\n\nExécuter une PCA pour dégager des profils type de température et quelles villes les suivent."
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-temperature.csv",
    "href": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-temperature.csv",
    "title": "PCA - Exercises",
    "section": "",
    "text": "Cette base contient:\n\nles enregistrements des températures des capitales européennes de Janvier à Décembre\nles coordonnées GPS de chaque ville\namplitude thermale : Différence entre les températures maximales et minimales\nmoyenne annuelle\nune variable qualitative : la direction (S, N, O, E).\n\nExécuter une PCA pour dégager des profils type de température et quelles villes les suivent."
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-chicken.csv",
    "href": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-chicken.csv",
    "title": "PCA - Exercises",
    "section": "Le jeu de données chicken.csv",
    "text": "Le jeu de données chicken.csv\n\nDescription : 43 poulets ayant subis 6 régimes : régime normal (N), Jeûne pendant 16h (F16), Jeûne pendant 16h et puis se réalimenter pendant 5h (F16R5), (F16R16), (F48), (F48R24)\nVariables : Après le régime, on a effectué une analyse des gènes utilisant une puce ADN : 7407 expressions de gènes.\nObjectif : Voir si les gènes s’expriment différemment selon le niveau de stress. Combien de temps faut-il au poulet pour revenir à la situation normale ?"
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-orange.csv",
    "href": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-orange.csv",
    "title": "PCA - Exercises",
    "section": "Le jeu de données orange.csv",
    "text": "Le jeu de données orange.csv\nSix jus d’orange de fabriquants différents ont été évalués. Toutes les variables sont-elles indisensables ? Y a-t-il des jus qui se dégagent comme particulièrement bons ? mauvais ?"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html",
    "title": "Linear Regression with R",
    "section": "",
    "text": "In the following exercises, you will be asked to study the relationship of a continuous response variable and one or more predictors. In doing so, remember to:\n\nperform model diagnosis\nincluding visualization tools\nincluding multicollinearity assessment\nperform informed model selection\ncomment each result of an analysis you run with R"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#expectations",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#expectations",
    "title": "Linear Regression with R",
    "section": "",
    "text": "In the following exercises, you will be asked to study the relationship of a continuous response variable and one or more predictors. In doing so, remember to:\n\nperform model diagnosis\nincluding visualization tools\nincluding multicollinearity assessment\nperform informed model selection\ncomment each result of an analysis you run with R"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-1",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-1",
    "title": "Linear Regression with R",
    "section": "Exercise 1",
    "text": "Exercise 1\nAnalysis of the production data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nx\nNumber of produced pieces\n\n\ny\nProduction cost\n\n\n\nStudy the relationship between x and y.\n\nSolution\nThe first thing to do is to inspect a very brief summary of the data mainly to get an idea of\n\nthe sample size;\nthe presence of missing values;\nthe number of categorical and continuous variables.\n\nThe skimr package proposes the skim() function that can fit this purpose:\n\nskimr::skim(production)\n\n\nData summary\n\n\nName\nproduction\n\n\nNumber of rows\n10\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx\n0\n1\n1702.8\n671.30\n852\n1213.50\n1534.0\n2161.25\n3050\n▇▃▂▃▂\n\n\ny\n0\n1\n18806.3\n5693.01\n11314\n14839.75\n16860.5\n23158.75\n29349\n▇▇▁▇▂\n\n\n\n\n\nWe do not have missing values but only 10 observations.\nNext, we can visualize the variation of y in terms of x to get an idea about the kind of relationship that we should model, should there be any:\n\nproduction |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe graph suggests that a linear relationship seems to hold.\nNow, we can get to the modeling part:\n\nmod &lt;- lm(y ~ x, data = production)\ndt_glance(mod)\n\n\n\n\n\nThe model exhibits an adjusted \\(R^2\\) of 0.9657089 which is very high, meaning that most of the variability in y is explained by the modeled relationship with x. Moreover, the p-value of the test of significance of the regression is 2.39^{-7} which is very low, suggesting that the model is statistically significant.\n\ndt_tidy(mod)\n\n\n\n\n\nLooking at the table of estimated coefficients, we read a coefficient of 8.3503139 for the predictor x which means that a positive correlation exists between x and y: an increase of x implies an increase of y. Moreover, the p-value of the significance of that coefficient is 2.39^{-7} which is very low, meaning that the coefficient is statistically different from zero, proving that x does have a significant impact on y.\nAll these conclusions will be only valid if the estimated model satisfies the assumptions behind the simple linear regression. We can plot a number of diagnosis plot to help us investigate this:\n\nlibrary(ggfortify)\nggplot2::autoplot(mod)\n\n\n\n\n\n\n\n\nThe Residuals vs Fitted plot does show some structure while we ideally want to see no pattern. This is to be mitigated with the low sample size and the presence of one or two visible outliers (obs. 9 and 10) which seem to drive the visible pattern. These two observations have high residuals and great departure from normality which means that they tend to be outliers. However, the Residuals vs Leverage plot reveals that they have little influence on the estimated coefficients. Hence, overall, we cannot say that the estimated model violates critical assumptions and the previous conclusions drawn from it are valid.\nAt this point, we can generated a publication-ready summary table of the model:\n\njtools::summ(mod)\n\n\n\n\n\nObservations\n10\n\n\nDependent variable\ny\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(1,8)\n254.46\n\n\nR²\n0.97\n\n\nAdj. R²\n0.97\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n4587.39\n951.67\n4.82\n0.00\n\n\nx\n8.35\n0.52\n15.95\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nWe can also provide a nice visualization of the estimated coefficients along with the incertainty about their estimation in the form of confidence intervals:\n\njtools::plot_summs(mod, inner_ci_level = 0.90)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-2",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-2",
    "title": "Linear Regression with R",
    "section": "Exercise 2",
    "text": "Exercise 2\nAnalysis of the brain data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nbody_weight\nBody weight in kg\n\n\nbrain_weight\nBrain weight in kg\n\n\n\nStudy the relationship between body and brain weights, to establish how the variable brain_weight changes with the variable body_weight.\n\nSolution\nThe first thing to do is to inspect a very brief summary of the data mainly to get an idea of\n\nthe sample size;\nthe presence of missing values;\nthe number of categorical and continuous variables.\n\nThe skimr package proposes the skim() function that can fit this purpose:\n\nskimr::skim(brain)\n\n\nData summary\n\n\nName\nbrain\n\n\nNumber of rows\n62\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbody_weight\n0\n1\n198.79\n899.18\n0.00\n0.60\n3.34\n48.2\n6654.18\n▇▁▁▁▁\n\n\nbrain_weight\n0\n1\n283.14\n930.28\n0.14\n4.25\n17.25\n166.0\n5711.86\n▇▁▁▁▁\n\n\n\n\n\nWe do not have missing values and the sample size is \\(62\\), which is reasonable should we resort to asymptotic results such as the CLT.\nNext, we can visualize the variation of brain_weight in terms of body_weight to get an idea about the kind of relationship that we should model, should there be any:\n\nbrain |&gt; \n  ggplot(aes(x = body_weight, y = brain_weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe relationship between brain_weight and body_weight does not seem to be linear. This however does not mean that we cannot use a linear model. Remember that the linear model is called this way because it is linear in the coefficients, not in the predictors!\nHere, we observe that\n\nboth variables are positive;\nmost of the mass is concentrated in the bottom-left part of the plot;\nthe rest of the points seem to follow a logarithm trend.\n\nThis suggests that a log-log transformation might help. This can be achieved in two ways. One way is actually compute the logarithm of the variables via log() and visualize the newly created variables:\n\nbrain |&gt; \n  mutate(\n    body_weight_log = log(body_weight),\n    brain_weight_log = log(brain_weight)\n  ) |&gt; \n  ggplot(aes(x = body_weight_log, y = brain_weight_log)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis shows that the relationship seems linear between the logarithm of both variables.\nWe can also keep asking ggplot2 to graph brain_weight in terms of body_weight but to use logarithmic scales on the axes of the plot:\n\nbrain |&gt; \n  ggplot(aes(x = body_weight, y = brain_weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  scale_x_log10() + \n  scale_y_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis essentially plots the same thing.\nNow we can estimate the model:\n\nmod &lt;- lm(log(brain_weight) ~ log(body_weight), data = brain)\ndt_glance(mod)\n\n\n\n\n\nWe can now inspect the estimated coefficients and comment the table, carry on model diagnosis and report summary table and CI for estimated coefficients as done in the previous exercise."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-3",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-3",
    "title": "Linear Regression with R",
    "section": "Exercise 3",
    "text": "Exercise 3\nAnalysis of the anscombe data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nx1\nPredictor to be used for explaining y1\n\n\nx2\nPredictor to be used for explaining y2\n\n\nx3\nPredictor to be used for explaining y3\n\n\nx4\nPredictor to be used for explaining y4\n\n\ny1\nResponse to be explained by x1\n\n\ny2\nResponse to be explained by x2\n\n\ny3\nResponse to be explained by x3\n\n\ny4\nResponse to be explained by x4\n\n\n\nStudy the relationship between each \\(y_i\\) and the corresponding \\(x_i\\)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-4",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-4",
    "title": "Linear Regression with R",
    "section": "Exercise 4",
    "text": "Exercise 4\nAnalysis of the cement data set, which contains the following variables:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\naluminium\nPercentage of \\(\\mathrm{Ca}_3 \\mathrm{Al}_2 \\mathrm{O}_6\\)\n\n\nsilicate\nPercentage of \\(\\mathrm{C}_2 \\mathrm{S}\\)\n\n\naluminium_ferrite\nPercentage of \\(4 \\mathrm{CaO} \\mathrm{Al}_2 \\mathrm{O}_3 \\mathrm{Fe}_2 \\mathrm{O}_3\\)\n\n\nsilicate_bic\nPercentage of \\(\\mathrm{C}_3 \\mathrm{S}\\)\n\n\nhardness\nHardness of the cement obtained by mixing the above four components\n\n\n\nStudy, using a multiple linear regression model, how the variable hardness depends on the four predictors.\n\nSolution\nWe can take a look at the data summary:\n\nskimr::skim(cement)\n\n\nData summary\n\n\nName\ncement\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naluminium\n0\n1\n7.46\n5.88\n1.0\n2.0\n7.0\n11.0\n21.0\n▇▃▇▁▂\n\n\nsilicate\n0\n1\n46.62\n18.03\n16.0\n31.0\n52.0\n56.0\n71.0\n▃▃▃▇▆\n\n\naluminium_ferrite\n0\n1\n11.77\n6.41\n4.0\n8.0\n9.0\n17.0\n23.0\n▅▇▂▃▃\n\n\nsilicate_bic\n0\n1\n30.00\n16.74\n6.0\n20.0\n26.0\n44.0\n60.0\n▆▇▃▃▃\n\n\nhardness\n0\n1\n95.42\n15.04\n72.5\n83.8\n95.9\n109.2\n115.9\n▆▃▃▃▇\n\n\n\n\n\nThis shows that we have a rather low sample size (13) but no missing values.\nThe novelty in this exercise is that we are going to move to multivariate linear regression in the sense that we will have more than one continuous predictor. Here the skim() function reveals that we can use 4 continuous variables to build up predictors. The next thing to do is then to visualize how the variable that we want to predict (hardness) varies with each one of the 4 potential predictors:\n\ncement |&gt; \n  pivot_longer(-hardness) |&gt; \n  ggplot(aes(x = value, y = hardness)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_wrap(vars(name), nrow = 2, scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFor all potential predictors, a linear relationhip with the outcome hardness seems appropriate.\nNow, before moving to the modeling part, when it comes to integrating several predictors in a model, it is important to address the problem of collinearity and multicollinearity.\nWe can look at potential collinearities by visualizing the correlation matrix between variables:\n\nlibrary(ggcorrplot)\ncorr &lt;- round(cor(cement), digits = 2)\nggcorrplot(corr, lab = TRUE)\n\n\n\n\n\n\n\n\nIn this matrix, values of correlation very close to 1 are the ones that might create problems by making \\(\\mathbb{X}^\\top \\mathbb{X}\\) ill-conditioned. Here we notice such a high correlation (-0.98) between silicate and silicate_bic. It might therefore not be a good idea to put them both as predictors in a model.\nWe can test this assertion by estimating the model with all 4 predictors first and look at the estimated coefficient and corresponding standard error and then do the same thing with a model without, say, silicate_bic.\n\nmod1 &lt;- lm(hardness ~ ., data = cement)\ndt_tidy(mod1)\n\n\n\n\n\n\nmod2 &lt;- lm(hardness ~ . - silicate_bic, data = cement)\ndt_tidy(mod2)\n\n\n\n\n\nWe observe that:\n\nthe coefficient for silicate in model 1 is negative will previous plots suggests that the covariation between hardness and silicate should be positive;\nthe standard error of this same coefficient is huge in comparison to its value in model 2.\n\nThese two observations confirms that we should not include both silicate and silicate_bic in the same model as predictors.\nNow we can focus on model 2 and assess whether we now have multicollinearity issues. This can be inspected by computed the VIFs of each predictor:\n\njtools::summ(mod2, vifs = TRUE)\n\n\n\n\n\nObservations\n13\n\n\nDependent variable\nhardness\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(3,9)\n90.25\n\n\nR²\n0.97\n\n\nAdj. R²\n0.96\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\nVIF\n\n\n\n\n(Intercept)\n58.83\n4.96\n11.87\n0.00\nNA\n\n\naluminium\n1.40\n0.28\n4.95\n0.00\n3.42\n\n\nsilicate\n0.57\n0.05\n10.84\n0.00\n1.11\n\n\naluminium_ferrite\n-0.03\n0.25\n-0.13\n0.90\n3.24\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\nHere, all 3 VIFs are relatively small (lower than 5) which suggests that multicollinearity should not be an issue.\nLastly, we can perform a step of model selection by stepwise backward elimination starting with the model with all 3 predictors:\n\nmod3 &lt;- step(mod2, direction = \"backward\")\n\nStart:  AIC=32.77\nhardness ~ (aluminium + silicate + aluminium_ferrite + silicate_bic) - \n    silicate_bic\n\n                    Df Sum of Sq     RSS    AIC\n- aluminium_ferrite  1      0.16   87.53 30.792\n&lt;none&gt;                             87.37 32.768\n- aluminium          1    238.09  325.46 47.864\n- silicate           1   1139.70 1227.07 65.117\n\nStep:  AIC=30.79\nhardness ~ aluminium + silicate\n\n            Df Sum of Sq     RSS    AIC\n&lt;none&gt;                     87.53 30.792\n- aluminium  1    796.13  883.67 58.849\n- silicate   1   1178.15 1265.69 63.519\n\n\nThe model selection suggests that, on the basis of the AIC, we can get rid of the aluminium_ferrite predictor as well which seems to have little non-significant influence on hardness.\nWe have now a good candidate model that we need to inspect, diagnose and report about as done in Exercise 1.\n\ndt_glance(mod3)\n\n\n\n\n\n\n# diagnostic\n\n\njtools::summ(mod3, vifs = TRUE)\n\n\n\n\n\nObservations\n13\n\n\nDependent variable\nhardness\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(2,10)\n150.13\n\n\nR²\n0.97\n\n\nAdj. R²\n0.96\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\nVIF\n\n\n\n\n(Intercept)\n58.28\n2.42\n24.10\n0.00\nNA\n\n\naluminium\n1.43\n0.15\n9.54\n0.00\n1.07\n\n\nsilicate\n0.57\n0.05\n11.60\n0.00\n1.07\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\n\njtools::plot_summs(mod3, inner_ci_level = 0.90)\n\n\n\n\n\n\n\n\nIn particular, when reporting about the model, we can add effect plots to focus on the effect of each individual predictors while correcting the observed data points to account for the impact of the other predictors:\n\njtools::effect_plot(mod3, aluminium, partial.residuals = TRUE)\n\n\n\n\n\n\n\n\n\njtools::effect_plot(mod3, silicate, partial.residuals = TRUE)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-5",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-5",
    "title": "Linear Regression with R",
    "section": "Exercise 5",
    "text": "Exercise 5\nAnalysis of the job data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\naverage_score\nAverage score obtained by the employee in the test\n\n\nyears_service\nNumber of years of service\n\n\nsex\nMale or female\n\n\n\nWe want to see if it is possible to use the sex of the person in addition to the years of service to predict, with a linear model, the average score obtained in the test. Estimate a linear regression of average_score vs. years_service, considering the categorical variable sex.\n\nskimr::skim(job)\n\n\nData summary\n\n\nName\njob\n\n\nNumber of rows\n20\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naverage_score\n0\n1\n6.69\n1.86\n3.4\n5.23\n6.95\n8.03\n9.8\n▆▃▇▇▆\n\n\nyears_service\n0\n1\n9.80\n8.30\n1.0\n4.00\n7.00\n12.00\n30.0\n▇▇▁▁▂\n\n\n\n\n\n\njob |&gt; \n  ggplot(aes(x = years_service, y = average_score, color = sex)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(average_score ~ years_service * sex, data = job)\ndt_glance(mod)\n\n\n\n\n\n\ndt_tidy(mod)\n\n\n\n\n\nModel selection by stepwise backward elimination minimizing AIC:\n\nmod2 &lt;- step(mod, direction = \"backward\")\n\nStart:  AIC=-7.41\naverage_score ~ years_service * sex\n\n                    Df Sum of Sq     RSS     AIC\n&lt;none&gt;                            9.2538 -7.4140\n- years_service:sex  1    1.2506 10.5044 -6.8788\n\n\nIt does not suggest to remove the varying slopes. But this is mainly due to the small sample size because AIC is known to be biased in these cases. One could use the corrected AIC:\n\\[\n\\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1},\n\\]\nwhere \\(k\\) is the number of estimated parameters in the model. It leads to considering a common slope.\nModel selection by stepwise backward elimination minimizing BIC:\n\nn &lt;- nrow(job)\nmod3 &lt;- step(mod, direction = \"backward\", k = log(n))\n\nStart:  AIC=-3.43\naverage_score ~ years_service * sex\n\n                    Df Sum of Sq     RSS     AIC\n- years_service:sex  1    1.2506 10.5044 -3.8916\n&lt;none&gt;                            9.2538 -3.4311\n\nStep:  AIC=-3.89\naverage_score ~ years_service + sex\n\n                Df Sum of Sq    RSS     AIC\n&lt;none&gt;                       10.504 -3.8916\n- years_service  1    11.696 22.200  8.0787\n- sex            1    31.905 42.409 21.0241\n\n\nLooking at the BIC also suggests considering a common slope."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-6",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-6",
    "title": "Linear Regression with R",
    "section": "Exercise 6",
    "text": "Exercise 6\nAnalysis of the cars data set, which contains the following variables:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nspeed\nSpeed of the car before starting braking\n\n\ndist\nDistance travelled by the car during the braking period until it completely stops\n\n\n\nVerify if the distance travelled during the braking depends on the starting velocity of the car:\n\nChoose the best model to explain the distance as function of the speed,\nPredict the braking distance for a starting velocity of 25 km/h, using a point estimate and a prediction interval."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-7",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Solutions.html#exercise-7",
    "title": "Linear Regression with R",
    "section": "Exercise 7",
    "text": "Exercise 7\nAnalysis of the mussels data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nlength\nLength of a mussel (mm)\n\n\nwidth\nWidth of a mussel (mm)\n\n\nheight\nHeight of a mussel (mm)\n\n\nsize\nMass of a mussel (g)\n\n\nweight\nWeight of eatable part of a mussel (g)\n\n\n\nWe want to study how the eatable part of a mussel varies as a function of the other four variables using a multiple linear regression."
  }
]