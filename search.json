[
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html",
    "title": "Linear Regression with R",
    "section": "",
    "text": "In the following exercises, you will be asked to study the relationship of a continuous response variable and one or more predictors. In doing so, remember to:\n\nperform model diagnosis\nincluding visualization tools\nincluding multicollinearity assessment\nperform informed model selection\ncomment each result of an analysis you run with R"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#expectations",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#expectations",
    "title": "Linear Regression with R",
    "section": "",
    "text": "In the following exercises, you will be asked to study the relationship of a continuous response variable and one or more predictors. In doing so, remember to:\n\nperform model diagnosis\nincluding visualization tools\nincluding multicollinearity assessment\nperform informed model selection\ncomment each result of an analysis you run with R"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-1",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-1",
    "title": "Linear Regression with R",
    "section": "Exercise 1",
    "text": "Exercise 1\nAnalysis of the production data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nx\nNumber of produced pieces\n\n\ny\nProduction cost\n\n\n\nStudy the relationship between x and y."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-2",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-2",
    "title": "Linear Regression with R",
    "section": "Exercise 2",
    "text": "Exercise 2\nAnalysis of the brain data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nbody_weight\nBody weight in kg\n\n\nbrain_weight\nBrain weight in kg\n\n\n\nStudy the relationship between body and brain weights, to establish how the variable brain_weight changes with the variable body_weight."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-3",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-3",
    "title": "Linear Regression with R",
    "section": "Exercise 3",
    "text": "Exercise 3\nAnalysis of the anscombe data set which is composed of the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nx1\nPredictor to be used for explaining y1\n\n\nx2\nPredictor to be used for explaining y2\n\n\nx3\nPredictor to be used for explaining y3\n\n\nx4\nPredictor to be used for explaining y4\n\n\ny1\nResponse to be explained by x1\n\n\ny2\nResponse to be explained by x2\n\n\ny3\nResponse to be explained by x3\n\n\ny4\nResponse to be explained by x4\n\n\n\nStudy the relationship between each \\(y_i\\) and the corresponding \\(x_i\\)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-4",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-4",
    "title": "Linear Regression with R",
    "section": "Exercise 4",
    "text": "Exercise 4\nAnalysis of the cement data set, which contains the following variables:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\naluminium\nPercentage of \\(\\mathrm{Ca}_3 \\mathrm{Al}_2 \\mathrm{O}_6\\)\n\n\nsilicate\nPercentage of \\(\\mathrm{C}_2 \\mathrm{S}\\)\n\n\naluminium_ferrite\nPercentage of \\(4 \\mathrm{CaO} \\mathrm{Al}_2 \\mathrm{O}_3 \\mathrm{Fe}_2 \\mathrm{O}_3\\)\n\n\nsilicate_bic\nPercentage of \\(\\mathrm{C}_3 \\mathrm{S}\\)\n\n\nhardness\nHardness of the cement obtained by mixing the above four components\n\n\n\nStudy, using a multiple linear regression model, how the variable hardness depends on the four predictors."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-5",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-5",
    "title": "Linear Regression with R",
    "section": "Exercise 5",
    "text": "Exercise 5\nAnalysis of the job data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\naverage_score\nAverage score obtained by the employee in the test\n\n\nyears_service\nNumber of years of service\n\n\nsex\nMale or female\n\n\n\nWe want to see if it is possible to use the sex of the person in addition to the years of service to predict, with a linear model, the average score obtained in the test. Estimate a linear regression of average_score vs. years_service, considering the categorical variable sex."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-6",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-6",
    "title": "Linear Regression with R",
    "section": "Exercise 6",
    "text": "Exercise 6\nAnalysis of the cars data set, which contains the following variables:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nspeed\nSpeed of the car before starting braking\n\n\ndist\nDistance travelled by the car during the braking period until it completely stops\n\n\n\nVerify if the distance travelled during the braking depends on the starting velocity of the car:\n\nChoose the best model to explain the distance as function of the speed,\nPredict the braking distance for a starting velocity of 25 km/h, using a point estimate and a prediction interval."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-7",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Exercises.html#exercise-7",
    "title": "Linear Regression with R",
    "section": "Exercise 7",
    "text": "Exercise 7\nAnalysis of the mussels data set, which contains the following variables:\n\n\n\nVariable name\nDescription\n\n\n\n\nlength\nLength of a mussel (mm)\n\n\nwidth\nWidth of a mussel (mm)\n\n\nheight\nHeight of a mussel (mm)\n\n\nsize\nMass of a mussel (g)\n\n\nweight\nWeight of eatable part of a mussel (g)\n\n\n\nWe want to study how the eatable part of a mussel varies as a function of the other four variables using a multiple linear regression."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html",
    "href": "12_PCA/12-PCA-Slides.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "If you’ve worked with a lot of variables before, you know this can present problems. Do you understand the relationships between each variable? Do you have so many variables that you are in danger of overfitting your model to your data or that you might be violating assumptions of whichever modeling tactic you’re using?\nYou might ask the question, “How do I take all of the variables I’ve collected and focus on only a few of them?” In technical terms, you want to “reduce the dimension of your feature space.” By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model.\nThink about linear regression:\n\nmulticollinearity issues;\ndifficult to use strategies like backward elimination.\n\n\n\n\nWhat ?\nPrincipal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA are all independent of one another.\nHow ?\nThe dimension reduction is achieved by identifying the principal directions, called principal components, in which the data varies.\nPCA assumes that the directions with the largest variances are the most “important” (i.e, the most principal).\nWhen to use?\n\nDo you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\nDo you want to ensure your variables are independent of one another?\nAre you comfortable making your independent variables less interpretable?\n\n\n\n\n\n\n\nThe 1st principal component (PC) accounts for the largest possible variance in the data set;\ni.e. the line in which the projection of the points (red dots) is the most spread out.\nThe second PC is uncorrelated to the 1st PC and maximizes the remaining variance;\netc.\n\n\n\n\n\nContext.\n\\(\\mathbf{X}_1, \\dots, \\mathbf{X}_n\\) a sample of \\(n\\) i.i.d. random vectors in \\(\\mathbb{R}^p\\). Let \\(\\mathbb{X} = (\\mathbf{X}_1, \\dots, \\mathbf{X}_n)^\\top\\) be the \\(n \\times p\\) data matrix, whose \\(j\\)-th column is the vector \\(\\mathbf{x}_j\\) containing the \\(n\\) observations on the \\(j\\)-th variable. Let \\(\\Sigma\\) be the covariance structure of the \\(\\mathbf{X}_i\\)’s.\nDetermination of the 1st PC\nThe 1st principal component (PC) accounts for the largest possible variance in the data set.\n\n\nSearch a linear projection \\(\\mathbf{a} \\in \\mathbb{R}^p\\) of the original variables (columns of \\(\\mathbb{X}\\)) with maximal variance. Such projection reads \\(\\mathbb{X} \\mathbf{a}\\) of which each component has variance: \\[ \\mathrm{Var}( \\left[ \\mathbb{X} \\mathbf{a} \\right]_i) = \\mathbf{a}^\\top \\Sigma \\mathbf{a}. \\]\n\n\n\n\n\n\nRequires additional constraint because maximum does not exist in \\(\\mathbb{R}^p\\): search for \\(\\| \\mathbf{a} \\| = 1\\).\nThe sample covariance \\(S\\) is an unbiased estimator of \\(\\Sigma\\), hence we can instead solve: \\[ \\max_{\\mathbf{a} \\in \\mathcal{S}^{p-1}} \\mathbf{a}^\\top S \\mathbf{a} = \\max_{\\mathbf{a} \\in \\mathbb{R}^p} \\frac{\\mathbf{a}^\\top S \\mathbf{a}}{\\mathbf{a}^\\top \\mathbf{a}} \\]\nSolution is eigenvector of \\(S\\) associated to the largest eigenvalue:\n\nconstrained problem is equivalent to maximizing \\(\\mathbf{a}^\\top S \\mathbf{a} - \\lambda (\\mathbf{a}^\\top \\mathbf{a} - 1)\\);\ndifferentiation leads to \\(S \\mathbf{a} = \\lambda \\mathbf{a}\\); hence, \\(\\mathbf{a}\\) is an eigenvector of \\(S\\);\nhence we get \\(\\mathbf{a}^\\top S \\mathbf{a} = \\lambda\\) which means that \\(\\lambda\\) must be the largest eigenvalue of \\(S\\) in order to maximize \\(\\mathbf{a}^\\top S \\mathbf{a}\\).\n\n\n\n\n\n\n\n\n\n\nprcomp() and princomp() from built-in R stats package,\nPCA() from FactoMineR package,\ndudi.pca() from ade4 package.\n\n\n\n\nThe FactoMineR::PCA() function.\n\n\n\nThe factoextra package.\n\n\n\n\n\nAthletes’ performance during two sporting events (Decastar and OlympicG)\nIt contains \\(41\\) individuals (athletes) described by \\(13\\) variables.\n\n\n\n\n\n\n\n\n\n\nPCA allows to describe a data set, to summarize a data set, to reduce the dimensionality. We want to perform a PCA on all the individuals of the data set to answer several questions:\n\n\nTwo athletes will be close to each other if their results to the events are close. We want to see the variability between the individuals.\n\nAre there similarities between individuals for all the variables?\nCan we establish different profiles of individuals?\nCan we oppose a group of individuals to another one?\n\n\n\n\nWe want to see if there are linear relationships between variables. The two objectives are to summarize the correlation matrix and to look for synthetic variables: can we resume the performance of an athlete by a small number of variables?\n\n\n\nCan we characterize groups of individuals by variables?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActive individuals: Individuals used for PCA.\nSupplementary individuals: Coordinates of these individuals will be predicted using the PCA information and parameters obtained using active individuals.\nActive variables: Variables used for PCA.\nSupplementary variables: Coordinates of these variables will be predicted also. These can be:\n\nSupplementary continuous variables: Cols 11 and 12 correspond respectively to the rank and the points of athletes.\nSupplementary qualitative variables: Col 13 corresponds to the two athletic meetings (2004 Olympic Game or 2004 Decastar). This is a categorical variable. It can be used to color individuals by groups.\n\n\n\n\n\n\n\n\nPCA linearly combines original variables to maximize variance\nIf one variable is measured in meter and another in centimeter, the first one will contribute more to the variance than the second one, even if the intrinsic variability of each variable is the same.\n\n\n \\(\\Longrightarrow\\) We need to scale the variables prior to performing PCA! \n\n\n\n\n\\[ x_{ij} \\leftarrow \\frac{x_{ij} - \\overline{x}_j}{\\sqrt{\\frac{1}{n-1} \\sum_{\\ell=1}^n (x_{\\ell j} - \\overline{x}_j)^2}} \\]\n\n \\(\\Longrightarrow\\) The function PCA() in FactoMineR, standardizes the data automatically. \n\n\n\n\n\n\nPCA(\n  # a data frame with n rows (individuals) and p columns (numeric variables)\n  X = , \n  \n  # number of dimensions kept in the results (by default 5)\n  ncp = , \n  \n  # a vector indicating the indexes of the supplementary individuals\n  ind.sup = , \n  \n  # a vector indicating the indexes of the quantitative supplementary variables\n  quanti.sup = ,\n  \n  # a vector indicating the indexes of the categorical supplementary variables\n  quali.sup = , \n  \n  # boolean, if TRUE (default) a graph is displayed\n  graph = \n)\n\n\n\n\n\nPCA(X = decathlon[, 1:10], graph = FALSE)\n\n\n\n**Results for the Principal Component Analysis (PCA)**\nThe analysis was performed on 41 individuals, described by 10 variables\n*The results are available in the following objects:\n\n   name               description                          \n1  \"$eig\"             \"eigenvalues\"                        \n2  \"$var\"             \"results for the variables\"          \n3  \"$var$coord\"       \"coord. for the variables\"           \n4  \"$var$cor\"         \"correlations variables - dimensions\"\n5  \"$var$cos2\"        \"cos2 for the variables\"             \n6  \"$var$contrib\"     \"contributions of the variables\"     \n7  \"$ind\"             \"results for the individuals\"        \n8  \"$ind$coord\"       \"coord. for the individuals\"         \n9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n10 \"$ind$contrib\"     \"contributions of the individuals\"   \n11 \"$call\"            \"summary statistics\"                 \n12 \"$call$centre\"     \"mean of the variables\"              \n13 \"$call$ecart.type\" \"standard error of the variables\"    \n14 \"$call$row.w\"      \"weights for the individuals\"        \n15 \"$call$col.w\"      \"weights for the variables\"          \n\n\n\n\n\n\n\n\nIt helps with interpretation of PCA.\nNo matter what function you decide to use (stats::prcomp(), FactoMiner::PCA(), ade4::dudi.pca(), ExPosition::epPCA()), you can easily extract and visualize the results of PCA using R functions provided in the factoextra R package.\n\n\n\n\n\nget_eigenvalue(): Extract the eigenvalues/variances of principal components.\nfviz_screeplot(): Visualize the eigenvalues / proportion of explained variance.\nget_pca_ind(), get_pca_var(): Extract the results for individuals and variables, respectively.\nfviz_pca_ind(), fviz_pca_var(): Visualize the results individuals and variables, respectively.\nfviz_pca_biplot(): Make a biplot of individuals and variables."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#dimensionality-reduction",
    "href": "12_PCA/12-PCA-Slides.html#dimensionality-reduction",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "If you’ve worked with a lot of variables before, you know this can present problems. Do you understand the relationships between each variable? Do you have so many variables that you are in danger of overfitting your model to your data or that you might be violating assumptions of whichever modeling tactic you’re using?\nYou might ask the question, “How do I take all of the variables I’ve collected and focus on only a few of them?” In technical terms, you want to “reduce the dimension of your feature space.” By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model.\nThink about linear regression:\n\nmulticollinearity issues;\ndifficult to use strategies like backward elimination."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#pca",
    "href": "12_PCA/12-PCA-Slides.html#pca",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "What ?\nPrincipal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA are all independent of one another.\nHow ?\nThe dimension reduction is achieved by identifying the principal directions, called principal components, in which the data varies.\nPCA assumes that the directions with the largest variances are the most “important” (i.e, the most principal).\nWhen to use?\n\nDo you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\nDo you want to ensure your variables are independent of one another?\nAre you comfortable making your independent variables less interpretable?"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#principle",
    "href": "12_PCA/12-PCA-Slides.html#principle",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The 1st principal component (PC) accounts for the largest possible variance in the data set;\ni.e. the line in which the projection of the points (red dots) is the most spread out.\nThe second PC is uncorrelated to the 1st PC and maximizes the remaining variance;\netc."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#mathematical-formalism",
    "href": "12_PCA/12-PCA-Slides.html#mathematical-formalism",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Context.\n\\(\\mathbf{X}_1, \\dots, \\mathbf{X}_n\\) a sample of \\(n\\) i.i.d. random vectors in \\(\\mathbb{R}^p\\). Let \\(\\mathbb{X} = (\\mathbf{X}_1, \\dots, \\mathbf{X}_n)^\\top\\) be the \\(n \\times p\\) data matrix, whose \\(j\\)-th column is the vector \\(\\mathbf{x}_j\\) containing the \\(n\\) observations on the \\(j\\)-th variable. Let \\(\\Sigma\\) be the covariance structure of the \\(\\mathbf{X}_i\\)’s.\nDetermination of the 1st PC\nThe 1st principal component (PC) accounts for the largest possible variance in the data set.\n\n\nSearch a linear projection \\(\\mathbf{a} \\in \\mathbb{R}^p\\) of the original variables (columns of \\(\\mathbb{X}\\)) with maximal variance. Such projection reads \\(\\mathbb{X} \\mathbf{a}\\) of which each component has variance: \\[ \\mathrm{Var}( \\left[ \\mathbb{X} \\mathbf{a} \\right]_i) = \\mathbf{a}^\\top \\Sigma \\mathbf{a}. \\]\n\n\n\n\n\n\nRequires additional constraint because maximum does not exist in \\(\\mathbb{R}^p\\): search for \\(\\| \\mathbf{a} \\| = 1\\).\nThe sample covariance \\(S\\) is an unbiased estimator of \\(\\Sigma\\), hence we can instead solve: \\[ \\max_{\\mathbf{a} \\in \\mathcal{S}^{p-1}} \\mathbf{a}^\\top S \\mathbf{a} = \\max_{\\mathbf{a} \\in \\mathbb{R}^p} \\frac{\\mathbf{a}^\\top S \\mathbf{a}}{\\mathbf{a}^\\top \\mathbf{a}} \\]\nSolution is eigenvector of \\(S\\) associated to the largest eigenvalue:\n\nconstrained problem is equivalent to maximizing \\(\\mathbf{a}^\\top S \\mathbf{a} - \\lambda (\\mathbf{a}^\\top \\mathbf{a} - 1)\\);\ndifferentiation leads to \\(S \\mathbf{a} = \\lambda \\mathbf{a}\\); hence, \\(\\mathbf{a}\\) is an eigenvector of \\(S\\);\nhence we get \\(\\mathbf{a}^\\top S \\mathbf{a} = \\lambda\\) which means that \\(\\lambda\\) must be the largest eigenvalue of \\(S\\) in order to maximize \\(\\mathbf{a}^\\top S \\mathbf{a}\\)."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#r-packages",
    "href": "12_PCA/12-PCA-Slides.html#r-packages",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "prcomp() and princomp() from built-in R stats package,\nPCA() from FactoMineR package,\ndudi.pca() from ade4 package.\n\n\n\n\nThe FactoMineR::PCA() function.\n\n\n\nThe factoextra package."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#case-study",
    "href": "12_PCA/12-PCA-Slides.html#case-study",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Athletes’ performance during two sporting events (Decastar and OlympicG)\nIt contains \\(41\\) individuals (athletes) described by \\(13\\) variables."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#objectives",
    "href": "12_PCA/12-PCA-Slides.html#objectives",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "PCA allows to describe a data set, to summarize a data set, to reduce the dimensionality. We want to perform a PCA on all the individuals of the data set to answer several questions:\n\n\nTwo athletes will be close to each other if their results to the events are close. We want to see the variability between the individuals.\n\nAre there similarities between individuals for all the variables?\nCan we establish different profiles of individuals?\nCan we oppose a group of individuals to another one?\n\n\n\n\nWe want to see if there are linear relationships between variables. The two objectives are to summarize the correlation matrix and to look for synthetic variables: can we resume the performance of an athlete by a small number of variables?\n\n\n\nCan we characterize groups of individuals by variables?"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#pca-terminology",
    "href": "12_PCA/12-PCA-Slides.html#pca-terminology",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Active individuals: Individuals used for PCA.\nSupplementary individuals: Coordinates of these individuals will be predicted using the PCA information and parameters obtained using active individuals.\nActive variables: Variables used for PCA.\nSupplementary variables: Coordinates of these variables will be predicted also. These can be:\n\nSupplementary continuous variables: Cols 11 and 12 correspond respectively to the rank and the points of athletes.\nSupplementary qualitative variables: Col 13 corresponds to the two athletic meetings (2004 Olympic Game or 2004 Decastar). This is a categorical variable. It can be used to color individuals by groups."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#data-standardization",
    "href": "12_PCA/12-PCA-Slides.html#data-standardization",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "PCA linearly combines original variables to maximize variance\nIf one variable is measured in meter and another in centimeter, the first one will contribute more to the variance than the second one, even if the intrinsic variability of each variable is the same.\n\n\n \\(\\Longrightarrow\\) We need to scale the variables prior to performing PCA! \n\n\n\n\n\\[ x_{ij} \\leftarrow \\frac{x_{ij} - \\overline{x}_j}{\\sqrt{\\frac{1}{n-1} \\sum_{\\ell=1}^n (x_{\\ell j} - \\overline{x}_j)^2}} \\]\n\n \\(\\Longrightarrow\\) The function PCA() in FactoMineR, standardizes the data automatically."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#pca-syntax",
    "href": "12_PCA/12-PCA-Slides.html#pca-syntax",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "PCA(\n  # a data frame with n rows (individuals) and p columns (numeric variables)\n  X = , \n  \n  # number of dimensions kept in the results (by default 5)\n  ncp = , \n  \n  # a vector indicating the indexes of the supplementary individuals\n  ind.sup = , \n  \n  # a vector indicating the indexes of the quantitative supplementary variables\n  quanti.sup = ,\n  \n  # a vector indicating the indexes of the categorical supplementary variables\n  quali.sup = , \n  \n  # boolean, if TRUE (default) a graph is displayed\n  graph = \n)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#running-pca",
    "href": "12_PCA/12-PCA-Slides.html#running-pca",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "PCA(X = decathlon[, 1:10], graph = FALSE)\n\n\n\n**Results for the Principal Component Analysis (PCA)**\nThe analysis was performed on 41 individuals, described by 10 variables\n*The results are available in the following objects:\n\n   name               description                          \n1  \"$eig\"             \"eigenvalues\"                        \n2  \"$var\"             \"results for the variables\"          \n3  \"$var$coord\"       \"coord. for the variables\"           \n4  \"$var$cor\"         \"correlations variables - dimensions\"\n5  \"$var$cos2\"        \"cos2 for the variables\"             \n6  \"$var$contrib\"     \"contributions of the variables\"     \n7  \"$ind\"             \"results for the individuals\"        \n8  \"$ind$coord\"       \"coord. for the individuals\"         \n9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n10 \"$ind$contrib\"     \"contributions of the individuals\"   \n11 \"$call\"            \"summary statistics\"                 \n12 \"$call$centre\"     \"mean of the variables\"              \n13 \"$call$ecart.type\" \"standard error of the variables\"    \n14 \"$call$row.w\"      \"weights for the individuals\"        \n15 \"$call$col.w\"      \"weights for the variables\""
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#visualization-and-interpretation",
    "href": "12_PCA/12-PCA-Slides.html#visualization-and-interpretation",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "It helps with interpretation of PCA.\nNo matter what function you decide to use (stats::prcomp(), FactoMiner::PCA(), ade4::dudi.pca(), ExPosition::epPCA()), you can easily extract and visualize the results of PCA using R functions provided in the factoextra R package.\n\n\n\n\n\nget_eigenvalue(): Extract the eigenvalues/variances of principal components.\nfviz_screeplot(): Visualize the eigenvalues / proportion of explained variance.\nget_pca_ind(), get_pca_var(): Extract the results for individuals and variables, respectively.\nfviz_pca_ind(), fviz_pca_var(): Visualize the results individuals and variables, respectively.\nfviz_pca_biplot(): Make a biplot of individuals and variables."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#eigenvalues-variances",
    "href": "12_PCA/12-PCA-Slides.html#eigenvalues-variances",
    "title": "Principal Component Analysis",
    "section": "Eigenvalues / Variances",
    "text": "Eigenvalues / Variances\n\n Eigenvalues measure the amount of variation retained by each principal component. \n\n\neig_val &lt;- get_eigenvalue(res_pca)\nhtml_table(eig_val)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#choosing-the-appropriate-number-of-pcs",
    "href": "12_PCA/12-PCA-Slides.html#choosing-the-appropriate-number-of-pcs",
    "title": "Principal Component Analysis",
    "section": "Choosing the appropriate number of PCs",
    "text": "Choosing the appropriate number of PCs\nEigenvalues can be used to determine the number of principal components to retain after PCA.\n\nGuidelines.\n\nAn eigenvalue &gt; 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized.\nBalance between a small number to achieve dimensionality reduction well and proporition of total variance explained.\nLook at the first few principal components to find interesting patterns in the data.\n\n\n\nGraphical tool.\n\n\n\n\n\n\n\n\n\n\nfviz_screeplot(res_pca, addlabels = TRUE)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#variable-information-in-pca",
    "href": "12_PCA/12-PCA-Slides.html#variable-information-in-pca",
    "title": "Principal Component Analysis",
    "section": "Variable Information in PCA",
    "text": "Variable Information in PCA\n\nVariable information extraction\n\nvar_info &lt;- get_pca_var(res_pca)\n\n\n\nComponents\n\nvar_info$coord: Coordinates for the variables.\nvar_info$cos2: Quality of representation of variables by PC: var.cos2 = var.coord * var.coord.\nvar_info$contrib: Contributions of the variables to the principal components: (var.cos2 * 100) / (sum cos2 of the component).\n\n\n\nUsage\n\nInvestigate the ability of PCs to strongly represent variables.\nDescribe each PC in terms of the most contributing variables."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#coord-correlation-variables-components",
    "href": "12_PCA/12-PCA-Slides.html#coord-correlation-variables-components",
    "title": "Principal Component Analysis",
    "section": "$coord: Correlation variables & components",
    "text": "$coord: Correlation variables & components\n\n\n\n\n\n\n\nProperty: norms of column vectors is 1 (eigenvectors).\nConsequence: each coordinate is in [-1,1]."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#fviz_pca_var-correlation-circle",
    "href": "12_PCA/12-PCA-Slides.html#fviz_pca_var-correlation-circle",
    "title": "Principal Component Analysis",
    "section": "fviz_pca_var(): Correlation circle",
    "text": "fviz_pca_var(): Correlation circle\n\nfviz_pca_var(res_pca, col.var = \"black\")"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#cos2-quality-of-representation",
    "href": "12_PCA/12-PCA-Slides.html#cos2-quality-of-representation",
    "title": "Principal Component Analysis",
    "section": "$cos2: Quality of representation",
    "text": "$cos2: Quality of representation\n\n\n\n\n\n\n\nPositive and negative strong correlations of PCs to a given variable imply that the PCs represent it well.\nSquared correlations thus measure quality of representation."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#cos2-graphical-tool-1",
    "href": "12_PCA/12-PCA-Slides.html#cos2-graphical-tool-1",
    "title": "Principal Component Analysis",
    "section": "$cos2: Graphical tool 1",
    "text": "$cos2: Graphical tool 1\n\ncorrplot::corrplot(var_info$cos2, is.corr = FALSE)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#cos2-graphical-tool-2",
    "href": "12_PCA/12-PCA-Slides.html#cos2-graphical-tool-2",
    "title": "Principal Component Analysis",
    "section": "$cos2: Graphical tool 2",
    "text": "$cos2: Graphical tool 2\n\n# Total cos2 of variables on Dim.1 and Dim.2\nfactoextra::fviz_cos2(res_pca, choice = \"var\", axes = 1:2)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#cos2-summary",
    "href": "12_PCA/12-PCA-Slides.html#cos2-summary",
    "title": "Principal Component Analysis",
    "section": "$cos2: Summary",
    "text": "$cos2: Summary\n\n\nThe cos2 values are used to estimate the quality of the representation.\nHigh cos2 indicates a good representation of the variable by the corresponding PC.\nLow cos2 indicates that the variable is poorly represented by the corresponding PC.\nVariables close to circle in plot are very well represented by the 1st two PCs.\nVariables close to center in plot are not well represented by the 1st two PCs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfactoextra::fviz_pca_var(X = res_pca, \n                         col.var = \"cos2\",\n                         gradient.cols = viridis::viridis(3),\n                         repel = TRUE) # Avoid text overlapping"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#contrib-important-variables",
    "href": "12_PCA/12-PCA-Slides.html#contrib-important-variables",
    "title": "Principal Component Analysis",
    "section": "$contrib: Important variables",
    "text": "$contrib: Important variables\n\n\n\n\n\n\n\n\nVariables correlating with Dim.1 (PC1) and Dim.2 (PC2) are the most important to explain the variability in the data set.\nVariables that do not correlate with any PC or only with the last ones are variables might be removed to simplify the overall analysis."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#contrib-graphical-tool-1",
    "href": "12_PCA/12-PCA-Slides.html#contrib-graphical-tool-1",
    "title": "Principal Component Analysis",
    "section": "$contrib: Graphical tool 1",
    "text": "$contrib: Graphical tool 1\n\ncorrplot::corrplot(var_info$contrib, is.corr = FALSE)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#contrib-graphical-tool-2",
    "href": "12_PCA/12-PCA-Slides.html#contrib-graphical-tool-2",
    "title": "Principal Component Analysis",
    "section": "$contrib: Graphical tool 2",
    "text": "$contrib: Graphical tool 2\n\n\n# Contributions of variables to PC1\nfactoextra::fviz_contrib(\n  X = res_pca, \n  choice = \"var\", \n  axes = 1, \n  top = 10\n)\n\n\n\n\n\n\n\n\n\n\n\n# Contributions of variables to PC2\nfactoextra::fviz_contrib(\n  X = res_pca, \n  choice = \"var\", \n  axes = 2, \n  top = 10\n)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#contrib-summary",
    "href": "12_PCA/12-PCA-Slides.html#contrib-summary",
    "title": "Principal Component Analysis",
    "section": "$contrib: Summary",
    "text": "$contrib: Summary\n\nfactoextra::fviz_pca_var(\n  X = res_pca, \n  col.var = \"contrib\",\n  gradient.cols = viridis::viridis(3),\n  repel = TRUE\n)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#component-description",
    "href": "12_PCA/12-PCA-Slides.html#component-description",
    "title": "Principal Component Analysis",
    "section": "Component description",
    "text": "Component description\n\nres_pca_var_desc &lt;- FactoMineR::dimdesc(res = res_pca, axes = c(1, 2), proba = 0.05)\n\n\nDescription of PC1\n\n\n\n\n\n\n\n\nDescription of PC2"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#the-reduced-space",
    "href": "12_PCA/12-PCA-Slides.html#the-reduced-space",
    "title": "Principal Component Analysis",
    "section": "The reduced space",
    "text": "The reduced space\n\nIndividual information extraction\n\nind_info &lt;- get_pca_ind(res_pca)\n\n\n\nComponents\n\nind_info$coord: Coordinates of the individuals.\nind_info$cos2: Quality of representation of individuals by PC: ind.cos2 = ind.coord * ind.coord.\nind_info$contrib: Contributions of the individuals to the principal components: (ind.cos2 * 100) / (sum cos2 of the component).\n\n\n\nUsage\n\nThe reduced space should allow easier identification of patterns, provided that it represents them well.\nIndividuals that are similar are grouped together on the plot."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#first-factorial-plane-quality",
    "href": "12_PCA/12-PCA-Slides.html#first-factorial-plane-quality",
    "title": "Principal Component Analysis",
    "section": "First factorial plane & quality",
    "text": "First factorial plane & quality\n\nfactoextra::fviz_pca_ind(\n  X = res_pca, \n  col.ind = \"cos2\", \n  gradient.cols = viridis::viridis(3),\n  repel = TRUE # Avoid text overlapping (slow if many points)\n)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#first-factorial-plane-contribution",
    "href": "12_PCA/12-PCA-Slides.html#first-factorial-plane-contribution",
    "title": "Principal Component Analysis",
    "section": "First factorial plane & contribution",
    "text": "First factorial plane & contribution\n\n# Total contribution on PC1 and PC2\nfactoextra::fviz_contrib(res_pca, choice = \"ind\", axes = 1:2)"
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#first-factorial-plane-colored-by-groups",
    "href": "12_PCA/12-PCA-Slides.html#first-factorial-plane-colored-by-groups",
    "title": "Principal Component Analysis",
    "section": "First factorial plane colored by groups",
    "text": "First factorial plane colored by groups\n\nfactoextra::fviz_pca_ind(\n  X = res_pca,\n  col.ind = decathlon$competition[1:23], # color by groups\n  palette = viridis::viridis(3),\n  addEllipses = TRUE, # Concentration ellipses\n  legend.title = \"Competition\"\n)\n\n\n\nThis plot is useful to understand the ability of a factorial plane to discriminate individuals based on some feature.\nHere, we see a tendency with positive score on PC1 for athletes at the Olympics and negative PC1 scores for athletes at the Decastar.\nBut, ellipses seem to overlap significantly."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#biplot",
    "href": "12_PCA/12-PCA-Slides.html#biplot",
    "title": "Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\nfactoextra::fviz_pca_biplot(\n  X = res.pca, \n  repel = TRUE,\n  col.var = \"#2E9FDF\", # Variables color\n  col.ind = \"#696969\"  # Individuals color\n)\n\n\n\nOnly useful when there is a low number of variables and individuals in the data set; otherwise unreadable.\nCoordinate of individuals and variables are not constructed on the same space: focus on the direction of variables but not on their absolute positions.\nAn individual that is on the same side of a given variable has a high value for this variable.\nAn individual that is on the opposite side of a given variable has a low value for this variable."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#interpretation",
    "href": "12_PCA/12-PCA-Slides.html#interpretation",
    "title": "Principal Component Analysis",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe first two dimensions resume 50% of the total inertia (i.e. the total variance of data set).\nThe variable 100m is correlated negatively to the variable long.jump. When an athlete performs a short time when running 100m, he can jump a big distance. Here one has to be careful because a low value for the variables 100m, 400m, 110m.hurdle and 1500m means a high score: the shorter an athlete runs, the more points he scores.\nThe first axis opposes athletes who are good everywhere like Karpov during the Olympic Games to those who are bad everywhere like Bourguignon during the Decastar. This dimension is particularly linked to the variables of speed and long jump which constitute a homogeneous group.\nThe second axis opposes athletes who are strong (variables Discus and Shot.put) to those who are weak.\nThe variables Discus, Shot.put and High.jump are not much correlated to the variables 100m, 400m, 110m.hurdle and Long.jump. This means that strength is not much correlated to speed.\n\n\n At this point, we can divide the first factorial plane into four parts: fast and strong athletes (like Sebrle), slow athletes (like Casarsa), fast but weak athletes (like Warners) and slow and weak athletes (like Lorenzo)."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#your-turn",
    "href": "12_PCA/12-PCA-Slides.html#your-turn",
    "title": "Principal Component Analysis",
    "section": "Your turn",
    "text": "Your turn\n\nDownload the lab file and related data sets from the course website in a folder on your computer.\nOpen the file 12-PCA-Exercises.qmd and perform the PCA analyses for the proposed data sets."
  },
  {
    "objectID": "12_PCA/12-PCA-Slides.html#references",
    "href": "12_PCA/12-PCA-Slides.html#references",
    "title": "Principal Component Analysis",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\nhttp://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/\nhttps://builtin.com/data-science/step-step-explanation-principal-component-analysis\nhttps://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2015.0202\nhttp://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/\nhttp://factominer.free.fr/factomethods/principal-components-analysis.html"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html",
    "href": "02_Visualize/02-Visualize-Exercises.html",
    "title": "Visualize Data",
    "section": "",
    "text": "Add a setup chunk that loads the tidyverse packages and turn the global option eval to true in the above YAML header.\n\nmpg"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-0",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-0",
    "title": "Visualize Data",
    "section": "",
    "text": "Add a setup chunk that loads the tidyverse packages and turn the global option eval to true in the above YAML header.\n\nmpg"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-1",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-1",
    "title": "Visualize Data",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nRun the code on the slide to make a graph. Pay strict attention to spelling, capitalization, and parentheses!"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-2",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-2",
    "title": "Visualize Data",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nAdd color, size, alpha, and shape aesthetics to your graph. Experiment.\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#help-me",
    "href": "02_Visualize/02-Visualize-Exercises.html#help-me",
    "title": "Visualize Data",
    "section": "Help Me",
    "text": "Help Me\nWhat do facet_grid() and facet_wrap() do? (run the code, interpret, convince your group)\n\n# Makes a plot that the commands below will modify\nq &lt;- ggplot(mpg) + geom_point(aes(x = displ, y = hwy))\n\nq + facet_grid(cols = vars(cyl))\nq + facet_grid(rows = vars(drv))\nq + facet_grid(rows = vars(drv), cols = vars(cyl))\nq + facet_wrap(facets = vars(class))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-3",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-3",
    "title": "Visualize Data",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nAdd the black code to your graph. What does it do?\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(displ, hwy, color = class))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-4",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-4",
    "title": "Visualize Data",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nReplace this scatterplot with one that draws boxplots. Use the cheatsheet. Try your best guess.\n\nggplot(mpg) + geom_point(aes(class, hwy))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-5",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-5",
    "title": "Visualize Data",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nMake a histogram of the hwy variable from mpg. Hint: do not supply a y variable."
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-6",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-6",
    "title": "Visualize Data",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nUse the help page for geom_histogram to make the bins 2 units wide."
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#your-turn-7",
    "href": "02_Visualize/02-Visualize-Exercises.html#your-turn-7",
    "title": "Visualize Data",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nMake a bar chart class colored by class. Use the help page for geom_bar to choose a “color” aesthetic for class."
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#quiz",
    "href": "02_Visualize/02-Visualize-Exercises.html#quiz",
    "title": "Visualize Data",
    "section": "Quiz",
    "text": "Quiz\nWhat will this code do?\n\nggplot(mpg) + \n  geom_point(aes(displ, hwy)) +\n  geom_smooth(aes(displ, hwy))"
  },
  {
    "objectID": "02_Visualize/02-Visualize-Exercises.html#quiz-1",
    "href": "02_Visualize/02-Visualize-Exercises.html#quiz-1",
    "title": "Visualize Data",
    "section": "Quiz",
    "text": "Quiz\nWhat is different about this plot? Run the code!\n\np &lt;- ggplot(mpg) + \n  geom_point(aes(displ, hwy)) +\n  geom_smooth(aes(displ, hwy))\n\nlibrary(plotly)\nggplotly(p)"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html",
    "href": "04_Model/04-Model-Exercises.html",
    "title": "Models",
    "section": "",
    "text": "Change the working directory to the folder where wages.xlsx is located and this file is saved.\nThen import wages.xlsx as wages and copy the code to your setup chunk.\nBe sure to set NA: to NA.\nSwitch the eval option in the YAML header to true."
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-1",
    "href": "04_Model/04-Model-Exercises.html#your-turn-1",
    "title": "Models",
    "section": "",
    "text": "Change the working directory to the folder where wages.xlsx is located and this file is saved.\nThen import wages.xlsx as wages and copy the code to your setup chunk.\nBe sure to set NA: to NA.\nSwitch the eval option in the YAML header to true."
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-2",
    "href": "04_Model/04-Model-Exercises.html#your-turn-2",
    "title": "Models",
    "section": "Your Turn 2",
    "text": "Your Turn 2\n\nFit the model\n\n\\[\n\\log(\\text{income}) = \\beta_0 + \\beta_1 \\cdot \\text{education} + \\epsilon\n\\]\n\nStore the result in an object called mod_e.\nExamine the output. What does it look like?"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-3",
    "href": "04_Model/04-Model-Exercises.html#your-turn-3",
    "title": "Models",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nUse a pipe to model log(income) against height. Then use broom and dplyr functions to extract:\n\nThe coefficient estimates and their related statistics\nThe adj.r.squared and p.value for the overall model"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-4",
    "href": "04_Model/04-Model-Exercises.html#your-turn-4",
    "title": "Models",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nModel log(income) against education and height and sex. Can you interpret the coefficients?"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-5",
    "href": "04_Model/04-Model-Exercises.html#your-turn-5",
    "title": "Models",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nAdd + geom_smooth(method = lm) to the code below. What happens?\n\nwages |&gt; \n  ggplot(aes(x = height, y = log(income))) +\n  geom_point(alpha = 0.1)"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-6",
    "href": "04_Model/04-Model-Exercises.html#your-turn-6",
    "title": "Models",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nUse add_predictions() to make the plot below. Facetting is by level of education.\n\n\n# In case you haven't made the ehs model\nmod_ehs &lt;- wages|&gt; \n  lm(log(income) ~ education + height + sex, data = _)\n\n# Make plot here"
  },
  {
    "objectID": "04_Model/04-Model-Exercises.html#your-turn-7",
    "href": "04_Model/04-Model-Exercises.html#your-turn-7",
    "title": "Models",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nUse gather_residuals() to make the plot below.\n\n\n\n\n\n\nCaution\n\n\n\nModels mod_h and mod_ehs should be available in your environment because you fitted them in previous sections. But you have to fit and store the model mod_eh which stands for education and height.\n\n\n\n\n# Make the plot here"
  },
  {
    "objectID": "01_Introduction/authoring-with-quarto.html",
    "href": "01_Introduction/authoring-with-quarto.html",
    "title": "Authoring with Quarto",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "01_Introduction/authoring-with-quarto.html#section-1",
    "href": "01_Introduction/authoring-with-quarto.html#section-1",
    "title": "Authoring with Quarto",
    "section": "Section 1",
    "text": "Section 1\nText written in markdown.\n\n# Code written in R\n(x &lt;- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "01_Introduction/authoring-with-quarto.html#section-2",
    "href": "01_Introduction/authoring-with-quarto.html#section-2",
    "title": "Authoring with Quarto",
    "section": "Section 2",
    "text": "Section 2\nText written in markdown.\n\nggplot(data = mpg) +\n  geom_point(aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n\n\n…and so on."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We want to study the energy efficiency of a chemical reaction that is documented having a nominal energy efficiency of \\(90\\%\\). Based on previous experiments on the same reaction, we know that the energy efficiency is a Gaussian random variable with unknown mean \\(\\mu\\) and variance equal to \\(2\\). In the last \\(5\\) days, the plant has given the following energy efficiencies (in percentage):\n\\[ 91.6, \\quad 88.75, \\quad 90.8, \\quad 89.95, \\quad 91.3 \\]\n\nIs the data in accordance with the specifications?\nWhat is a point estimate of the mean energy efficiency?\nDoes that mean that the data significantly prove that the mean energy efficiency is larger than the expected nominal value?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-1",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We want to study the energy efficiency of a chemical reaction that is documented having a nominal energy efficiency of \\(90\\%\\). Based on previous experiments on the same reaction, we know that the energy efficiency is a Gaussian random variable with unknown mean \\(\\mu\\) and variance equal to \\(2\\). In the last \\(5\\) days, the plant has given the following energy efficiencies (in percentage):\n\\[ 91.6, \\quad 88.75, \\quad 90.8, \\quad 89.95, \\quad 91.3 \\]\n\nIs the data in accordance with the specifications?\nWhat is a point estimate of the mean energy efficiency?\nDoes that mean that the data significantly prove that the mean energy efficiency is larger than the expected nominal value?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-2",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-2",
    "title": "Hypothesis Testing",
    "section": "Exercise 2",
    "text": "Exercise 2\nA study about air pollution done by a research station measured, on \\(8\\) different air samples, the following values of a polluant concentration (in \\(\\mu\\)g/m\\(^2\\)):\n\\[ 2.2 \\quad 1.8 \\quad 3.1 \\quad 2.0 \\quad 2.4 \\quad 2.0 \\quad 2.1 \\quad 1.2 \\]\nAssuming that the sampled population is normal,\n\nCan we say that the mean polluant concentration is present with less than \\(2.5 \\mu\\)g/m\\(^2\\)?\nCan we say that the mean polluant concentration is present with less than \\(2.4 \\mu\\)g/m\\(^2\\)?\nIs the normality hypothesis essential to justify the method used?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-3",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-3",
    "title": "Hypothesis Testing",
    "section": "Exercise 3",
    "text": "Exercise 3\nA medical inspection in an elementary school during a measles epidemic led to the examination of \\(30\\) children to assess whether they were affected. The results are in a tibble exam which contains the following:\n\n\n# A tibble: 30 × 2\n      Id Status \n   &lt;int&gt; &lt;chr&gt;  \n 1     1 Healthy\n 2     2 Healthy\n 3     3 Healthy\n 4     4 Healthy\n 5     5 Healthy\n 6     6 Healthy\n 7     7 Healthy\n 8     8 Healthy\n 9     9 Healthy\n10    10 Healthy\n# ℹ 20 more rows\n\n\nLet \\(p\\) be the probability that a child from the same school is sick.\n\nDetermine a point estimate \\(\\widehat{p}\\) for \\(p\\).\nThe school will be closed if more than 5% of the children are sick. Can you conclude that, statistically, this is the case? Use a significance level of 5%."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-4",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-4",
    "title": "Hypothesis Testing",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe capacities (in ampere-hours) of \\(10\\) batteries were recorded as follows:\n\\[ 140, \\quad 136, \\quad 150, \\quad 144, \\quad 148, \\quad 152, \\quad 138, \\quad 141, \\quad 143, \\quad 151 \\]\n\nEstimate the population variance \\(\\sigma^2\\).\nCan we claim that the mean capacity of a battery is greater than 142 ampere-hours ?\nCan we claim that the mean capacity of a battery is greater than 140 ampere-hours ?\nCan we claim that the standard deviation of the capacity is less than 6 ampere-hours ?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-5",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-5",
    "title": "Hypothesis Testing",
    "section": "Exercise 5",
    "text": "Exercise 5\nA company produces barbed wire in skeins of \\(100\\)m each, nominally. The real length of the skeins is a random variable \\(X\\) distributed as a \\(\\mathcal{N}(\\mu, 4)\\). Measuring \\(10\\) skeins, we get the following lengths:\n\\[ 98.683, 96.599, 99.617, 102.544, 100.110, 102.000, 98.394, 100.324, 98.743, 103.247 \\]\n\nPerform a conformity test at significance level \\(\\alpha = 5\\%\\).\nDetermine, on the basis of the observed values, the p-value of the test."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-6",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-6",
    "title": "Hypothesis Testing",
    "section": "Exercise 6",
    "text": "Exercise 6\nIn an atmospheric study the researchers registered, over \\(8\\) different samples of air, the following concentration of COG (in micrograms over cubic meter):\n\\[ 2.3;\\; 1.7;\\; 3.2;\\; 2.1;\\; 2.3;\\; 2.0;\\; 2.2;\\; 1.2 \\]\n\nUsing unbiased estimators, determine a point estimate of the mean and variance of COG concentration.\n\nAssume now that the COG concentration is normally distributed.\n\nUsing a suitable statistical tool, establish whether the measured data allow to say that the mean concentration of COG is greater than \\(1.8\\) \\(\\mu\\)g/m\\(^3\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-7",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-7",
    "title": "Hypothesis Testing",
    "section": "Exercise 7",
    "text": "Exercise 7\nOn a total of \\(2350\\) interviewed citizens, \\(1890\\) approve the construction of a new movie theater.\n\nPerform an hypothesis test of level \\(5\\%\\), with null hypothesis that the percentage of citizens that approve the construction is at least \\(81\\%\\), versus the alternative hypothesis that the percentage is less than \\(81\\%\\).\nCompute the \\(p\\)-value of the test.\n[difficult] Determine the minimum sample size such that the power of the test with significance level \\(\\alpha = 0.05\\) when the real proportion \\(p\\) is \\(0.8\\) is at least \\(50\\%\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-8",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-8",
    "title": "Hypothesis Testing",
    "section": "Exercise 8",
    "text": "Exercise 8\nA computer chip manufacturer claims that no more than \\(1\\%\\) of the chips it sends out are defective. An electronics company, impressed with this claim, has purchased a large quantity of such chips. To determine if the manufacturer’s claim can be taken literally, the company has decided to test a sample of \\(300\\) of these chips. If \\(5\\) of these \\(300\\) chips are found to be defective, should the manufacturer’s claim be rejected?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-9",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-9",
    "title": "Hypothesis Testing",
    "section": "Exercise 9",
    "text": "Exercise 9\nTo determine the impurity level in alloys of steel, two different tests can be used. \\(8\\) specimens are tested, with both procedures, and the results are written in the following table:\n\n\n\nspecimen n.\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nTest 1\n1.2\n1.3\n1.7\n1.8\n1.5\n1.4\n1.4\n1.3\n\n\nTest 2\n1.4\n1.7\n2.0\n2.1\n1.5\n1.3\n1.7\n1.6\n\n\n\nAssume that the data are normal.\n\nbased on the data in the table, can we state that at significance level \\(\\alpha=5\\%\\) the Test 1 and 2 give a different average level of impurity?\nbased on the data in the table, can we state that at significance level \\(\\alpha=1\\%\\) the Test 2 gives an average level of impurity greater than Test 1?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-10",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-10",
    "title": "Hypothesis Testing",
    "section": "Exercise 10",
    "text": "Exercise 10\nA sample of \\(300\\) voters from region A and \\(200\\) voters from region B showed that the \\(56\\%\\) and the \\(48\\%\\), respectively, prefer a certain candidate. Can we say that at a significance level of \\(5\\%\\) there is a difference between the two regions?"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-11",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Exercises.html#exercise-11",
    "title": "Hypothesis Testing",
    "section": "Exercise 11",
    "text": "Exercise 11\nIn a sample of \\(100\\) measures of the boiling temperature of a certain liquid, we obtain a sample mean \\(\\overline{x} = 100^{o}C\\) with a sample variance \\(s^2 = 0.0098^{o}C^2\\). Assuming that the observation comes from a normal population:\n\nWhat is the smallest level of significance that would lead to reject the null hypothesis that the variance is \\(\\leq 0.015\\)?\nOn the basis of the previous answer, what decision do we take if we fix the level of the test equal to \\(0.01\\)?"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html",
    "href": "08_Types/08-Types-Exercises.html",
    "title": "Data Types",
    "section": "",
    "text": "Use flights to create delayed, a variable that displays whether a flight was delayed (arr_delay &gt; 0).\nThen, remove all rows that contain an NA in delayed.\nFinally, create a summary table that shows:\n\nHow many flights were delayed\n\nWhat proportion of flights were delayed"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-1",
    "href": "08_Types/08-Types-Exercises.html#your-turn-1",
    "title": "Data Types",
    "section": "",
    "text": "Use flights to create delayed, a variable that displays whether a flight was delayed (arr_delay &gt; 0).\nThen, remove all rows that contain an NA in delayed.\nFinally, create a summary table that shows:\n\nHow many flights were delayed\n\nWhat proportion of flights were delayed"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-2",
    "href": "08_Types/08-Types-Exercises.html#your-turn-2",
    "title": "Data Types",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nFill in the blanks to:\n\nIsolate the last letter of every name\nCreate a logical variable that displays whether the last letter is one of “a”, “e”, “i”, “o”, “u”, or “y”.\nUse a weighted mean to calculate the proportion of children whose name ends in a vowel (by year and sex)\n\nand then display the results as a line plot.\n\n(Hint: Be sure to remove each _ before turning eval to true)\n\nbabynames |&gt; \n  _______(last = _________, \n          vowel = __________) |&gt; \n  group_by(__________) |&gt; \n  _________(p_vowel = weighted.mean(vowel, n)) |&gt; \n  _________ +\n  __________"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-3",
    "href": "08_Types/08-Types-Exercises.html#your-turn-3",
    "title": "Data Types",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nRepeat the demonstration, some of whose code is below, to make a sensible graph of average TV consumption by marital status.\n(Hint: Be sure to remove each _ before turning eval to true)\n\ngss_cat |&gt; \n  filter(_is.na(________)) |&gt;\n  group_by(________) |&gt;\n  summarise(_________________) |&gt;\n  ggplot() +\n    geom_point(mapping = aes(x = _______, y = _________________________))"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-4",
    "href": "08_Types/08-Types-Exercises.html#your-turn-4",
    "title": "Data Types",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nDo you think liberals or conservatives watch more TV? Compute average tv hours by party ID an then plot the results."
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-5",
    "href": "08_Types/08-Types-Exercises.html#your-turn-5",
    "title": "Data Types",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nWhat is the best time of day to fly?\nUse the hour and minute variables in flights to make a new variable that shows the time of each flight as an hms.\nThen use a smooth line to plot the relationship between time of day and arr_delay."
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-6",
    "href": "08_Types/08-Types-Exercises.html#your-turn-6",
    "title": "Data Types",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nWhat is the best day of the week to fly?\nLook at the code skeleton for Your Turn 7. Discuss with your neighbor:\n\nWhat does each line do?\nWhat will the missing parts need to do?"
  },
  {
    "objectID": "08_Types/08-Types-Exercises.html#your-turn-7",
    "href": "08_Types/08-Types-Exercises.html#your-turn-7",
    "title": "Data Types",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nFill in the blank to:\nExtract the day of the week of each flight (as a full name) from time_hour.\nPlot the average arrival delay by day as a column chart (bar chart).\n(Hint: Be sure to remove each _ before turning eval to true)\n\nflights |&gt; \n  mutate(weekday = _______________________________) |&gt; \n  group_by(weekday) |&gt; \n  filter(!is.na(arr_delay)) |&gt; \n  summarise(avg_delay = mean(arr_delay)) |&gt; \n  ggplot() +\n    geom_col(mapping = aes(x = weekday, y = avg_delay))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Instructions for the class",
    "section": "",
    "text": "Homework Assignment (2025-01-01)\n\n\n\nNew announcement about the homework assignment.\nDelivrable is expected to be a ZIP archive containing the two .qmd files (report and dashboard) along with all the necessary data files.\nPlease ensure that both files compile withour error.\nCheckout the homework assignment page."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Instructions for the class",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis repository is build on the work of Garrett Grolemund from posit. In particular, it reuses an important part of the material he developed for tidyverse-related workshops, which is available at https://github.com/rstudio-education/remaster-the-tidyverse under the Creative Commons BY-SA 4.0 copyright."
  },
  {
    "objectID": "index.html#material",
    "href": "index.html#material",
    "title": "Instructions for the class",
    "section": "Material",
    "text": "Material\nThe main webpage is at https://astamm.github.io/data-science-with-r/."
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Instructions for the class",
    "section": "Outline",
    "text": "Outline\n\nData wrangling with R\nThe class is organised in 9 parts each of which has its own set of slides and exercises. The slides are available in the above Data Wranging - Slides tab and the exercises in the above Data Wranging - Labs tab. The slides are written partly with Keynote (exported as PDFs) and partly in Quarto reveajs slides. The exercises are written in Quarto.\n\n\n\nPart\nTitle\nSlides\nExercises\nSuppl. Material\n\n\n\n\n1\nIntroduction\nPDF\nQuarto\n\n\n\n2\nVisualize Data\nPDF\nQuarto\n\n\n\n3\nTransform Data\nPDF\nQuarto\nCSV\n\n\n4\nModel Data\nPDF\nQuarto\nZIP\n\n\n5\nCommunicate Data\nPDF\nQuarto\nQuarto\n\n\n6\nTidy Data\nPDF\nQuarto\n\n\n\n7\nJoin Data\nPDF\nQuarto\n\n\n\n8\nManipulate Data Types\nPDF\nQuarto\n\n\n\n9\nManipulate Lists\nPDF\nQuarto\n\n\n\n\n\n\nExploratory Data Analysis with R\nThe class is organised in 4 parts each of which has its own set of slides and exercises. The slides are available in the above Exploratory Data Analysis - Slides tab and the exercises in the the above Exploratory Data Analysis - Labs tab. The slides are written in Quarto revealjs slides. The exercises are written in Quarto.\n\n\n\nPart\nTitle\nSlides\nExercises\nSuppl. Material\n\n\n\n\n1\nHypothesis Testing\nQuarto\nQuarto\n\n\n\n2\nLinear Regression\nQuarto\nQuarto\nZIP\n\n\n3\nPrincipal Component Analysis\nRMarkdown\nQuarto\nZIP\n\n\n4\nClustering\nQuarto\nQuarto"
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Instructions for the class",
    "section": "Requirements",
    "text": "Requirements\n\nR: https://www.r-project.org\nRStudio: https://posit.co/download/rstudio-desktop/\nQuarto: https://quarto.org/docs/get-started/\nQuarto Drop extension: https://github.com/r-wasm/quarto-drop\nTidyverse: https://www.tidyverse.org\nSpecific R packages by theme:\n\nData sets:\n\n{babynames}: a data set of frequency of baby names in the US from 1880 to 2017.\n\nData visualization:\n\n{ggplot2}: a package that implements the grammar of graphics.\n{plotly}: an interactive plotting library.\n\nModel summaries:\n\n{broom}: a package that provides tidy summaries of model outputs.\n{modelr}: a package that provides functions for modelling within the tidyverse.\n{jtools}: a package that provides functions for summarizing and visualizing model outputs and main effects.\n{interactions}: a package that provides functions for visualizing the effect of interactions in regression models.\n\nPCA:\n\n{FactoMineR}: a package that provides ready-to-use implementations of standard statistical methods for data analysis.\n{factoextra}: a package that provides functions for extracting and visualizing the results of multivariate data analyses.\n{corrplot}: a package that provides functions for visualizing correlation matrices.\n{viridis}: a package that provides color palettes that are perceptually uniform in both color and black-and-white.\n{huxtable}: a package that provides functions for creating tables in HTML documents."
  },
  {
    "objectID": "05_Report/05-Report-Exercises.html",
    "href": "05_Report/05-Report-Exercises.html",
    "title": "Garrett",
    "section": "",
    "text": "There have been TODO children named Garrett. The name Garrett was most popular in TODO, when TODO TODO were named Garrett. Garrett is traditionally a TODO name."
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html",
    "href": "03_Transform/03-Transform-Exercises.html",
    "title": "Transform Data",
    "section": "",
    "text": "Make sure that the .csv file is in the same directory as the .qmd file you’re currently working on. Then import the babynames.csv data set. Give it the name babynames. Then copy the import code into the code chunk below. Does it run?\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-1",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-1",
    "title": "Transform Data",
    "section": "",
    "text": "Make sure that the .csv file is in the same directory as the .qmd file you’re currently working on. Then import the babynames.csv data set. Give it the name babynames. Then copy the import code into the code chunk below. Does it run?\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-2",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-2",
    "title": "Transform Data",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nAlter the code to select just the n column:\n\nselect(babynames, name, prop)\n\n# A tibble: 1,924,665 × 2\n   name        prop\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Mary      0.0724\n 2 Anna      0.0267\n 3 Emma      0.0205\n 4 Elizabeth 0.0199\n 5 Minnie    0.0179\n 6 Margaret  0.0162\n 7 Ida       0.0151\n 8 Alice     0.0145\n 9 Bertha    0.0135\n10 Sarah     0.0132\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#quiz",
    "href": "03_Transform/03-Transform-Exercises.html#quiz",
    "title": "Transform Data",
    "section": "Quiz",
    "text": "Quiz\nWhich of these is NOT a way to select the name and n columns together?\n\nselect(babynames, -c(year, sex, prop))\n\n# A tibble: 1,924,665 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Mary       7065\n 2 Anna       2604\n 3 Emma       2003\n 4 Elizabeth  1939\n 5 Minnie     1746\n 6 Margaret   1578\n 7 Ida        1472\n 8 Alice      1414\n 9 Bertha     1320\n10 Sarah      1288\n# ℹ 1,924,655 more rows\n\nselect(babynames, name:n)\n\n# A tibble: 1,924,665 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Mary       7065\n 2 Anna       2604\n 3 Emma       2003\n 4 Elizabeth  1939\n 5 Minnie     1746\n 6 Margaret   1578\n 7 Ida        1472\n 8 Alice      1414\n 9 Bertha     1320\n10 Sarah      1288\n# ℹ 1,924,655 more rows\n\nselect(babynames, starts_with(\"n\"))\n\n# A tibble: 1,924,665 × 2\n   name          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Mary       7065\n 2 Anna       2604\n 3 Emma       2003\n 4 Elizabeth  1939\n 5 Minnie     1746\n 6 Margaret   1578\n 7 Ida        1472\n 8 Alice      1414\n 9 Bertha     1320\n10 Sarah      1288\n# ℹ 1,924,655 more rows\n\nselect(babynames, ends_with(\"n\"))\n\n# A tibble: 1,924,665 × 1\n       n\n   &lt;int&gt;\n 1  7065\n 2  2604\n 3  2003\n 4  1939\n 5  1746\n 6  1578\n 7  1472\n 8  1414\n 9  1320\n10  1288\n# ℹ 1,924,655 more rows"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-3",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-3",
    "title": "Transform Data",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nUse filter, babynames, and the logical operators to find:\n\nAll of the names where prop is greater than or equal to 0.08\n\nAll of the children named “Sea”"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-4",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-4",
    "title": "Transform Data",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nUse Boolean operators to return only the rows that contain:\n\nBoys named Sue\n\nNames that were used by exactly 5 or 6 children in 1880\n\nNames that are one of Acura, Lexus, or Yugo"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#help-me",
    "href": "03_Transform/03-Transform-Exercises.html#help-me",
    "title": "Transform Data",
    "section": "Help Me",
    "text": "Help Me\nWhat is the smallest value of n? What is the largest?"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-5",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-5",
    "title": "Transform Data",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nUse |&gt; to write a sequence of functions that:\n\nFilters babynames to just the girls that were born in 2017, then…\n\nSelects the name and n columns, then…\n\nArranges the results so that the most popular names are near the top."
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-6",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-6",
    "title": "Transform Data",
    "section": "Your Turn 6",
    "text": "Your Turn 6\n\nTrim babynames to just the rows that contain your name and your sex\n\nTrim the result to just the columns that will appear in your graph (not strictly necessary, but useful practice)\nPlot the results as a line graph with year on the x axis and prop on the y axis"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-7",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-7",
    "title": "Transform Data",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nCpmplete the code below to extract the rows where name == \"Khaleesi\". Then use summarise() and sum() and min() to find:\n\nThe total number of children named Khaleesi\nThe first year Khaleesi appeared in the data\n\nMake sure to turn the eval option to true before running the code.\n(Hint: Be sure to remove each _ before running the code)\n\nbabynames ___ \n  filter(_______________________) ___\n  ___________(total = ________, first = _______)"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-8",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-8",
    "title": "Transform Data",
    "section": "Your Turn 8",
    "text": "Your Turn 8\nUse group_by(), summarise(), and arrange() to display the ten most popular names. Compute popularity as the total number of children of a single gender given a name.\nMake sure to turn the eval option to true before running the code.\n(Hint: Be sure to remove each _ before running the code)\n\nbabynames |&gt; \n  _______(name, sex) |&gt; \n  _______(total = _____(n)) |&gt; \n  _______(desc(_____))"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-9",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-9",
    "title": "Transform Data",
    "section": "Your Turn 9",
    "text": "Your Turn 9\nUse group_by() to calculate and then plot the total number of children born each year over time."
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-10",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-10",
    "title": "Transform Data",
    "section": "Your Turn 10",
    "text": "Your Turn 10\nUse mutate() and min_rank()to rank each row in babynames from largest n to lowest n.\nMake sure to turn the eval option to true before running the code.\n(Hint: Be sure to remove each _ before running the code)\n\nbabynames |&gt; \n  ______(rank = _______(____(prop)))"
  },
  {
    "objectID": "03_Transform/03-Transform-Exercises.html#your-turn-11",
    "href": "03_Transform/03-Transform-Exercises.html#your-turn-11",
    "title": "Transform Data",
    "section": "Your Turn 11",
    "text": "Your Turn 11\nGroup babynames by year and then re-rank the data. Filter the results to just rows where rank == 1."
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html",
    "href": "06_Tidy/06-Tidy-Exercises.html",
    "title": "Tidy Data",
    "section": "",
    "text": "table1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\ntable5\n\n# A tibble: 6 × 4\n  country     century year  rate             \n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan 19      99    745/19987071     \n2 Afghanistan 20      00    2666/20595360    \n3 Brazil      19      99    37737/172006362  \n4 Brazil      20      00    80488/174504898  \n5 China       19      99    212258/1272915272\n6 China       20      00    213766/1280428583"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-1",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-1",
    "title": "Tidy Data",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nOn a sheet of paper, draw how the cases data set would look if it had the same values grouped into three columns: country, year, n"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-2",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-2",
    "title": "Tidy Data",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nUse pivot_longer() to reorganize table4a into three columns: country, year, and cases."
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-3",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-3",
    "title": "Tidy Data",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nOn a sheet of paper, draw how this data set would look if it had the same values grouped into three columns: city, large, small"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-4",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-4",
    "title": "Tidy Data",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nUse pivot_wider() to reorganize table2 into four columns: country, year, cases, and population."
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-5",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-5",
    "title": "Tidy Data",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nGather the 5th through 60th columns of who into a pair of key:value columns named codes and n.\nThen select just the county, year, codes and n variables.\n\nwho\n\n# A tibble: 7,240 × 60\n   country  iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghani… AF    AFG    1980          NA           NA           NA           NA\n 2 Afghani… AF    AFG    1981          NA           NA           NA           NA\n 3 Afghani… AF    AFG    1982          NA           NA           NA           NA\n 4 Afghani… AF    AFG    1983          NA           NA           NA           NA\n 5 Afghani… AF    AFG    1984          NA           NA           NA           NA\n 6 Afghani… AF    AFG    1985          NA           NA           NA           NA\n 7 Afghani… AF    AFG    1986          NA           NA           NA           NA\n 8 Afghani… AF    AFG    1987          NA           NA           NA           NA\n 9 Afghani… AF    AFG    1988          NA           NA           NA           NA\n10 Afghani… AF    AFG    1989          NA           NA           NA           NA\n# ℹ 7,230 more rows\n# ℹ 52 more variables: new_sp_m4554 &lt;dbl&gt;, new_sp_m5564 &lt;dbl&gt;,\n#   new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;, new_sp_f1524 &lt;dbl&gt;,\n#   new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;, new_sp_f4554 &lt;dbl&gt;,\n#   new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;, new_sn_m014 &lt;dbl&gt;,\n#   new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;, new_sn_m3544 &lt;dbl&gt;,\n#   new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;, new_sn_m65 &lt;dbl&gt;, …"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-6",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-6",
    "title": "Tidy Data",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nSeparate the sexage column into sex and age columns.\n(Hint: Be sure to remove each _ before running the code and switch eval option to true)\n\nwho |&gt; \n  pivot_longer(cols = 5:60, names_to = \"codes\", values_to = \"n\") |&gt; \n  select(-iso2, -iso3) |&gt; \n  separate(codes, c(\"new\", \"type\", \"sexage\"), sep = \"_\") |&gt; \n  select(-new) |&gt; \n  _______________________________"
  },
  {
    "objectID": "06_Tidy/06-Tidy-Exercises.html#your-turn-7",
    "href": "06_Tidy/06-Tidy-Exercises.html#your-turn-7",
    "title": "Tidy Data",
    "section": "Your Turn 7",
    "text": "Your Turn 7\nExtend this code to reshape the data into a data set with three columns:\n\nyear\nM\nF\n\nCalculate the percent of male (or female) children by year. Then plot the percent over time.\n\nbabynames |&gt; \n  group_by(year, sex) |&gt; \n  summarise(n = sum(n))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 276 × 3\n# Groups:   year [138]\n    year sex        n\n   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n 1  1880 F      90993\n 2  1880 M     110491\n 3  1881 F      91953\n 4  1881 M     100743\n 5  1882 F     107847\n 6  1882 M     113686\n 7  1883 F     112319\n 8  1883 M     104627\n 9  1884 F     129020\n10  1884 M     114442\n# ℹ 266 more rows"
  },
  {
    "objectID": "05_Report/05-Report-Parameters.html",
    "href": "05_Report/05-Report-Parameters.html",
    "title": "Garrett",
    "section": "",
    "text": "There have been 130211 children named Garrett. The name Garrett was most popular in 2000, when there were 5840 boys named Garrett. Garrett is traditionally a boy’s name."
  },
  {
    "objectID": "exam1.html",
    "href": "exam1.html",
    "title": "First Exam",
    "section": "",
    "text": "The data set you are going to work on is the penguins data set. The data set contains data on the body mass, flipper length, species of penguins. The data set is available in the {palmerpenguins} package. You can install the package using the following code:\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "exam1.html#data",
    "href": "exam1.html#data",
    "title": "First Exam",
    "section": "",
    "text": "The data set you are going to work on is the penguins data set. The data set contains data on the body mass, flipper length, species of penguins. The data set is available in the {palmerpenguins} package. You can install the package using the following code:\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "exam1.html#goal",
    "href": "exam1.html#goal",
    "title": "First Exam",
    "section": "Goal",
    "text": "Goal\nThe goal of this exam is to reproduce the following plot using the penguins data set:\n\nHint: You need to carefully analze the plot to understand how you need to manipulate the original data set in order to get the correct answer. For the sake of clarity, with fixed slopes means that the slopes of the lines are the same for all species and sexes. with varying slopes means that the slopes of the lines are different for each species and sex."
  },
  {
    "objectID": "exam1.html#rules-expected-delivrables",
    "href": "exam1.html#rules-expected-delivrables",
    "title": "First Exam",
    "section": "Rules & Expected delivrables",
    "text": "Rules & Expected delivrables\nThe exam starts at 11am on Friday, 13th December 2024 and ends at 12pm on the same day. You are not allowed to communicate with anyone during the exam.\nThe following delivrable is expected by email to the instructor by 12:15pm on Friday, 13th December 2024:\n\nA Quarto file which renders without error with the code to reproduce the plot. The name of the file must be of the form exam1_&lt;your_name&gt;.qmd."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#general-data-modeling-framework",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#general-data-modeling-framework",
    "title": "Hypothesis Testing",
    "section": "General data modeling framework",
    "text": "General data modeling framework\n\n\nYou have some data \\(\\{x_1, \\dots, x_n\\}\\) that you presume have been sampled independently from their corresponding random variables \\(\\{X_1, \\dots, X_n\\}\\);\nYou formulate a simple hypothesis \\(H_0\\), called null hypothesis, about the distribution from which this data were sampled;\nYou want to confront this hypothesis against an alternative hypothesis \\(H_a\\) using your finite amount of data;\nYou use a test statistic \\(T(X_1, \\dots, X_n)\\) that depends on the sample (and thus that you can calculate anytime you observe a sample) and of which you know (an approximation of) the distribution when you assume that your null hypothesis is true; it is called the null distribution of the test statistic \\(T\\);\nYou compute the value \\(t_0\\) of this test statistic with your observed data;\nYou strongly reject \\(H_0\\) if \\(t_0\\) falls on the tails of the null distribution of \\(T\\); or,\nYou lack evidence to reject \\(H_0\\) if \\(t_0\\) ends up in the central part of the null distribution of \\(T\\).\nWarning: It is straightforward from this setup to understand that the problem is not symmetric in the hypotheses. Indeed, the procedure relies on what happens when \\(H_0\\) is true but does not depend on \\(H_a\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#theoretical-aspects",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#theoretical-aspects",
    "title": "Hypothesis Testing",
    "section": "Theoretical aspects",
    "text": "Theoretical aspects\n\nYou design a test statistic \\(T(X_1, \\dots, X_n)\\) for the purpose of performing this test which must satisfy at least the first three of the following four properties:\n\nIt evaluates to real values;\nYou have all the required knowledge to compute its value once you observe the data;\nIf \\(H_0\\) is true, then small values of the statistic should comfort you with the idea that \\(H_0\\) is a reasonable assumption, while larger values of the statistic should generate suspicion about \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\);\n[optional] If \\(H_0\\) is true, then it can be very helpful to have access to the (asymptotic) distribution of the test statistic under classical assumptions (such as normality)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean",
    "title": "Hypothesis Testing",
    "section": "Example: Test on the mean",
    "text": "Example: Test on the mean\n\nSuppose you have a sample of \\(n\\) i.i.d. random variables \\(X_1, \\dots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) and you know the value of \\(\\sigma^2\\). We want to test whether the mean \\(\\mu\\) of the distribution is equal to some pre-specified value \\(\\mu_0\\). We can therefore use \\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\). At this point, a good candidate test statistic to look at for performing this test is: \\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma}, \\quad \\mbox{with} \\quad \\overline X := \\frac{1}{n} \\sum_{i=1}^n X_i. \\]\n\nIt evaluates to real values;\nYou have all the required knowledge to compute its value once you observe the data (\\(\\overline x\\));\nThe sample mean \\(\\overline X\\) is an unbiased estimator of the true unknown mean \\(\\mu\\); so, if \\(H_0\\) is true, then \\(\\overline X\\) will produce values that are close to \\(\\mu_0\\); hence, small values of \\(T\\) will comfort you with the idea that \\(H_0\\) is a reasonable assumption, while larger values, both positive or negative, of the statistic should generate suspicion about \\(H_0\\) in favor of the alternative hypothesis \\(H_a\\);\nIf you assume normality and independence of the sample, then \\(T \\sim \\mathcal{N}(0, 1)\\) under \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#distribution-of-the-test-statistic-under-h_0",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#distribution-of-the-test-statistic-under-h_0",
    "title": "Hypothesis Testing",
    "section": "Distribution of the test statistic under \\(H_0\\)",
    "text": "Distribution of the test statistic under \\(H_0\\)\n\n\nParametric testing. If you designed your test statistic carefully, you might have access to its theoretical distribution when \\(H_0\\) is true under distributional assumptions about the data. This is called parametric hypothesis testing.\nAsymptotic testing. If it is not the case, you can often derive the theoretical distribution of the statistic under the null hypothesis asymptotically, i.e. assuming that you have a large sample (\\(n \\gg 1\\)); this is called asymptotic hypothesis testing.\nBootstrap testing. If you are in a large sample size regime but still cannot have access to the theoretical distribution of your test statistic, you can approach this distribution using bootstrapping; this is called bootstrap hypothesis testing.\nPermutation testing. If you are in a low sample size regime, then you can approach the distribution of the test statistic using permutations; this is called permutation hypothesis testing."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#two-sided-vs.-one-sided-hypothesis-tests",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#two-sided-vs.-one-sided-hypothesis-tests",
    "title": "Hypothesis Testing",
    "section": "Two-sided vs. one-sided hypothesis tests",
    "text": "Two-sided vs. one-sided hypothesis tests\n\nDepending on what you put into the alternative hypothesis \\(H_a\\), larger values of the test statistic that raise suspicion regarding the validity of \\(H_0\\) might mean:\n\nlarger values only on the right tail of the null distribution of \\(T\\);\nlarger values only on the left tail of the null distribution of \\(T\\);\nlarger values on both tails.\n\nIn the first two cases, we say that the test is one-sided. In the latter case, we say that the test is two-sided because we interpret large values in both tails as suspicious."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean-continued",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#example-test-on-the-mean-continued",
    "title": "Hypothesis Testing",
    "section": "Example: Test on the mean (continued)",
    "text": "Example: Test on the mean (continued)\n\nSuppose you have a sample of \\(n\\) i.i.d. random variables \\(X_1, \\dots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) and you know the value of \\(\\sigma^2\\). We want to test whether the mean \\(\\mu\\) of the distribution is equal to some pre-specified value \\(\\mu_0\\). As we have seen, a good candidate test statistic to look at for performing this test is:\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma}, \\quad \\mbox{with} \\quad \\overline X := \\frac{1}{n} \\sum_{i=1}^n X_i. \\]\nNow, using this test statistic, we might be interested in performing three different tests:\n\n\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &gt; \\mu_0\\);\n\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &lt; \\mu_0\\);\n\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0",
    "title": "Hypothesis Testing",
    "section": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu > \\mu_0\\)",
    "text": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &gt; \\mu_0\\)\n\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma} \\] Remember that we must look at large values of \\(T\\) that raise suspicion in favor of \\(H_a\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\overline X\\) is an unbiased estimator of the true mean.\nLarge negative values of \\(T\\) that happen when the true mean is far less than \\(\\mu_0\\) does not raise suspicion in favor of \\(H_a\\).\nLarge positive values of \\(T\\) that happen when the true mean is far more than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nConclusion: we are interested only in the right tail of the null distribution of \\(T\\) to find evidence to reject \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0-1",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-mu_0-1",
    "title": "Hypothesis Testing",
    "section": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu < \\mu_0\\)",
    "text": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu &lt; \\mu_0\\)\n\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma} \\] Remember that we must look at large values of \\(T\\) that raise suspicion in favor of \\(H_a\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\overline X\\) is an unbiased estimator of the true mean.\nLarge negative values of \\(T\\) that happen when the true mean is far more than \\(\\mu_0\\) does not raise suspicion in favor of \\(H_a\\).\nLarge positive values of \\(T\\) that happen when the true mean is far less than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nConclusion: we are interested only in the left tail of the null distribution of \\(T\\) to find evidence to reject \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-ne-mu_0",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#h_0-mu-mu_0-vs.-h_a-mu-ne-mu_0",
    "title": "Hypothesis Testing",
    "section": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\)",
    "text": "\\(H_0: \\mu = \\mu_0\\) vs. \\(H_a: \\mu \\ne \\mu_0\\)\n\n\\[ T(X_1, \\dots, X_n) := \\sqrt{n} \\frac{\\overline X - \\mu_0}{\\sigma} \\] Remember that we must look at large values of \\(T\\) that raise suspicion in favor of \\(H_a\\).\n\n\n\n\n\n\n\n\n\n\n\\(\\overline X\\) is an unbiased estimator of the true mean.\nLarge negative values of \\(T\\) that happen when the true mean is far more than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nLarge positive values of \\(T\\) that happen when the true mean is far less than \\(\\mu_0\\) does raise suspicion in favor of \\(H_a\\).\nConclusion: we are interested in both the left and the right tails of the null distribution of \\(T\\) to find evidence to reject \\(H_0\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#type-i-and-type-ii-errors",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#type-i-and-type-ii-errors",
    "title": "Hypothesis Testing",
    "section": "Type I and Type II errors",
    "text": "Type I and Type II errors\n\nSo we are now at a point where we know which tail(s) which we should look at and we now need to make a decision as to what large means. In other words, above which threshold on all possible values of my test statistic should I consider that I can reject \\(H_0\\).\n\nNotice that you are going to take this decision based on the null distribution.\nWhen you decide to reject or not, you might make an error:\n\n\n\n\n\n\nDecision\n\\(H_0\\) is true\n\\(H_a\\) is true\n\n\n\n\nDo not reject \\(H_0\\)\nWell done\nType II error\n\n\nReject \\(H_0\\)\nType I error\nWell done\n\n\n\n\n\n\n\n\nThe only error rate you can control is the type I error rate because it is a probability computed assuming that the null hypothesis is true, which is exactly the situation we put ourselves in for making the decision (i.e. looking at the tails of the null distribution of \\(T\\))."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level",
    "title": "Hypothesis Testing",
    "section": "Significance level",
    "text": "Significance level\n\n\nAt this point, you can decide that you do not want to make more than a certain amount of type I errors. So you want to force that \\(\\mathbb{P}_{H_0}(\\mbox{reject } H_0) \\le \\alpha\\), for some upper bound threshold \\(\\alpha\\) on the probability of type I errors. This threshold is called significance level of the test and is often denoted by the greek letter \\(\\alpha\\).\nLet us now translate what this rule implies for the right-tail alternative case. The event “\\(\\mbox{reject } H_0\\)” translates in this case into \\(T &gt; x\\) for some \\(x\\) value of the test statistic \\(T\\). Hence, the rule becomes: \\[ \\mathbb{P}_{H_0}(T &gt; x) \\le \\alpha \\\\ \\Leftrightarrow 1 - \\mathbb{P}_{H_0}(T \\le x) \\le \\alpha \\\\ \\Leftrightarrow 1 - F_T^{\\left(H_0\\right)}(x) \\le \\alpha \\\\ \\Leftrightarrow F_T^{\\left(H_0\\right)}(x) \\ge 1 - \\alpha \\] verified for all \\(x \\ge q_{1-\\alpha}\\), where \\(q_{1-\\alpha}\\) is the quantile of order \\(1-\\alpha\\) of the null distribution of \\(T\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level-continued",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#significance-level-continued",
    "title": "Hypothesis Testing",
    "section": "Significance level (continued)",
    "text": "Significance level (continued)\n\nDecision-making rule:\n\nWe reject \\(H_0\\) if the value \\(t_0\\) of the test statistic computed on the observed sample is greater than the quantile of order \\(1-\\alpha\\) of the null distribution of \\(T\\);\nWe decide that we lack evidence to reject \\(H_0\\) if the value \\(t_0\\) of the test statistic calculated on the observed sample is smaller than the quantile of order \\(1-\\alpha\\) of the null distribution of \\(T\\).\nThis decision-making rule guarantees that the probability of making a type I error is upper-bounded by the significance level \\(\\alpha\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#p-value",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#p-value",
    "title": "Hypothesis Testing",
    "section": "p-value",
    "text": "p-value\n\n\nDefinition. The p-value is a scalar value between \\(0\\) and \\(1\\) that measures what was the probability, assuming that the null hypothesis \\(H_0\\) is true, of observing the data we did observe, or data even more in favor of the alternative hypothesis.\nMathematical expression. If \\(t_0\\) is the value of the test statistic computed from the observed sample, then:\n\n\\(p = \\mathbb{P}_{H_0}(T &gt; t_0)\\) for right-tail hypothesis tests (e.g. \\(H_a: \\mu &gt; \\mu_0\\) when testing the mean);\n\\(p = \\mathbb{P}_{H_0}(T &lt; t_0)\\) for left-tail hypothesis tests (e.g. \\(H_a: \\mu &lt; \\mu_0\\) when testing the mean);\n\\(p = 2 \\min\\left( \\mathbb{P}_{H_0} \\left( T &gt; t_0 \\right), \\mathbb{P}_{H_0} \\left( T &lt; t_0 \\right) \\right)\\) for two-tail hypothesis tests (e.g. \\(H_a: \\mu \\ne \\mu_0\\) when testing the mean).\n\nInterpretation. If the \\(p\\)-value is very small, it means that\n\neither we observed a miracle,\nor the null hypothesis might be wrong.\n\nDecision-making rule. We can show that rejecting the null hypothesis when \\(p \\le \\alpha\\) also produces a decision-making rule that guarantees a probability of type I error at most \\(\\alpha\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#decision-making-summary-right-tail-scenario",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#decision-making-summary-right-tail-scenario",
    "title": "Hypothesis Testing",
    "section": "Decision-Making: Summary (right-tail scenario)",
    "text": "Decision-Making: Summary (right-tail scenario)"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#statistical-power-of-a-test",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#statistical-power-of-a-test",
    "title": "Hypothesis Testing",
    "section": "Statistical power of a test",
    "text": "Statistical power of a test\n\n\nDefinition. It is the probability of correctly rejecting the null hypothesis, i.e. to reject it when the alternative is in fact correct. It is often denoted by \\(\\mu\\). In terms of events, it is defined by: \\[ \\mu := \\mathbb{P}_{H_a}(\\mbox{Reject } H_0). \\]\nUsage. The statistical power of a test is an important aspect of the test because:\n\nit is an important performance indicator to compare different testing procedures (observe that there is not a unique statistic to perform a given test);\nit is often used in clinical trials or other types of trials (e.g. crash tests) to calibrate the number of observations required to achieve a given statistical power.\n\nRemarks.\n\nThe statistical power \\(\\mu\\) is equal to \\(1 - \\beta\\), where \\(\\beta\\) is the greek letter often used to designate the probability of type II errors, \\(\\beta := \\mathbb{P}_{H_a}(\\mbox{Do not reject } H_0)\\).\nPower calculations are difficult because it requires to put ourselves under \\(H_a\\), which is often of the form \\(\\mu &gt; \\mu_0\\) or \\(\\mu &lt; \\mu_0\\) or \\(\\mu \\ne \\mu_0\\). In other words, you often lack information to compute probabilities assuming that the alternative hypothesis is true. You have to assess how the power changes as you explore different alternatives."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean",
    "title": "Hypothesis Testing",
    "section": "Testing the mean",
    "text": "Testing the mean\n\n\nModel. Let \\((X_1, \\dots, X_n)\\) be \\(n\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nHypotheses. \\[ H_0: \\mu = \\mu_0 \\quad \\mbox{vs.} \\quad H_a: \\mu \\ne \\mu_0 \\quad \\mbox{or} \\quad H_a: \\mu &gt; \\mu_0 \\quad \\mbox{or} \\quad H_a: \\mu &lt; \\mu_0. \\]\nTest Statistic. \\[ Z_n = \\sqrt{n} \\frac{\\overline X_n - \\mu_0}{\\sigma}. \\]\nProblem. Under the null hypothesis, I do not have complete knowledge to compute the test statistic because I do not know the value of \\(\\sigma\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-known-variance",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-known-variance",
    "title": "Hypothesis Testing",
    "section": "Testing the mean with known variance",
    "text": "Testing the mean with known variance\n\n\nProblem solved. I now have complete knowledge to compute the test statistic.\nNull distribution. The null distribution is \\[ Z_n \\stackrel{H_0}{\\sim} \\mathcal{N}(0, 1). \\]\nR function. None"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-unknown-variance",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-mean-with-unknown-variance",
    "title": "Hypothesis Testing",
    "section": "Testing the mean with unknown variance",
    "text": "Testing the mean with unknown variance\n\n\nProblem solved. I plug in the empirical standard deviation instead of \\(\\sigma\\) in the definition of the test statistic.\nTest Statistic. \\[ T_n = \\frac{\\overline X_n - \\mu_0}{\\sqrt{\\frac{S_{n-1}^2}{n}}}, \\quad \\mbox{with} \\quad S_{n-1}^2 := \\frac{1}{n-1} \\sum_{i = 1}^n (X_i - \\overline X)^2. \\]\nNull distribution. \\[ T_n \\stackrel{H_0}{\\sim} \\mathcal{S}tudent(n-1). \\]\nR function. t.test()"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance",
    "title": "Hypothesis Testing",
    "section": "Testing the variance",
    "text": "Testing the variance\n\n\nAssumptions. Let \\((X_1, \\dots, X_n)\\) be \\(n\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nHypotheses. \\[ H_0: \\sigma^2 = \\sigma_0^2 \\quad \\mbox{vs.} \\quad H_a: \\sigma^2 \\ne \\sigma_0^2 \\quad \\mbox{or} \\quad H_a: \\sigma^2 &gt; \\sigma_0^2 \\quad \\mbox{or} \\quad H_a: \\sigma^2 &lt; \\sigma_0^2. \\]\nTest Statistic. \\[ U_n = \\sum_{i = 1}^n \\frac{(X_i - \\mu)^2}{\\sigma_0^2}. \\]\nProblem. Under the null hypothesis, I do not have complete knowledge to compute the test statistic because I do not know the value of \\(\\mu\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-known-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-known-mean",
    "title": "Hypothesis Testing",
    "section": "Testing the variance with known mean",
    "text": "Testing the variance with known mean\n\n\nProblem solved. I now have complete knowledge to compute the test statistic.\nNull distribution. The null distribution is \\[ U_n \\stackrel{H_0}{\\sim} \\chi^2(n). \\]\nR function. None."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-unknown-mean",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-the-variance-with-unknown-mean",
    "title": "Hypothesis Testing",
    "section": "Testing the variance with unknown mean",
    "text": "Testing the variance with unknown mean\n\n\nProblem solved. I plug in the empirical mean instead of \\(\\mu\\) in the definition of the test statistic.\nTest Statistic. \\[ U_{n-1} = \\sum_{i = 1}^n \\frac{\\left( X_i - \\overline X_n \\right)^2}{\\sigma_0^2}. \\]\nNull distribution. \\[ U_{n-1} \\stackrel{H_0}{\\sim} \\chi^2(n-1). \\]\nR function. None."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-a-proportion",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-a-proportion",
    "title": "Hypothesis Testing",
    "section": "Testing a proportion",
    "text": "Testing a proportion\n\nHere we want to test whether the proportion \\(p\\) of individuals in a given population who have a feature of interest is equal to a pre-specified rate.\n\nModel. Let \\((X_1, \\dots, X_n)\\) be \\(n\\) i.i.d. random variables that follow a Bernoulli distribution \\(\\mathcal{B}e(p)\\). The interpretation is that \\(X_i\\) measures if individual \\(i\\) possesses the characteristic of which we want to know the proportion or not.\nHypotheses. \\[ H_0: p = p_0 \\quad \\mbox{vs.} \\quad H_a: p \\ne p_0 \\quad \\mbox{or} \\quad H_a: p &gt; p_0 \\quad \\mbox{or} \\quad H_a: p &lt; p_0. \\]\nTest Statistic. \\[ B_n = \\sum_{i=1}^n X_i \\]\nNull distribution. \\[ B_n \\sim \\mathcal{B}inom(n,p_0) \\]\nR function. binom.test()"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-variance-differences",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-variance-differences",
    "title": "Hypothesis Testing",
    "section": "Testing for variance differences",
    "text": "Testing for variance differences\n\n\nModel. Let \\((X_1, \\dots, X_{n_x})\\) be \\(n_X\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_X, \\sigma_X^2)\\) and \\((Y_1, \\dots, Y_{n_Y})\\) be \\(n_Y\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_Y, \\sigma_Y^2)\\). Assume furthermore that the two samples are statistically independent.\nHypotheses. \\[ H_0: \\sigma_X^2 = \\sigma_Y^2 \\quad \\mbox{vs.} \\quad H_a: \\sigma_X^2 \\ne \\sigma_Y^2 \\quad \\mbox{or} \\quad H_a: \\sigma_X^2 &gt; \\sigma_Y^2 \\quad \\mbox{or} \\quad H_a: \\sigma_X^2 &lt; \\sigma_Y^2 \\]\nTest Statistic. \\[ V_n = \\frac{S_X^2}{S_Y^2}, \\quad \\mbox{with} \\quad S_X^2 = \\frac{1}{n_X - 1} \\sum_{i = 1}^{n_X} (X_i - \\overline X_n)^2 \\quad \\mbox{and} \\quad S_Y^2 = \\frac{1}{n_Y - 1} \\sum_{i = 1}^{n_Y} (Y_i - \\overline Y_n)^2 \\]\nNull distribution. \\[ V_n \\stackrel{H_0}{\\sim} \\mathcal{F}isher(n_X - 1, n_Y - 1) \\]\nR function. var.test()\nValidity. Relies on the normality assumption and independence within and between samples."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences",
    "title": "Hypothesis Testing",
    "section": "Testing for mean differences",
    "text": "Testing for mean differences\n\n\nModel. Let \\((X_1, \\dots, X_{n_x})\\) be \\(n_X\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_X, \\sigma_X^2)\\) and \\((Y_1, \\dots, Y_{n_Y})\\) be \\(n_Y\\) i.i.d. random variables that follow a normal distribution \\(\\mathcal{N}(\\mu_Y, \\sigma_Y^2)\\). Assume furthermore that the two samples are statistically independent.\nHypotheses. \\[ H_0: \\mu_X = \\mu_Y \\quad \\mbox{vs.} \\quad H_a: \\mu_X \\ne \\mu_Y \\quad \\mbox{or} \\quad H_a: \\mu_X &gt; \\mu_Y \\quad \\mbox{or} \\quad H_a: \\mu_X &lt; \\mu_Y \\]\nR function. t.test()\nValidity. Relies on the normality assumption and independence within and between samples."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-equal",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-equal",
    "title": "Hypothesis Testing",
    "section": "Testing for mean differences when variances are equal",
    "text": "Testing for mean differences when variances are equal\n\n\nTest Statistic. Let \\(\\delta = \\mu_X - \\mu_Y\\) be the mean difference and \\(\\delta_0\\) be the assumed mean difference under \\(H_0\\). Then, \\[\nT_n = \\frac{\\left( \\overline X_n - \\overline Y_n \\right) - \\delta_0}{\\sqrt{S_\\mathrm{pooled}^2 \\left( \\frac{1}{n_X} + \\frac{1}{n_Y} \\right)}} \\mbox{ with } S_\\mathrm{pooled}^2 = \\frac{(n_X - 1) S_X^2 + (n_Y - 1) S_Y^2}{n_X + n_Y - 2}\n\\]\nNull distribution. \\[\nT_n \\stackrel{H_0}{\\sim} \\mathcal{S}tudent(n_X + n_Y - 2)\n\\]"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-not-equal",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#testing-for-mean-differences-when-variances-are-not-equal",
    "title": "Hypothesis Testing",
    "section": "Testing for mean differences when variances are not equal",
    "text": "Testing for mean differences when variances are not equal\n\n\nTest Statistic. \\[\nT_n = \\frac{\\left( \\overline X_n - \\overline Y_n \\right) - \\delta_0}{\\sqrt{\\frac{S_X^2}{n_X} + \\frac{S_Y^2}{n_Y}}}\n\\]\nNull distribution. \\[\nT_n \\stackrel{H_0}{\\sim} \\mathcal{S}tudent(m) \\mbox{ with } m = \\frac{\\left( \\frac{S_X^2}{n_X} + \\frac{S_Y^2}{n_Y} \\right)^2}{\\frac{\\left( \\frac{S_X^2}{n_X} \\right)^2}{n_X-1} + \\frac{\\left( \\frac{S_Y^2}{n_Y} \\right)^2}{n_Y-1}}.\n\\]"
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#shapiro-wilk-test",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#shapiro-wilk-test",
    "title": "Hypothesis Testing",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\n\n\nHypotheses. \\[ H_0: (X_1, \\dots, X_n) \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\mu, \\sigma^2) \\quad \\mbox{vs.} \\quad H_a: (X_1, \\dots, X_n) \\not\\sim \\mathcal{N}(\\mu, \\sigma^2). \\]\nTest Statistic. \\[\nT_n = {\\left(\\sum_{i=1}^n a_i X_{(i)}\\right)^2 \\over \\sum_{i=1}^n (X_i-\\overline{X}_n)^2}\n\\] where \\(X_{(i)}\\) is the order statistic for observation \\(i\\), \\(\\overline X_n\\) the sample mean and \\((a_1, \\dots, a_n)\\) are weights computed from the first two moments of the order statistics of standard normal variables.\nNull distribution. \\[ T \\stackrel{H_0}{\\sim} \\mathcal{W}ilks(n). \\]\nR function. shapiro.test()\nValidity. The sample size should meet \\(3 \\le n \\le 5000\\). The Wilks distribution is approximated except for \\(n=3\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#kolmogorov-smirnov-test",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#kolmogorov-smirnov-test",
    "title": "Hypothesis Testing",
    "section": "Kolmogorov-Smirnov test",
    "text": "Kolmogorov-Smirnov test\n\n\nHypotheses. \\[ H_0: (X_1, \\dots, X_n) \\stackrel{i.i.d.}{\\sim} F_0 \\quad \\mbox{vs.} \\quad H_a: (X_1, \\dots, X_n) \\not\\sim F_0. \\]\nTest Statistic. \\[\nT_n = \\sup_{x \\in \\mathbb R} | F_n(x) - F(x)|\n\\] where \\(F_n\\) is the cumulative distribution function and \\(F\\) is the cumulative distribution function of the law under testing.\nNull distribution. \\[ \\sqrt{n} T \\xrightarrow{\\mathcal{L}} \\sup_{x \\in \\mathbb R} | B(F(x))|, \\] where \\(B\\) is the Brownian bridge.\nR function. ks.test()\nValidity. Asymptotic."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi2-test-of-adequacy-for-a-single-categorical-variable",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi2-test-of-adequacy-for-a-single-categorical-variable",
    "title": "Hypothesis Testing",
    "section": "\\(\\chi^2\\) test of adequacy for a single categorical variable",
    "text": "\\(\\chi^2\\) test of adequacy for a single categorical variable\n\n\nModel. We group observations in classes and we compare the observed frequencies of these classes to the corresponding theoretical frequencies as given by the hypothesized law \\(F_0\\).\nHypotheses. \\(H_0: (X_1, \\dots, X_n) \\stackrel{i.i.d.}{\\sim} F_0 \\quad \\mbox{vs.} \\quad H_a: (X_1, \\dots, X_n) \\not\\sim F_0\\).\nTest Statistic. \\[\nU_n = n\\sum_{i=1}^k\\frac{(f_i-f_{0i})^2}{f_{0i}},\n\\] where \\(n\\) is the total number of observations, \\(k\\) the number of classes, \\(f_i = n_i / n\\) and \\(f_{i0}\\) the theoretical frequency of class \\(i\\), i.e. the probability that the random variable ends up in class \\(i\\).\nNull distribution. \\[ U_n \\xrightarrow{\\mathcal{L}} \\chi^2_{k - 1 - \\ell}, \\mbox{ where } \\ell \\mbox{ is the number of estimated parameters for } F_0.\\]\nR function. chisq.test()\nValidity. Requires large class frequencies. Typically, \\(n_i \\ge 5\\)."
  },
  {
    "objectID": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi-2-test-of-independence-between-two-categorical-variables",
    "href": "10_Hypothesis_Testing/10-Hypothesis-Testing-Slides.html#chi-2-test-of-independence-between-two-categorical-variables",
    "title": "Hypothesis Testing",
    "section": "\\(\\chi ^2\\) test of independence between two categorical variables",
    "text": "\\(\\chi ^2\\) test of independence between two categorical variables\n\n\nModel. Let \\(X = (Y, Z) = ((Y_1, Z_1), \\dots, (Y_n, Z_n))\\) be a bivariate sample of \\(n\\) i.i.d. pairs of categorical random variables. Let \\(\\nu\\) be the law of \\((Y_1, Z_1)\\), \\(\\mu\\) the law of \\(Y_1\\) and \\(\\lambda\\) the law of \\(Z_1\\). Let \\(\\{y_1, \\dots, y_s\\}\\) be the set of possible values for \\(Y_1\\) and \\(\\{z_1, \\dots, z_r\\}\\) the set of possible values for \\(Z_1\\). For \\(\\ell \\in \\{1, \\dots, s\\}\\) et \\(h \\in \\{1, \\dots, r\\}\\), let \\[ N_{\\ell,\\cdot} = \\left| \\left\\{ i  \\in \\{1, \\dots, n\\}; Y_i = y_\\ell \\right\\} \\right|,\\quad N_{\\cdot, h} = \\left| \\left\\{ i  \\in \\{1, \\dots, n\\}; Z_i = z_h \\right\\} \\right|, \\\\ N_{\\ell,h} = \\left| \\left\\{(i \\in \\{1, \\dots, n\\}; Y_i = y_\\ell, Z_i = z_h \\right\\} \\right|.\n\\]\nHypotheses. \\(H_0: \\nu = \\mu \\otimes \\lambda \\quad \\mbox{vs.} \\quad H_a: \\nu \\ne \\mu \\otimes \\lambda\\).\nTest statistic. \\[ U_n = n  \\sum_{\\ell = 1}^s \\sum_{h = 1}^r \\frac{ \\left( \\frac{N_{\\ell, h}}{n}  -  \\frac{N_{\\ell, \\cdot}}{n} \\frac{N_{\\cdot, h}}{n} \\right)^2 }{ \\frac{N_{\\ell, \\cdot}}{n} \\frac{N_{\\cdot, h}}{n} }. \\]\nNull distribution. \\(U_n \\xrightarrow{\\mathcal{L}} \\chi^2 \\left( (s-1)(r-1) \\right)\\).\nR function. chisq.test()\nValidity. Asymptotic. Often, \\(n \\gg 30\\) and \\(N_{\\ell, h} \\gg 5\\), for each pair \\((\\ell, h)\\)."
  },
  {
    "objectID": "01_Introduction/01-Introduction-Exercises.html",
    "href": "01_Introduction/01-Introduction-Exercises.html",
    "title": "Quarto",
    "section": "",
    "text": "This is a Quarto file. It contains plain text interspersed with grey chunks of code. You can use the file to take notes and run code. For example, you can write your name on the line below. Try it:\n\n# You can write code in chunks that look like this.\n# This chunk uses the code plot(cars) to plot a data set.\n# To run the code, click the Green play button at the\n# top right of this chunk. Try it!\nplot(cars)\n\n\n\n\n\n\n\n\nGood job! The results of a code chunk will appear beneath the chunk. You can click the x above the results to make them go away, but let’s not do that.\nYou can open a new Quarto file by going to File &gt; New File &gt; Quarto Document…. Then click OK. But let’s not open a new file now—keep reading this one!\n\nAdding chunks\nTo add a new code chunk, press Cmd+Option+I (Ctrl+Alt+I on Windows), or click the Insert button at the top of this document, then select R. Quarto will add a new, empty chunk at your cursor’s location.\nTry making a code chunk below:\nGood job! For today, you should place all of your R code inside of code chunks.\n\n# Sometimes you might want to run only some of the code \n# in a code chunk. To do that, highlight the code to \n# run and then press Cmd + Enter (Control + Enter on \n# Windows). If you do not highlight any code, R will \n# run the line of code that your cursor is on.\n# Try it now. Run mean(1:5) but not the line below.\nmean(1:5)\n\n[1] 3\n\nwarning(\"You shouldn't run this!\")\n\nWarning: You shouldn't run this!\n\n\n\n# You can click the downward facing arrow to the left of the play button to run\n# every chunk above the current code chunk. This is useful if the code in your\n# chunk uses object that you made in previous chunks.\n# Sys.Date()\n\nDid you notice the green lines (if your RStudio theme is the default one) in the code chunk above? They are code comments, lines of text that R ignores when it runs the code. R will treat everything that appears after # on a line as a code comment. As a result, if you run the chunk above, nothing will happen—it is all code comments (and that’s fine)!\nRemove the # on the last line of the chunk above and then rerun the chunk. Can you tell what Sys.Date() does?\nBy the way, you only need to use code comments inside of code chunks. R knows not to try to run the text that you write outside of code chunks. You can press Cmd+Shift+C to comment or uncomment a line of code.\n\n\nText formatting\nHave you noticed the funny highlighting that appears in this document? Quarto treats text surrounded by asterisks, double asterisks, and backticks in special ways. It is Quarto’s way of saying that these words are in\n\nitalics\nalso italics\nbold, and\ncode font\n\n*, **, and ` are signals used by a text editing format known as markdown. Quarto uses markdown to turn your plain looking .Rmd documents into polished reports. Let’s give that a try.\n\n\nReports\nWhen you click the Render button at the top of an Quarto file (like this one), Quarto generates a polished copy of your report. Quarto:\n\nTransforms all of your markdown cues into actual formatted text (e.g. bold text, italic text, etc.)\nReruns all of your code chunks in a clean R session and appends the results to the finished report.\nSaves the finished report alongside your .Rmd file\n\nClick the Render button at the top of this document or press Cmd+Shift+K (Ctrl+Shift+K on Windows) to render the finished report. The RStudio IDE will open the report so you can see its contents. For now, our reports will be HTML files. Try clicking Render now.\nGood job! You’ll learn more about Quarto throughout the day!\n\n\nR Packages\nHere is one last code chunk that we will use in the next exercise. If you uncomment the code and try to run it, it won’t work. If you don’t believe me try!\n\n# ggplot(data = diamonds) + geom_point(aes(x = carat, y = price))"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html",
    "href": "07_Join/07-Join-Exercises.html",
    "title": "Join Data with dplyr",
    "section": "",
    "text": "flights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nVisualizing a raw tibble is not that pretty. Let’s wrap the next output in a DT::datatable() function to make it more readable:\n\nDT::datatable(airlines)"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html#your-turn-1",
    "href": "07_Join/07-Join-Exercises.html#your-turn-1",
    "title": "Join Data with dplyr",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nWhich airlines had the largest arrival delays? Complete the code below.\n\nJoin airlines to flights\nCompute and order the average arrival delays by airline. Display full names, no codes.\n\n(Hint: Be sure to remove each _ before turning eval to true)\n\nflights |&gt;\n  filter(!is.na(arr_delay)) |&gt;\n  __________________  |&gt;\n  group_by(_________) |&gt;\n  __________________  |&gt;\n  arrange(__________)"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html#your-turn-2",
    "href": "07_Join/07-Join-Exercises.html#your-turn-2",
    "title": "Join Data with dplyr",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nJoin flights and airports by dest and faa.\nThen for each name, compute the distance from NYC and the average arr_delay. Hint: use first() to get the first value of distance.\nOrder by average delay, worst to best.\n(Hint: Be sure to remove each _ before turning eval to true)\n\nflights |&gt; \n  filter(!is.na(arr_delay)) |&gt;\n  ______(airports, __________________) |&gt;\n  group_by(name) |&gt;\n  _________(distance = _____________, \n               delay = _____________) |&gt;\n  ______(_____(delay))"
  },
  {
    "objectID": "07_Join/07-Join-Exercises.html#your-turn-3",
    "href": "07_Join/07-Join-Exercises.html#your-turn-3",
    "title": "Join Data with dplyr",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nHow many airports in airports are serviced by flights in flights? (i.e. how many places can you fly to direct from New York?)\nNotice that the column to join on is named faa in the airports data set and dest in the flights data set.\n(Hint: Be sure to remove each _ before turning eval to true)\n\n__________ |&gt;\n _____________________________ |&gt;\n  select(faa)"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Car accidents in France",
    "section": "",
    "text": "This page explains the project proposed to dig deeper into Quarto and its features. The project is about car accidents in France. The data is available on the Open Data platform of the French government. The data is available in the form of several CSV files. The data contains information about car accidents in France from 2005 to 2023. The data contains information about the date and time of the accident, the location of the accident, the number of people involved in the accident, the number of people injured, the number of people killed, the type of vehicles involved in the accident, the type of road, the weather conditions, the lighting conditions, the type of intersection, the type of collision, the type of accident, the causes of the accident, etc."
  },
  {
    "objectID": "project.html#description-of-the-data-set",
    "href": "project.html#description-of-the-data-set",
    "title": "Car accidents in France",
    "section": "",
    "text": "This page explains the project proposed to dig deeper into Quarto and its features. The project is about car accidents in France. The data is available on the Open Data platform of the French government. The data is available in the form of several CSV files. The data contains information about car accidents in France from 2005 to 2023. The data contains information about the date and time of the accident, the location of the accident, the number of people involved in the accident, the number of people injured, the number of people killed, the type of vehicles involved in the accident, the type of road, the weather conditions, the lighting conditions, the type of intersection, the type of collision, the type of accident, the causes of the accident, etc."
  },
  {
    "objectID": "project.html#aims-of-the-project",
    "href": "project.html#aims-of-the-project",
    "title": "Car accidents in France",
    "section": "Aims of the project",
    "text": "Aims of the project\nThe project aims to dig into interactive Quarto dashboards to analyze the data and extract insights. The project can involve the following steps:\n\nData collection: Download the data from the Open Data platform of the French government. You can use packages such as {readr} or {readxl} to read the data into R.\nData preprocessing: Clean the data, handle missing values, and transform the data into a suitable format for analysis. For this task, you can use:\n\n{dplyr} and {tidyr} and other tidyverse packages for data manipulation and reshaping;\n{skimr} to get a summary of the data;\n{janitor} to clean the data;\n{naniar} to visualize missing values and take informed decisions about handling them.\n\nData analysis: Use Quarto Dashboard and shiny to create interactive dashboards to analyze the data and extract insights. See https://quarto.org/docs/dashboards/interactivity/ for easy interactivity in Quarto dashboard using Shiny.\nData visualization: Use {ggplot2} and {plotly} to create interactive visualizations to present the insights.\nSpatial Data: Use appropriate packages to create maps to visualize the location of accidents with superimposed relevant information.\n\nThe project will help to understand the patterns and trends in car accidents in France and identify the factors that contribute to accidents. The project will also help to explore the relationship between different variables and their impact on accidents. The project will be a good example to showcase the capabilities of Quarto for data analysis and visualization."
  },
  {
    "objectID": "project.html#rules-expected-delivrables",
    "href": "project.html#rules-expected-delivrables",
    "title": "Car accidents in France",
    "section": "Rules & Expected delivrables",
    "text": "Rules & Expected delivrables\nThe following delivrables are expected from the project on Monday, 6th January 2025 at 11:59 PM at the latest:\n\nInteractive dashboards to analyze the data and extract insights.\nA Quarto document detailing the thinking process and actual steps taken that led to the provided dashboards.\n\nHence, delivrables should be made of two .qmd files.\nThe project is to be done in groups of 3 students. You should email the instructor with the names of the students in your group by Tuesday, 10th December 2024. The instructor will assign a group number to each group. The group number should be included in the Quarto document and the interactive dashboards.\n\n\n\n\n\n\nDelivrable\n\n\n\nDelivrable is expected to be a ZIP archive containing the two .qmd files (report and dashboard) along with all the necessary data files.\nPlease ensure that both files compile withour error."
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html",
    "href": "12_PCA/12-PCA-Exercises.html",
    "title": "PCA - Exercises",
    "section": "",
    "text": "Cette base contient:\n\nles enregistrements des températures des capitales européennes de Janvier à Décembre\nles coordonnées GPS de chaque ville\namplitude thermale : Différence entre les températures maximales et minimales\nmoyenne annuelle\nune variable qualitative : la direction (S, N, O, E).\n\nExécuter une PCA pour dégager des profils type de température et quelles villes les suivent."
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-temperature.csv",
    "href": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-temperature.csv",
    "title": "PCA - Exercises",
    "section": "",
    "text": "Cette base contient:\n\nles enregistrements des températures des capitales européennes de Janvier à Décembre\nles coordonnées GPS de chaque ville\namplitude thermale : Différence entre les températures maximales et minimales\nmoyenne annuelle\nune variable qualitative : la direction (S, N, O, E).\n\nExécuter une PCA pour dégager des profils type de température et quelles villes les suivent."
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-chicken.csv",
    "href": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-chicken.csv",
    "title": "PCA - Exercises",
    "section": "Le jeu de données chicken.csv",
    "text": "Le jeu de données chicken.csv\n\nDescription : 43 poulets ayant subis 6 régimes : régime normal (N), Jeûne pendant 16h (F16), Jeûne pendant 16h et puis se réalimenter pendant 5h (F16R5), (F16R16), (F48), (F48R24)\nVariables : Après le régime, on a effectué une analyse des gènes utilisant une puce ADN : 7407 expressions de gènes.\nObjectif : Voir si les gènes s’expriment différemment selon le niveau de stress. Combien de temps faut-il au poulet pour revenir à la situation normale ?"
  },
  {
    "objectID": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-orange.csv",
    "href": "12_PCA/12-PCA-Exercises.html#le-jeu-de-données-orange.csv",
    "title": "PCA - Exercises",
    "section": "Le jeu de données orange.csv",
    "text": "Le jeu de données orange.csv\nSix jus d’orange de fabriquants différents ont été évalués. Toutes les variables sont-elles indisensables ? Y a-t-il des jus qui se dégagent comme particulièrement bons ? mauvais ?"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-linear-regression-model",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-linear-regression-model",
    "title": "Linear Regression with R",
    "section": "The linear regression model",
    "text": "The linear regression model\n\nThe goal is to propose and estimate a model for explaining a continuous response variable \\(Y_i\\) from a number of fixed predictors \\(x_{i1}, \\dots, x_{ik}\\).\nMathematically, the model reads: \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik} + \\varepsilon_i, \\quad \\mbox{with} \\quad \\mathbb{E}[\\varepsilon_i] = 0 \\quad \\mbox{and} \\quad \\mathbb{V}\\mbox{ar}[\\varepsilon_i] = \\sigma^2. \\]\nWe can summarize the assumptions on which relies the linear regression model as follows:\n\nThe predictors are fixed. This means that we do not assume (or take into account the) intrinsic variability in the predictor values. The randomness of \\(Y_i\\) all comes from the error term \\(\\varepsilon_i\\). In particular, it implies that considering a sample of \\(n\\) i.i.d. random response variables \\(Y_1, \\dots, Y_n\\) boils down to assuming that \\(\\varepsilon_1, \\dots, \\varepsilon_n\\) are i.i.d.;\nThe error random variables \\(\\varepsilon_i\\) are centered and of constant variance. Combined with Assumption 1, this means that \\(\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}\\) and \\(\\mathrm{Cov}[\\boldsymbol{\\varepsilon}] = \\sigma^2 \\mathbb{I}_n\\);\n[optional] Parametric hypothesis testing and confidence intervals further require the assumption of normality for the error vector, i.e. \\(\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbb{I}_n)\\)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimation-problem",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimation-problem",
    "title": "Linear Regression with R",
    "section": "Model Estimation Problem",
    "text": "Model Estimation Problem\n\nMatrix representation\n\n\\(Y_1, \\dots, Y_n\\) sample of \\(n\\) i.i.d. random response variables with associated observed values \\(y_1, \\dots, y_n\\),\nDesign matrix \\(\\mathbb{X}\\) of size \\(n \\times (k + 1)\\) with \\(x_{ij}\\) at row \\(i\\) and column \\(j\\) leading to \\(\\mathbf{y} = \\mathbb{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\).\nWarning: a regression is linear in the \\(\\boldsymbol{\\beta}\\) coefficients, not in the predictors.\n\nParameters to be estimated\n\nthe regression coefficients \\(\\boldsymbol{\\beta}\\); and,\nthe common error variance term \\(\\sigma^2\\).\n\nHow to estimate the best parameters?\n\\[ \\mbox{SSD}(\\boldsymbol{\\beta}, \\sigma^2; \\mathbf{y}, \\mathbb{X}) := (\\mathbf{y} - \\mathbb{X} \\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbb{X} \\boldsymbol{\\beta}) = \\sum_{i = 1}^n (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik}))^2 .\\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-case-of-categorical-predictors",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#the-case-of-categorical-predictors",
    "title": "Linear Regression with R",
    "section": "The case of categorical predictors",
    "text": "The case of categorical predictors\n\nIf a predictor is categorical, how do we fill in the numeric matrix \\(\\mathbb{X}\\)?\nDummy variables\n\nA categorical variable with two categories (sex for instance) can be transformed in a single numeric variable sex == \"male\", which evaluates to \\(1\\) if sex is \"male\" and to \\(0\\) if sex is \"female\" (remember logicals are numbers in R).\nA categorical variable with three categories (origin which stores the acronym of the New York airport for departure in the flights data set) can be converted into two numeric variables as shown in the table below:\n\n\n\norigin\noriginJFK\noriginLGA\n\n\n\n\nEWR\n0\n0\n\n\nJFK\n1\n0\n\n\nLGA\n0\n1"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimators",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-estimators",
    "title": "Linear Regression with R",
    "section": "Model Estimators",
    "text": "Model Estimators\n\nEstimator of the coefficients (unbiased)\n\\[ \\widehat{\\boldsymbol{\\beta}} := (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^\\top \\mathbf{Y}, \\]\nFitted responses\n\\[ \\widehat{\\mathbf{Y}} = \\mathbb{X} \\widehat{\\boldsymbol{\\beta}} = \\mathbb{X} (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^\\top \\mathbf{Y} = \\mathbb{H} \\mathbf{Y}. \\]\nEstimator of the constant variance term \\(\\sigma^2\\) (unbiased)\n\\[ \\widehat{\\sigma^2} := \\frac{(\\mathbf{Y} - \\widehat{\\mathbf{Y}})^\\top (\\mathbf{Y} - \\widehat{\\mathbf{Y}})}{n - k - 1}. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#hat-matrix-and-leverage",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#hat-matrix-and-leverage",
    "title": "Linear Regression with R",
    "section": "Hat matrix and leverage",
    "text": "Hat matrix and leverage\n\nHat matrix (projection matrix, influence matrix)\n\\[ \\mathbb{H} = \\mathbb{X} (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^\\top \\]\nLeverage\n\nThe diagonal terms of \\(\\mathbb{H}\\) are such that \\(0 \\le h_{ii} \\le 1\\).\n\\(h_{ii}\\) is called the leverage score of observation \\((y_i, x_{i1}, \\dots, x_{ik})\\).\nThe leverage score does not depend on \\(y_i\\) at all but only on the predictor values.\nWith some abuse of notation, we can write: \\[ h_{ii} = \\frac{\\partial \\widehat{Y_i}}{\\partial Y_i}, \\] which illustrates that the leverage measures the degree by which the \\(i\\)-th measured value influences the \\(i\\)-th fitted value."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#residuals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#residuals",
    "title": "Linear Regression with R",
    "section": "Residuals",
    "text": "Residuals\n\n\nNatural definition. Difference between observed and fitted response values: \\[ \\widehat{\\boldsymbol{\\varepsilon}} := \\mathbf{y} - \\widehat{\\mathbf{y}} = (\\mathbb{I}_n - \\mathbb{H}) \\mathbf{y}. \\]\n\\(\\mathbb{E}[\\widehat{\\boldsymbol{\\varepsilon}}] = \\mathbf{0}\\)\nResiduals do not have constant variance: \\[ \\mathbb{V}\\mbox{ar}[\\widehat{\\varepsilon}_i] = \\sigma^2 (1 - h_{ii}). \\]\nResiduals are not uncorrelated: \\[ \\mbox{Cov}(\\widehat{\\varepsilon}_i, \\widehat{\\varepsilon}_j) = -\\sigma^2 h_{ij}, \\mbox{ for } i \\ne j. \\]\nStandardized residuals. Residuals with almost constant variance (also known as internally studentized residuals): \\[ r_i := \\frac{\\widehat{\\varepsilon}_i}{s(\\widehat{\\varepsilon}_i)} = \\frac{\\widehat{\\varepsilon}_i}{\\sqrt{\\widehat{\\sigma^2} \\left( 1 - h_{ii} \\right)}}. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#studentized-residuals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#studentized-residuals",
    "title": "Linear Regression with R",
    "section": "Studentized residuals",
    "text": "Studentized residuals\n\n\nDefinition. Studentized residuals for any given data point are calculated from a model fit to every other data point except the one in question. They are also called externally studentized residuals, deleted residuals or jackknifed residuals. \\[ t_i := \\frac{y_i - \\widehat{y}_i^{(-i)}}{\\sqrt{\\widehat{\\sigma^2}^{(-i)} \\left( 1 - h_{ii} \\right)}} = r_i \\left( \\frac{n - k - 2}{n - k - 1 - r_i^2} \\right)^{1/2}, \\] where \\(n\\) is the total number of observations and \\(k\\) is the number of predictors used in the model.\nDistribution. If the normality assumption of the original regression model is met, a studentized residual follows a Student’s t-distribution.\nApplication. The motivation behind studentized residuals comes from their use in outlier testing. If we suspect a point is an outlier, then it was not generated from the assumed model, by definition. Therefore it would be a mistake – a violation of assumptions – to include that outlier in the fitting of the model. Studentized residuals are widely used in practical outlier detection.\nSource. https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#cooks-distance",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#cooks-distance",
    "title": "Linear Regression with R",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\n\nDefinition. Cook’s distance \\(D_i\\) of observation \\(i\\) (for \\(i = 1, \\dots, n\\)) is defined as a scaled sum of squared differences between fitted values obtained by including or excluding that observation: \\[ D_i := \\frac{ \\sum_{j = 1}^{n} \\left( \\widehat{y}_j - \\widehat{y}_j^{(-i)} \\right)^2}{\\widehat{\\sigma^2} (k+1)} = \\frac{r_i^2}{k+1} \\frac{h_{ii}}{1-h_{ii}}. \\]\nIf observation \\(i\\) is an outlier, in the sense that it has probably been drawn from another distribution with respect to the other observations, then \\(r_i\\) should be excessively high, which tends to increase Cook’s distance;\nIf observation \\(i\\) has a lot of influence (high leverage score), then Cook’s distance increases.\nYou can have cases in which an observation might have high influence but is not necessarily an outlier and therefore should be kept for analysis.\nThe reverse can happen as well. Some points with low influence (low leverage score) can be outliers (high residual value). In this case, we could be tempted to remove the observation because it violates our assumption.\nCook’s distance does not help in the above two situations, but it does not really matter, because, in both cases, we can safely include the corresponding observation into our regression."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals",
    "title": "Linear Regression with R",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nGoal\nCompute a confidence interval for the mean response \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) at an already observed point \\(\\mathbf{x}_i\\), \\(i=1,\\dots,n\\).\nPoint estimator of \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)\n\\[ \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}} \\sim \\mathcal{N} \\left( \\mathbf{x}_i^\\top \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{x}_i^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_i \\right), \\]\nPoint estimator of \\(\\sigma^2\\)\nWe do not know \\(\\sigma^2\\) but we have that: \\[ \\frac{\\widehat{\\sigma^2}}{\\sigma^2} \\sim \\chi^2(n-k-1), \\] and \\(\\widehat{\\boldsymbol{\\beta}}\\) and \\(\\widehat{\\sigma^2}\\) are statistically independent."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals-continued",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#confidence-intervals-continued",
    "title": "Linear Regression with R",
    "section": "Confidence intervals (continued)",
    "text": "Confidence intervals (continued)\n\nPivotal statistic\nA pivotal statistic for a parameter \\(\\theta\\) is a random variable \\(T(\\theta)\\) such that\n\nthe only unknown in its definition (computation) is the parameter that we aim at estimating,\nthe distribution of \\(T(\\theta)\\) is known and does not depend on the unknown parameter \\(\\theta\\),\n\nWhen the parameter is the mean response \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) at \\(\\mathbf{x}_i\\), we can use the following pivotal statistic:\n\\[ \\frac{\\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}}}{\\sqrt{\\widehat{\\sigma^2} \\mathbf{x}_i^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_i}} \\sim \\mathcal{S}tudent(n-k-1). \\]\nConstruction of the confidence interval\nWe can express a \\((1-\\alpha)\\%\\) confidence interval for the mean response \\(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\) at \\(\\mathbf{x}_i\\) as: \\[ \\mathbb{P} \\left( \\mathbf{x}_i^\\top \\boldsymbol{\\beta} \\in \\left[ \\mathbf{x}_i^\\top \\widehat{\\boldsymbol{\\beta}} \\pm t_{1-\\frac{\\alpha}{2}}(n-k-1) \\sqrt{\\widehat{\\sigma^2} \\mathbf{x}_i^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_i} \\right] \\right) = 1 - \\alpha. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals",
    "title": "Linear Regression with R",
    "section": "Prediction intervals",
    "text": "Prediction intervals\n\nGoal\nCompute a prediction interval for a new not yet observed response \\(Y_{n+1}\\) at a new observed point \\(\\mathbf{x}_{n+1}\\): \\[ Y_{n+1} = \\mathbf{x}_{n+1}^\\top \\boldsymbol{\\beta} + \\varepsilon_{n+1} \\sim \\mathcal{N} \\left( \\mathbf{x}_{n+1}^\\top \\boldsymbol{\\beta}, \\sigma^2 \\right), \\]\nPoint estimator of \\(Y_{n+1}\\)\n\\[ \\widehat{Y_{n+1}} = \\mathbf{x}_{n+1}^\\top \\widehat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}  \\left( \\mathbf{x}_{n+1}^\\top \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{x}_{n+1}^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_{n+1} \\right), \\]\nPoint estimator of \\(\\sigma^2\\)\nWe do not know \\(\\sigma^2\\) but we have that: \\[ \\frac{\\widehat{\\sigma^2}}{\\sigma^2} \\sim \\chi^2(n-k-1), \\] and \\(\\widehat{\\boldsymbol{\\beta}}\\) and \\(\\widehat{\\sigma^2}\\) are statistically independent."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals-continued",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#prediction-intervals-continued",
    "title": "Linear Regression with R",
    "section": "Prediction intervals (continued)",
    "text": "Prediction intervals (continued)\n\nPivotal statistic\nWe can now define a random variable of known distribution whose only unknown (once the \\(n\\)-sample is observed) is \\(Y_{n+1}\\): \\[ \\frac{Y_{n+1} - \\widehat{Y_{n+1}}}{\\sqrt{\\widehat{\\sigma^2} \\left( 1 + \\mathbf{x}_{n+1}^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_{n+1} \\right)}} \\sim \\mathcal{S}tudent(n-k-1). \\]\nConstruction of the prediction interval\nWe can express a \\((1-\\alpha)\\%\\) prediction interval for the new not yet observed response \\(Y_{n+1}\\) at \\(\\mathbf{x}_{n+1}\\) as: \\[ \\mathbb{P} \\left( Y_{n+1} \\in \\left[ \\widehat{Y_{n+1}} \\pm t_{1-\\frac{\\alpha}{2}}(n-k-1) \\sqrt{\\widehat{\\sigma^2} \\left( 1 + \\mathbf{x}_{n+1}^\\top (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbf{x}_{n+1} \\right)} \\right] \\right) = 1 - \\alpha. \\]"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-step-visualization",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-step-visualization",
    "title": "Linear Regression with R",
    "section": "First step: Visualization",
    "text": "First step: Visualization\n\nThe most important first thing you should do before any modeling attempt is to look at your data.\n\n\n\n\n\n\n\n\n\nAs you can see, both data sets seem to generate the same linear regression predictions, although we already clearly understand which one will go sideways…"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-step-modeling",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-step-modeling",
    "title": "Linear Regression with R",
    "section": "Second step: Modeling",
    "text": "Second step: Modeling\n\n\nWe know the R function to fit linear regression models is lm().\nWe have seen its syntax for simple models.\nWe have been introduced to the broom package for retrieving relevant information from the output of lm().\nThe broom::glance() help page will provide you with a short summary of the information retrieved by glance().\nThe broom::tidy() help page will provide you with a short summary of the information retrieved by tidy().\nThe broom::augment() help page will provide you with a short summary of the information retrieved by augment()."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#third-step-model-diagnosis",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#third-step-model-diagnosis",
    "title": "Linear Regression with R",
    "section": "Third step: Model Diagnosis",
    "text": "Third step: Model Diagnosis\n\nYou must always remember that, even though R facilitates computations, it does not verify for you that the assumptions required by the model are met by your data!\nThis is where model diagnosis comes into play. You can almost entirely diagnose your model graphically. Using the grammar of graphics in ggplot2, this can be achieved using the ggfortify package.\n\n# install.packages(\"ggfortify\")\nlibrary(ggfortify)\nfirst_model |&gt; \n  autoplot(which = 1:6, label.size = 3, nrow = 2) + \n  theme_bw()"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#reminder-model-assumptions-13",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#reminder-model-assumptions-13",
    "title": "Linear Regression with R",
    "section": "Reminder: model assumptions (1/3)",
    "text": "Reminder: model assumptions (1/3)\n\n\n\n\n\n\n\n\n\nAssumptions to check\n\n\n\nLinearity. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.\nNormality. The error terms are assumed to be normally distributed.\nHomogeneity of variance. The error terms are assumed to have a constant variance (homoscedasticity).\nIndependence. The error terms are assumed to be independent.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPotential problems to check\n\n\n\nNon-linearity of the outcome - predictor relationships\nHeteroscedasticity: Non-constant variance of error terms.\nPresence of influential and potential outlier values in the data:\n\nOutliers: typically large standardized residuals\nHigh-leverage points: typically large leverage values"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-residuals-vs-fitted-values",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-residuals-vs-fitted-values",
    "title": "Linear Regression with R",
    "section": "First data set: Residuals vs fitted values",
    "text": "First data set: Residuals vs fitted values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nTo check for non-linearity not accounted for; a horizontal line, without distinct patterns is an indication for a linear relationship, which is good."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-normal-qq-plot",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-normal-qq-plot",
    "title": "Linear Regression with R",
    "section": "First data set: Normal QQ plot",
    "text": "First data set: Normal QQ plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nTo check the normality of residuals; good if residuals follow the straight dashed line."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-scale-location-plot",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-scale-location-plot",
    "title": "Linear Regression with R",
    "section": "First data set: Scale-location plot",
    "text": "First data set: Scale-location plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nTo check the homoscedasticity assumption (constant variance); horizontal line with equally spread points is a good indication of homoscedasticity."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-influential-points-outliers",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#first-data-set-influential-points-outliers",
    "title": "Linear Regression with R",
    "section": "First data set: Influential points, outliers",
    "text": "First data set: Influential points, outliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\nTo check the zero-mean assumption on the residuals and to spot potential outliers in the top and bottom right corners (residuals vs leverages plot)\nTo spot potential outliers as observations with the largest Cook’s distance."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-data-set-model-diagnosis",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#second-data-set-model-diagnosis",
    "title": "Linear Regression with R",
    "section": "Second data set: Model diagnosis",
    "text": "Second data set: Model diagnosis"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification",
    "title": "Linear Regression with R",
    "section": "Model specification",
    "text": "Model specification\n\n\nmod &lt;- lm(formula = response ~ predictors, data = fancy_data)\n\nThe left-hand side of ~ must contain the response variable as named in the atomic vector storing its values or in the data set containing it.\nThe right-hand side specifies the predictors and will therefore be a combination of potentially transformed variables from your data set (or from atomic vectors defined in your R environment). To get the proper syntax for the rhs of the formula, you should be know the set of allowed operators that have a specific interpretation by R when used within a formula:\n\n+ for adding terms.\n- for removing terms.\n: for adding interaction terms only.\n* for crossing variables, i.e. adding variables and their interactions.\n%in% for nesting variables, i.e. adding one variable and its interaction with another.\n^ for limiting variable crossing to a specified order\n. for adding all other variables in the matrix that have not yet been included in the model."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification-continued",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-specification-continued",
    "title": "Linear Regression with R",
    "section": "Model specification (continued)",
    "text": "Model specification (continued)\n\nLinear regression assumes that the response variable (y) and at least one predictor (x) are continuous variables. The following table summarizes the effect of operators when adding a variable z to the predictors. When z is categorical, we assume that it can take \\(h\\) unique possible values \\(z_0, z_1, \\dots, z_{h-1}\\).\n\n\n\n\n\n\n\n\nType of z\nFormula\nModel\n\n\n\n\nContinuous\ny ~ x + z\n\\(Y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\varepsilon\\)\n\n\nContinuous\ny ~ x : z\n\\(Y = \\beta_0 + \\beta_1 x z + \\varepsilon\\)\n\n\nContinuous\ny ~ x * z\n\\(Y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 x z + \\varepsilon\\)\n\n\nCategorical\ny ~ x + z\n\\(Y = \\beta_0^{(0)} + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(0)} \\delta_{\\{z = z_\\ell\\}} + \\beta_1^{(1)} x + \\varepsilon\\)\n\n\nCategorical\ny ~ x : z\n\\(Y = \\beta_0^{(0)} + \\beta_1^{(1)} x + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(1)} x \\delta_{\\{z = z_\\ell\\}} + \\varepsilon\\)\n\n\nCategorical\ny ~ x * z\n\\(Y = \\beta_0^{(0)} + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(0)} \\delta_{\\{z = z_\\ell\\}} + \\beta_0^{(1)} x + \\sum_{\\ell = 1}^{h-1} \\beta_\\ell^{(1)} x \\delta_{\\{z = z_\\ell\\}} + \\varepsilon\\)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#i",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#i",
    "title": "Linear Regression with R",
    "section": "I()",
    "text": "I()\n\nWhat strikes from the previous table is that the natural multiplication of variables within a formula does not simply adds the product of the predictors in the model but also the predictors themselves.\nWhat if you want to actually perform an arithmetic operation on your predictors to include a transformed predictor in your model? For example, you might want to include both \\(x\\) and \\(x^2\\) in your model. You have two options:\n\n\n\nYou compute all of the (possibly transformed) predictors you want to include in your model beforehand and store them in the data set (remember dplyr::mutate() which will help you achieve this goal easily); or,\nYou use the as-is operator I(). In this case, you instruct lm() that the operations declared within I() must be performed outside from the formula environment and before generating the design matrix for the model."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effective-model-summaries",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effective-model-summaries",
    "title": "Linear Regression with R",
    "section": "Effective Model Summaries",
    "text": "Effective Model Summaries\n\nThere are two possible software suites:\n\neither jtools and interactions packages:\nor ggeffects and sjPlot packages.\n\nThey both provide tools for summarizing and visualising models, marginal effects, interactions and model predictions.\n\n\n# install.packages(c(\"jtools\", \"interactions\"))\n# install.packages(c(\"ggeffects\", \"sjPlot\"))\nfit1 &lt;- lm(\n  metascore ~ imdb_rating + log(us_gross) + genre5, \n  data = jtools::movies\n)\nfit2 &lt;- lm(\n  metascore ~ imdb_rating + log(us_gross) + log(budget) + genre5, \n  data = jtools::movies\n)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-12",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-12",
    "title": "Linear Regression with R",
    "section": "Tabular summary (1/2)",
    "text": "Tabular summary (1/2)\n\njtools::summ(fit1)\n\n\n\n\n\nObservations\n831 (10 missing obs. deleted)\n\n\nDependent variable\nmetascore\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(6,824)\n169.37\n\n\nR²\n0.55\n\n\nAdj. R²\n0.55\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-39.96\n5.92\n-6.75\n0.00\n\n\nimdb_rating\n12.80\n0.49\n25.89\n0.00\n\n\nlog(us_gross)\n0.47\n0.31\n1.52\n0.13\n\n\ngenre5Comedy\n6.32\n1.06\n5.95\n0.00\n\n\ngenre5Drama\n7.66\n1.08\n7.12\n0.00\n\n\ngenre5Horror/Thriller\n-0.73\n1.51\n-0.48\n0.63\n\n\ngenre5Other\n5.86\n3.25\n1.80\n0.07\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-22",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#tabular-summary-22",
    "title": "Linear Regression with R",
    "section": "Tabular summary (2/2)",
    "text": "Tabular summary (2/2)\n\njtools::export_summs(\n  fit1, fit2, \n  error_format = \"[{conf.low}, {conf.high}]\", error_pos = \"right\"\n)\n\n\n\nModel 1Model 2\n\n(Intercept)-39.96 ***[-51.58, -28.34]-16.47 *  [-31.16, -1.78]\n\nimdb_rating12.80 ***[11.83, 13.77]12.28 ***[11.30, 13.26]\n\nlog(us_gross)0.47    [-0.14, 1.07]1.60 ***[0.86, 2.34]\n\ngenre5Comedy6.32 ***[4.24, 8.41]4.84 ***[2.70, 6.97]\n\ngenre5Drama7.66 ***[5.55, 9.77]6.71 ***[4.60, 8.83]\n\ngenre5Horror/Thriller-0.73    [-3.70, 2.24]-2.81    [-5.84, 0.23]\n\ngenre5Other5.86    [-0.52, 12.24]6.30 *  [0.01, 12.59]\n\nlog(budget)           -2.25 ***[-3.13, -1.37]\n\nN831           831           \n\nR20.55        0.57        \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#visual-summary",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#visual-summary",
    "title": "Linear Regression with R",
    "section": "Visual summary",
    "text": "Visual summary\n\njtools::plot_summs(fit1, fit2, inner_ci_level = .9)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---continuous-predictor",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---continuous-predictor",
    "title": "Linear Regression with R",
    "section": "Effect plot - Continuous predictor",
    "text": "Effect plot - Continuous predictor\n\nfit &lt;- lm(cty ~ displ + year + cyl + class + fl, data = mpg)\nfit_poly &lt;- lm(cty ~ poly(displ, 2) + year + cyl + class + fl, data = mpg)\n\n\n\n\njtools::effect_plot(fit, pred = displ, interval = TRUE, plot.points = TRUE)\n\n\n\n\n\n\n\n\n\njtools::effect_plot(fit_poly, pred = displ, interval = TRUE, plot.points = TRUE)\n\n\n\n\n\n\n\n\n\n\njtools::effect_plot(fit, pred = displ, interval = TRUE, partial.residuals = TRUE)\n\n\n\n\n\n\n\n\n\njtools::effect_plot(fit_poly, pred = displ, interval = TRUE, partial.residuals = TRUE)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---categorical-predictor",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#effect-plot---categorical-predictor",
    "title": "Linear Regression with R",
    "section": "Effect plot - Categorical predictor",
    "text": "Effect plot - Categorical predictor\n\njtools::effect_plot(fit, pred = fl, interval = TRUE, partial.residuals = TRUE, jitter = .2)"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#interaction-plots",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#interaction-plots",
    "title": "Linear Regression with R",
    "section": "Interaction plots",
    "text": "Interaction plots\nThis is handled by the interaction package.\n\nJohnson-Neyman plot: sim_slopes()\nTwo-way interaction with at least one continuous predictor: interact_plot()\nTwo-way interaction between categorical predictors: cat_plot()"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#collinearity-multicollinearity",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#collinearity-multicollinearity",
    "title": "Linear Regression with R",
    "section": "Collinearity & Multicollinearity",
    "text": "Collinearity & Multicollinearity\n\n(Multi)-collinearity refers to high correlation in two or more independent variables in the regression model.\nMulticollinearity can arise from poorly designed experiments (Data-based multicollinearity) or from creating new independent variables related to the existing ones (structural multicollinearity).\nCorrelation plots are a great tool to explore colinearity between two variables. They can be seamlessly computed and visualized using packages such as corrr or ggcorrplot."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-13",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-13",
    "title": "Linear Regression with R",
    "section": "Variation Inflation Factor (1/3)",
    "text": "Variation Inflation Factor (1/3)\nThe Variance inflation factor (VIF) measures the degree of multicollinearity or collinearity in the regression model.\n\\[\n\\mathrm{VIF}_i = \\frac{1}{1 - R_i^2},\n\\]\nwhere \\(R_i^2\\) is the multiple correlation coefficient associated to the regression of \\(X_i\\) against all remaining independent variables."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-23",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-23",
    "title": "Linear Regression with R",
    "section": "Variation Inflation Factor (2/3)",
    "text": "Variation Inflation Factor (2/3)\nYou can add VIFs to the summary of a model as follows:\n\njtools::summ(fit, vifs = TRUE)\n\n\n\n\n\nObservations\n234\n\n\nDependent variable\ncty\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(13,220)\n100.37\n\n\nR²\n0.86\n\n\nAdj. R²\n0.85\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\nVIF\n\n\n\n\n(Intercept)\n-199.79\n51.14\n-3.91\n0.00\nNA\n\n\ndispl\n-1.02\n0.29\n-3.53\n0.00\n11.76\n\n\nyear\n0.12\n0.03\n4.52\n0.00\n1.11\n\n\ncyl\n-0.85\n0.20\n-4.26\n0.00\n8.69\n\n\nclasscompact\n-2.81\n1.00\n-2.83\n0.01\n3.69\n\n\nclassmidsize\n-2.95\n0.97\n-3.05\n0.00\n3.69\n\n\nclassminivan\n-5.08\n1.05\n-4.82\n0.00\n3.69\n\n\nclasspickup\n-5.89\n0.92\n-6.37\n0.00\n3.69\n\n\nclasssubcompact\n-2.63\n1.01\n-2.61\n0.01\n3.69\n\n\nclasssuv\n-5.59\n0.88\n-6.33\n0.00\n3.69\n\n\nfld\n5.93\n1.85\n3.21\n0.00\n1.60\n\n\nfle\n-5.13\n1.81\n-2.84\n0.00\n1.60\n\n\nflp\n-2.97\n1.72\n-1.73\n0.08\n1.60\n\n\nflr\n-1.69\n1.70\n-0.99\n0.32\n1.60\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-33",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#variation-inflation-factor-33",
    "title": "Linear Regression with R",
    "section": "Variation Inflation Factor (3/3)",
    "text": "Variation Inflation Factor (3/3)\n\nVIFs are always at least equal to 1.\nIn some domains, VIF over 2 is worthy of suspicion. Others set the bar higher, at 5 or 10. Others still will say you shouldn’t pay attention to these at all.\nSmall effects are more likely to be drowned out by higher VIFs, but this may just be a natural, unavoidable fact with your model (e.g., there is no problem with high VIFs when you have an interaction effect)."
  },
  {
    "objectID": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-selection",
    "href": "11_Linear_Modeling/11-Linear-Modeling-Slides.html#model-selection",
    "title": "Linear Regression with R",
    "section": "Model selection",
    "text": "Model selection\n\n\n\n\n\n\n\nForward selection\n\n\n\nStart with no variables in the model.\nTest the addition of each variable using a chosen model fit criterion1, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit\nRepeat until none improves the model to a statistically significant extent.\n\n\n\n\n\n\n\n\n\n\n\n\nBackward elimination\n\n\n\nStart with all candidate variables\nTest the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit\nRepeat until no further variables can be deleted without a statistically significant loss of fit.\n\n\n\n\n\n\n\nA list of possible and often used model fit criterion is available on the Wikipedia Model Selection page. In R, the basic stats::step() function uses the Akaike’s information criterion (AIC) and allows to perform forward selection (direction = \"forward\") or backward elimination (direction = \"backward\")."
  }
]